(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[872],{9979:function(e,n,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/books/designing-cloud-data-platforms",function(){return s(7161)}])},7161:function(e,n,s){"use strict";s.r(n),s.d(n,{__toc:function(){return d}});var i=s(5893),r=s(2673),l=s(2643),a=s(7854),t=s(8397);let d=[{depth:2,value:"1 - Introducing the data platform",id:"1---introducing-the-data-platform"},{depth:2,value:"2 - Why a data platform and not just a data warehouse",id:"2---why-a-data-platform-and-not-just-a-data-warehouse"},{depth:2,value:"3 - Getting bigger and leveraging the Big 3: Amazon, Microsoft, and Google",id:"3---getting-bigger-and-leveraging-the-big-3-amazon-microsoft-and-google"},{depth:2,value:"4 - Getting data into the platform",id:"4---getting-data-into-the-platform"},{depth:2,value:"5 - Organizing and processing data",id:"5---organizing-and-processing-data"},{depth:2,value:"6 - Real-time data processing and analytics",id:"6---real-time-data-processing-and-analytics"},{depth:2,value:"7 - Metadata layer architecture",id:"7---metadata-layer-architecture"},{depth:2,value:"8 - Schema management",id:"8---schema-management"},{depth:2,value:"9 - Data access and security",id:"9---data-access-and-security"},{depth:2,value:"10 - Fueling business value with data platforms",id:"10---fueling-business-value-with-data-platforms"}];function c(e){let n=Object.assign({h1:"h1",h2:"h2",ul:"ul",li:"li",strong:"strong",em:"em",pre:"pre",code:"code",span:"span",a:"a"},(0,l.a)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{children:"Designing Cloud Data Platforms"}),"\n",(0,i.jsx)(n.h2,{id:"1---introducing-the-data-platform",children:"1 - Introducing the data platform"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"analytics"})," permettent essentiellement d'obtenir des m\xe9triques pour faire des choix business.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Avant l'av\xe8nement des ordinateurs, les entreprises utilisaient des moyens manuels, et leur intuition."}),"\n",(0,i.jsxs)(n.li,{children:["Dans les ann\xe9es 80 on a vu \xe9merger le concept de ",(0,i.jsx)(n.em,{children:"data warehouse"}),", qui est une base centralis\xe9e de donn\xe9es venant de diverses sources."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"data warehouses"})," posent de plus en plus de ",(0,i.jsx)(n.strong,{children:"probl\xe8mes"})," de nos jours.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les tendances suivantes y contribuent :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les donn\xe9es sont issues de sources de diverses nature, y compris certaines d'entre-elles non structur\xe9es, et leur volume est de plus en plus important."}),"\n",(0,i.jsx)(n.li,{children:"Le d\xe9coupage des applications en microservices fait que collecter des donn\xe9es revient forc\xe9ment \xe0 devoir agr\xe9ger de multiples sources."}),"\n",(0,i.jsxs)(n.li,{children:["Les data scientists ont aussi besoin d'acc\xe9der \xe0 une version brute de la donn\xe9e, et cet usage ne peut pas passer par un ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Elles ont du mal avec les ",(0,i.jsx)(n.strong,{children:"3V"})," (Variety, Volume, Velocity).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Variety"})," : les ",(0,i.jsx)(n.em,{children:"data warehouses"})," ne supportent que les ",(0,i.jsx)(n.em,{children:"structured data"})," dont le sch\xe9ma est stable, c'est-\xe0-dire en pratique qui sont issues de DB relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Or avec l'av\xe8nement des SaaS, des r\xe9seaux sociaux, et de l'IoT, on se retrouve avec :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"semistructured data"})," du type JSON, Avro etc, dont le sch\xe9ma varie souvent."]}),"\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"unstructured data"})," comme le binaire, le son, la vid\xe9o."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Volume"})," : le fait que dans un ",(0,i.jsx)(n.em,{children:"data warehouse"}),", la puissance de calcul et le stockage doivent se trouver sur ",(0,i.jsx)(n.strong,{children:"la m\xeame machine physique"}),", implique qu'on ne peut pas scaler les deux s\xe9par\xe9ment, et donc les co\xfbts explosent.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"M\xeame les petites organisations peuvent \xeatre amen\xe9es \xe0 traiter plusieurs TB de donn\xe9es."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Velocity"})," : les ",(0,i.jsx)(n.em,{children:"data warehouses"})," ne sont pas adapt\xe9es aux analytics en mode real time, elles sont plus orient\xe9es batch processing."]}),"\n",(0,i.jsxs)(n.li,{children:["Le machine learning en particulier pose tous les probl\xe8mes en m\xeame temps : il n\xe9cessite une grande quantit\xe9 de donn\xe9es vari\xe9es, et accapare la puissance de calcul du ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"data lakes"})," r\xe9pondent en partie \xe0 ces probl\xe8mes.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'id\xe9e principale des ",(0,i.jsx)(n.em,{children:"data lakes"})," c'est qu'on ",(0,i.jsx)(n.strong,{children:"stocke de la donn\xe9e telle quelle"})," (ou quasi), et qu'on essayera de la traiter et de lui coller un sch\xe9ma d\xe8s qu'on en aura besoin."]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"data lakes"})," se sont g\xe9n\xe9ralis\xe9s \xe0 partir de 2006 avec l'arriv\xe9e de ",(0,i.jsx)(t.U,{children:"Hadoop"}),", qui est un ",(0,i.jsx)(n.strong,{children:"filesystem distribu\xe9 sur plusieurs machines"})," pas ch\xe8res.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Hadoop r\xe9pond en partie aux 3V :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A la ",(0,i.jsx)(n.em,{children:"Variety"})," par l'\xe9criture schema-less."]}),"\n",(0,i.jsxs)(n.li,{children:["Au ",(0,i.jsx)(n.em,{children:"Volume"})," par le fait que ce soit distribu\xe9 sur des machines pas ch\xe8res."]}),"\n",(0,i.jsxs)(n.li,{children:["A la ",(0,i.jsx)(n.em,{children:"Velocity"})," par la facilit\xe9 de streaming \xe0 partir du filesystem distribu\xe9."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Mais il a aussi des probl\xe8mes :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"C'est un syst\xe8me complexe qu'il faut installer sur un datacenter et g\xe9rer par des Ops exp\xe9riment\xe9s."}),"\n",(0,i.jsxs)(n.li,{children:["D'un point de vue business, c'est plus difficile de travailler avec les outils qui traitent les donn\xe9es non structur\xe9es qu'avec du SQL comme dans un ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Bien qu'il soit distribu\xe9 sur de petites machines pas ch\xe8res, le computing et le stockage ne sont pas s\xe9par\xe9s, ce qui limite quand m\xeame la r\xe9duction de co\xfbt quand on a besoin de beaucoup de l'un sans l'autre."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"cloud public"})," vient r\xe9pondre aux probl\xe8mes de ",(0,i.jsx)(t.U,{children:"Hadoop"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"data warehouses"})," et les ",(0,i.jsx)(n.em,{children:"data lakes"})," ont \xe9t\xe9 propos\xe9s par les cloud providers, avec de nombreux avantages :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La possibilit\xe9 de scaler la puissance de calcul et le stockage s\xe9par\xe9ment."}),"\n",(0,i.jsx)(n.li,{children:"Payer uniquement \xe0 l'usage des machines qu'on emprunte."}),"\n",(0,i.jsx)(n.li,{children:"Ne plus avoir \xe0 g\xe9rer la complexit\xe9 de l'infrastructure."}),"\n",(0,i.jsx)(n.li,{children:"Des outils et frameworks avanc\xe9s d\xe9velopp\xe9s par les cloud providers autour de leurs produits."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Exemple : ",(0,i.jsx)(t.U,{children:"AWS EMR"})," permet de lancer un cluster sur lequel on va pouvoir ex\xe9cuter des jobs ",(0,i.jsx)(t.U,{children:"Hadoop"})," et ",(0,i.jsx)(t.U,{children:"Spark"}),",","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On a juste \xe0 indiquer le nombre de nœuds qu'on veut, et les packages qu'on veut installer dessus."}),"\n",(0,i.jsxs)(n.li,{children:["Et on a la possibilit\xe9 de faire des allers-retours vers ",(0,i.jsx)(t.U,{children:"S3"})," pour scaler diff\xe9remment le calcul et le stockage."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"cloud data platform"})," moderne utilise \xe0 la fois le ",(0,i.jsx)(n.em,{children:"data warehouse"})," et le ",(0,i.jsx)(n.em,{children:"data lake"}),", h\xe9berg\xe9s dans un cloud public, chacun d'entre eux remplissant un usage particulier.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour \xeatre polyvalente et pas ch\xe8re, la data platform doit avoir des ",(0,i.jsx)(n.strong,{children:"4 composants principaux faiblement coupl\xe9s"}),", interagissant entre-eux avec une API bien d\xe9finie.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ingestion layer"})," : on va chercher les donn\xe9es chez les diff\xe9rents types de sources (DB relationnelle, DB NoSQL, API externes etc.).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On va en g\xe9n\xe9ral utiliser un ensemble d'outils open source ou commerciaux pour chaque type de donn\xe9es \xe0 aller chercher."}),"\n",(0,i.jsxs)(n.li,{children:["Il ne faut ",(0,i.jsx)(n.strong,{children:"surtout pas alt\xe9rer la donn\xe9e \xe0 cette \xe9tape"}),", pour que la donn\xe9e brute soit disponible pour les data scientists qui en auraient l'usage."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage layer"})," : on utilise le stockage cloud comme stockage de notre ",(0,i.jsx)(n.em,{children:"data lake"}),", dans lequel on met ce qu'on a ing\xe9r\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le stockage cloud a l'avantage de ne pas avoir besoin de planifier la capacit\xe9 de stockage : il grossit automatiquement au besoin."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Processing layer"})," : on transforme la donn\xe9e pour la rendre utilisable par la plupart des clients de la plateforme.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["C'est la partie calcul de notre ",(0,i.jsx)(n.em,{children:"data lake"}),", il va lire depuis le cloud storage puis \xe9crire \xe0 nouveau dedans."]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas du ",(0,i.jsx)(n.strong,{children:"streaming"}),", on ne passe pas par le storage layer qui prend trop de temps, mais on envoie la donn\xe9e ",(0,i.jsx)(n.strong,{children:"directement au processing layer"}),", qui va ensuite la rendre disponible au layer d'apr\xe8s."]}),"\n",(0,i.jsxs)(n.li,{children:["Le processing est g\xe9n\xe9ralement fait avec des outils open source, les plus connus \xe9tant ",(0,i.jsx)(t.U,{children:"Spark"}),", ",(0,i.jsx)(t.U,{children:"Beam"})," et ",(0,i.jsx)(t.U,{children:"Flink"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Serving layer"})," : on rend la donn\xe9e disponible sous divers formats, selon les besoins des clients de la plateforme.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les usages peuvent \xeatre :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des analystes qui ont besoin d'ex\xe9cuter des requ\xeates SQL sur la donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut charger la donn\xe9e dans un ",(0,i.jsx)(n.em,{children:"data warehouse"})," chez le cloud provider."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Des applications qui ont besoin d'un acc\xe8s rapide \xe0 la donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut la charger dans une key / value DB, ou une document DB."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Des \xe9quipes de data scientists / engineers ont besoin de transformer la donn\xe9e eux-m\xeames.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut leur donner acc\xe8s au storage du ",(0,i.jsx)(n.em,{children:"data lake"}),", et les laisser utiliser ",(0,i.jsx)(t.U,{children:"Spark"}),", ",(0,i.jsx)(t.U,{children:"Beam"})," ou ",(0,i.jsx)(t.U,{children:"Flink"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La cloud data platform r\xe9pond aux 3V :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'ingestion layer coupl\xe9 au stockage sans sch\xe9ma permet une grande ",(0,i.jsx)(n.em,{children:"Variety"})," des donn\xe9es."]}),"\n",(0,i.jsxs)(n.li,{children:["La s\xe9paration calcul / stockage et le fait de ne payer que ce qu'on utilise permet d'optimiser les co\xfbts, et d'avoir un gros ",(0,i.jsx)(n.em,{children:"Volume"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["La possibilit\xe9 d'envoyer directement au ",(0,i.jsx)(n.em,{children:"processing layer"})," permet de la ",(0,i.jsx)(n.em,{children:"Velocity"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut aussi prendre en compte deux autres V :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"Veracity"})," qui indique le niveau de ",(0,i.jsx)(n.em,{children:"data governance"}),", c'est-\xe0-dire la qualit\xe9 de la donn\xe9e. On l'obtient it\xe9rativement, au cours d'\xe9tapes au sein du ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Et la ",(0,i.jsx)(n.em,{children:"Value"})," qu'on peut tirer de la donn\xe9e, qui peut \xeatre plus \xe9lev\xe9e si on prend plus de donn\xe9es en amont de notre processus de nettoyage."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il faut comprendre les ",(0,i.jsx)(n.strong,{children:"cas d'usages principaux"})," d'un ",(0,i.jsx)(n.em,{children:"data lake"}),", pour \xe9viter de le transformer en ",(0,i.jsx)(n.em,{children:"data swamp"}),". Parmi les plus courants il y a :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"vue 360\xb0 des clients"}),", o\xf9 il s'agit de r\xe9cup\xe9rer toutes les donn\xe9es d'interaction avec eux, pour proposer ensuite des services plus personnalis\xe9s, vendre plus etc."]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"donn\xe9es venant d'IoT"}),", qui ont la particularit\xe9 d'\xeatre incertaines et d'avoir un gros volume, ce qui rend l'utilisation du ",(0,i.jsx)(n.em,{children:"data warehouse"})," peu int\xe9ressante."]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"machine learning"})," qui a besoin d'une tr\xe8s grande quantit\xe9 de donn\xe9es, et qui tire avantage de puissance de calcul s\xe9par\xe9e des autres use-cases gr\xe2ce au ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2---why-a-data-platform-and-not-just-a-data-warehouse",children:"2 - Why a data platform and not just a data warehouse"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ce chapitre donne des ",(0,i.jsxs)(n.strong,{children:["arguments pour le choix d'une ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", plut\xf4t qu'une simple ",(0,i.jsx)(n.em,{children:"data warehouse"})]}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On impl\xe9mente les deux solutions pour une situation d'",(0,i.jsx)(n.strong,{children:"exemple"})," qu'on va utiliser dans ce chapitre :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Nous sommes l'\xe9quipe data, et le d\xe9partement marketing a besoin que nous r\xe9cup\xe9rions deux sources de donn\xe9es et qu'on les corr\xe8le r\xe9guli\xe8rement.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'une des sources est une table de campagnes de marketing, issue d'une DB ",(0,i.jsx)(t.U,{children:"MySQL"})," interne."]}),"\n",(0,i.jsxs)(n.li,{children:["Et l'autre est constitu\xe9e de fichiers CSV de clics utilisateurs, issus de logs applicatifs (et donc ",(0,i.jsx)(n.em,{children:"semistructured"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"On part sur Microsoft Azure pour les deux solutions."}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l'impl\xe9mentation ",(0,i.jsx)(n.em,{children:"data warehouse only"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - On va utiliser deux ",(0,i.jsx)(t.U,{children:"Azure Data Factory"})," pour r\xe9cup\xe9rer la donn\xe9e dans le serveur de DB et les fichiers CSV dans le serveur SFTP. C'est notre ",(0,i.jsx)(n.em,{children:"ingest layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Ensuite on redirige \xe7a vers l'",(0,i.jsx)(t.U,{children:"Azure Synapse"}),", qui est la ",(0,i.jsx)(n.em,{children:"data warehouse"})," de chez Azure. Elle va faire office de ",(0,i.jsx)(n.em,{children:"store layer"}),", ",(0,i.jsx)(n.em,{children:"process layer"})," et ",(0,i.jsx)(n.em,{children:"serve layer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l'impl\xe9mentation ",(0,i.jsx)(n.em,{children:"cloud data platform"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - On a notre ",(0,i.jsx)(n.em,{children:"ingest layer"})," avec ",(0,i.jsx)(t.U,{children:"Azure Data Factory"}),", qui redirige les donn\xe9es vers le ",(0,i.jsx)(n.em,{children:"store layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Le ",(0,i.jsx)(n.em,{children:"store layer"})," est impl\xe9ment\xe9 avec ",(0,i.jsx)(t.U,{children:"Azure Blob Storage"}),". Il s'agit d'un stockage de type ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["3 - On a un ",(0,i.jsx)(n.em,{children:"process layer"})," qui utilise ",(0,i.jsx)(t.U,{children:"Azure Databricks"}),", et qui fait tourner ",(0,i.jsx)(t.U,{children:"Spark"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["4 - Le ",(0,i.jsx)(n.em,{children:"serve layer"})," enfin utilise ",(0,i.jsx)(t.U,{children:"Azure Synapse"})," qui est le ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l'",(0,i.jsx)(n.strong,{children:"ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"data warehouse only"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La pipeline contient :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"linked services"})," : ici la ",(0,i.jsx)(n.em,{children:"data source"})," ",(0,i.jsx)(t.U,{children:"MySQL"})," en entr\xe9e, et la ",(0,i.jsx)(n.em,{children:"data sink"})," ",(0,i.jsx)(t.U,{children:"Azure Synapse"})," en sortie."]}),"\n",(0,i.jsxs)(n.li,{children:["Des ",(0,i.jsx)(n.em,{children:"data sets"})," : il s'agit de la description du sch\xe9ma de donn\xe9es d'entr\xe9e et de sortie, et leur mapping."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Si le sch\xe9ma de la DB source change, il faudra mettre \xe0 jour le sch\xe9ma d\xe9fini dans la pipeline et le mapping.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Mais surtout il faudra ",(0,i.jsx)(n.strong,{children:"g\xe9rer soi-m\xeame la migration"})," du ",(0,i.jsx)(n.em,{children:"data sink"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"cloud data platform"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Cette fois le ",(0,i.jsx)(n.em,{children:"data sink"})," est un ",(0,i.jsx)(t.U,{children:"Azure Blob Storage"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il n'y a plus besoin de sp\xe9cifier les sch\xe9mas et le mapping entre input et output puisque l'output accueille la donn\xe9e telle quelle."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Si le sch\xe9ma de la DB source change, il n'y a ",(0,i.jsx)(n.strong,{children:"rien \xe0 faire c\xf4t\xe9 ingestion"})," : on \xe9crira de toute fa\xe7on la donn\xe9e dans un nouveau fichier.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On d\xe9place le probl\xe8me de mapping plus loin."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le ",(0,i.jsx)(n.strong,{children:"processing"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Dans la version ",(0,i.jsx)(n.em,{children:"data warehouse only"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va charger les deux donn\xe9es :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La DB ",(0,i.jsx)(t.U,{children:"MySQL"})," sans charger sa structure parce qu'elle est d\xe9j\xe0 relationnelle."]}),"\n",(0,i.jsx)(n.li,{children:"La donn\xe9e CSV semistructur\xe9e dans des rows de type texte qu'on parsera en JSON avec une fonction SQL built-in."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"requ\xeate SQL"})," qu'on va \xe9crire aura les d\xe9savantages suivants :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Elle sera ",(0,i.jsx)(n.strong,{children:"peu lisible"}),", \xe0 cause du code de parsing n\xe9cessaire.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On pourrait la rendre plus lisible en pr\xe9-parsant la donn\xe9e, mais \xe7a veut dire plus de temps et des co\xfbts plus \xe9lev\xe9s."}),"\n",(0,i.jsxs)(n.li,{children:["Une autre solution de lisibilit\xe9 pourrait \xeatre d'ajouter des UDF (User Defined Functions), qu'il faudrait maintenir et d\xe9ployer sur chaque instance d'",(0,i.jsx)(t.U,{children:"Azure Synapse"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Elle sera ",(0,i.jsx)(n.strong,{children:"difficile \xe0 tester"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Elle risque de ne pas profiter de la ",(0,i.jsx)(n.strong,{children:"performance"})," offerte par la structure en colonne du ",(0,i.jsx)(n.em,{children:"data warehouse"}),", parce que les donn\xe9es texte qu'on parse en JSON ne sont pas organisables physiquement en colonnes."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans la version ",(0,i.jsx)(n.em,{children:"cloud data platform"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On a la possibilit\xe9 d'utiliser un ",(0,i.jsx)(n.em,{children:"distributed data processing engine"})," comme ",(0,i.jsx)(t.U,{children:"Apache Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On pourra \xe9crire des requ\xeates SQL pour des exp\xe9rimentations rapides."}),"\n",(0,i.jsxs)(n.li,{children:["Et on pourra aussi \xe9crire du code ",(0,i.jsx)(n.strong,{children:"lisible, maintenable et testable"})," dans un langage comme Python ou Scala, quand il s'agit de projet de plus long terme."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant l'",(0,i.jsx)(n.strong,{children:"acc\xe8s \xe0 la donn\xe9e"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il peut y avoir plusieurs types de consommateurs :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des utilisateurs plut\xf4t ",(0,i.jsx)(n.strong,{children:"orient\xe9s business"})," comme des \xe9quipes marketing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ils vont pr\xe9f\xe9rer utiliser des outils de reporting type ",(0,i.jsx)(t.U,{children:"Power BI"}),", et donc auront besoin de la donn\xe9e sous forme relationnelle, par exemple dans ",(0,i.jsx)(t.U,{children:"Azure Synapse"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Des utilisateurs orient\xe9s ",(0,i.jsx)(n.strong,{children:"data analyse / data science"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ils pourront b\xe9n\xe9ficier de SQL qu'ils utilisent souvent directement, au travers de ",(0,i.jsx)(t.U,{children:"Spark SQL"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Ils pourront avoir acc\xe8s \xe0 des donn\xe9es non filtr\xe9es pour leur projets data science, gr\xe2ce ",(0,i.jsx)(t.U,{children:"Spark"})," directement."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Au final la ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", qui contient \xe0 la fois la donn\xe9e sous forme brute dans le ",(0,i.jsx)(n.em,{children:"data lake"}),", et la donn\xe9e dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", est ",(0,i.jsx)(n.strong,{children:"adapt\xe9e \xe0 chaque usage"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["A propos des ",(0,i.jsx)(n.strong,{children:"co\xfbts financiers"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est difficile de comparer les co\xfbts des services cloud.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"En g\xe9n\xe9ral on constate que le stockage est plut\xf4t pas cher, et que l'essentiel des co\xfbts se trouve dans les calculs."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["L'",(0,i.jsx)(n.strong,{children:"elastic scaling"})," consiste \xe0 pouvoir calibrer le service pour l'usage exact qu'on en a, et de ne pas avoir \xe0 payer plus.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"C'est un des \xe9l\xe9ments qui permet de vraiment optimiser les co\xfbts."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"data warehouse only"}),", l'essentiel des co\xfbts va aller dans ",(0,i.jsx)(t.U,{children:"Azure Synapse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le scaling de ce service peut prendre des dizaines de minutes, donc c'est quelque chose qu'on ne peut faire que de temps en temps."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la version ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", l'essentiel des co\xfbts est port\xe9 par le ",(0,i.jsx)(n.em,{children:"processing layer"}),", par exemple ",(0,i.jsx)(t.U,{children:"Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Spark"})," est particuli\xe8rement \xe9lastique, au point o\xf9 il est commun de d\xe9marrer une instance juste le temps d'une requ\xeate."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"3---getting-bigger-and-leveraging-the-big-3-amazon-microsoft-and-google",children:"3 - Getting bigger and leveraging the Big 3: Amazon, Microsoft, and Google"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il existe un trade off entre choisir des ",(0,i.jsx)(n.strong,{children:"services vendor-specific"})," de type PaaS, et choisir des ",(0,i.jsx)(n.strong,{children:"services open source"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"D'un c\xf4t\xe9 on se couple au vendor mais on minimise les co\xfbts d'Ops, et de l'autre on permet une meilleure portabilit\xe9 mais on augmente les co\xfbts d'Ops."}),"\n",(0,i.jsxs)(n.li,{children:["Les auteurs trouvent que ",(0,i.jsx)(n.strong,{children:"la solution vendor-specific est celle qui a en g\xe9n\xe9ral le moins de d\xe9savantages"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour r\xe9pondre aux probl\xe9matiques de la data moderne, il faut une ",(0,i.jsx)(n.strong,{children:"architecture en 6 couches"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Data ingestion layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Se connecter aux sources et r\xe9cup\xe9rer la donn\xe9e dans le ",(0,i.jsx)(n.em,{children:"data lake"})," sans trop la modifier."]}),"\n",(0,i.jsxs)(n.li,{children:["Enregistrer des statistiques et un statut dans le ",(0,i.jsx)(n.em,{children:"metadata repository"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Selon les auteurs, il vaut mieux mettre en place ",(0,i.jsx)(n.strong,{children:"\xe0 la fois un m\xe9canisme de type batch et un m\xe9canisme de type streaming"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"L'industrie est en train de se diriger vers le streaming, mais de nombreuses sources externes fournissent la donn\xe9e sous un format de type batch avec des \xe9l\xe9ments group\xe9s, par exemple CSV, JSON, XML."}),"\n",(0,i.jsxs)(n.li,{children:["On pourrait utiliser la partie batch pour ing\xe9rer des donn\xe9es par petits batchs, et \xe9viter de faire la version streaming. Mais \xe7a cr\xe9erait de la ",(0,i.jsx)(n.strong,{children:"dette technique"})," parce qu'on finira par avoir besoin du streaming \xe0 un moment ou un autre."]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"lambda architecture"})," consiste \xe0 avoir la donn\xe9e qui passe \xe0 la fois par le m\xe9canisme de batch et par le m\xe9canisme de streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Cette duplication \xe9tait n\xe9cessaire parce que le streaming n'\xe9tait pas fiable dans les d\xe9buts de ",(0,i.jsx)(t.U,{children:"Hadoop"}),", mais ce n'est plus le cas."]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"cloud data platform"})," ne consiste pas \xe0 faire une telle duplication : selon la source, la donn\xe9e va passer par le m\xe9canisme de streaming ou de batch."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On entend parfois plusieurs choses diff\xe9rentes quand on parle de ",(0,i.jsx)(n.em,{children:"real time"})," pour des analytics :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - La ",(0,i.jsx)(n.em,{children:"real time ingestion"})," consiste \xe0 avoir la donn\xe9e disponible pour de l'analyse d\xe8s qu'elle arrive."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Le ",(0,i.jsx)(n.em,{children:"real time analytics"})," consiste \xe0 avoir des fonctionnalit\xe9s d'analytics qui se mettent \xe0 jour \xe0 chaque arriv\xe9e de donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cette derni\xe8re est plus difficile \xe0 faire, donc il vaut mieux bien clarifier les besoins."}),"\n",(0,i.jsx)(n.li,{children:"Exemple : d\xe9tection de fraude en temps r\xe9el."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Storage layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Stocker la donn\xe9e pour du court terme et du long terme."}),"\n",(0,i.jsx)(n.li,{children:"La rendre disponible pour la consommation streaming et la consommation batch."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"slow storage"})," est l\xe0 pour le mode batch.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La donn\xe9e y est persist\xe9e pour pas cher, gr\xe2ce \xe0 la possibilit\xe9 de scaler le stockage sans ajouter de capacit\xe9 de calcul."}),"\n",(0,i.jsx)(n.li,{children:"Par contre les temps d'acc\xe8s sont grands."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"fast storage"})," est l\xe0 pour le mode streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s'agit d'utiliser un outil qui est fait pour l'acc\xe8s rapide, comme ",(0,i.jsx)(t.U,{children:"Apache Kafka"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Par contre, on n'a en g\xe9n\xe9ral pas la possibilit\xe9 de scaler le stockage sans ajouter de puissance de calcul, et donc les co\xfbts sont plus grands."}),"\n",(0,i.jsx)(n.li,{children:"On va donc purger r\xe9guli\xe8rement la donn\xe9e du fast storage, et de la transf\xe9rer dans le slow storage."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - Processing layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Lire la donn\xe9e depuis le stockage et y appliquer de la business logic."}),"\n",(0,i.jsx)(n.li,{children:"Persister la donn\xe9e modifi\xe9e \xe0 nouveau dans le stockage pour un usage par les data scientists."}),"\n",(0,i.jsx)(n.li,{children:"D\xe9livrer la donn\xe9e aux autres consumers."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il faut un ou plusieurs outils qui permettent de r\xe9aliser des transformations de donn\xe9es, y compris avec du calcul distribu\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Un exemple peut \xeatre ",(0,i.jsx)(t.U,{children:"Google Dataflow"}),", qui est une version PaaS d'",(0,i.jsx)(t.U,{children:"Apache Beam"}),", qui supporte \xe0 la fois le mode streaming et le mode batch."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - Technical metadata layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Stocker des informations techniques sur chaque layer.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\xc7a peut \xeatre les sch\xe9mas d'ingestion, le statut de telle ou telle \xe9tape, des statistiques sur les donn\xe9es ou les erreurs, etc."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Permettre \xe0 chaque layer d'ajouter/modifier ou consulter des informations."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Par exemple, le ",(0,i.jsx)(n.em,{children:"processing layer"})," peut v\xe9rifier dans la ",(0,i.jsx)(n.em,{children:"technical metadata layer"})," qu'une certaine donn\xe9e est disponible pour aller la chercher, plut\xf4t que de demander \xe0 l'",(0,i.jsx)(n.em,{children:"ingestion layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ce qui permet un certain ",(0,i.jsx)(n.strong,{children:"d\xe9couplage"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["D'autres exemples peuvent impliquer des usages de ",(0,i.jsx)(n.strong,{children:"monitoring"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"business metadata"})," est une autre notion qui peut avoir son layer, mais qui n'est pas explor\xe9e dans ce livre.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit d'identifier l'usage business qui est fait de chaque donn\xe9e qu'on r\xe9cup\xe8re des sources, et d'en faire un catalogue."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il n'y a pas vraiment d'outil unique qui permette de remplir ce r\xf4le pour le moment, donc on devra sans doute en utiliser plusieurs.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple ",(0,i.jsx)(t.U,{children:"Confluent Schema Registry"})," et ",(0,i.jsx)(t.U,{children:"Amazon Glue"})," peuvent supporter certains des cas d'usages."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"5 - Serving layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Servir les consumers qui ont besoin de donn\xe9es relationnelles via une ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Servir les consumers qui ont besoin de la donn\xe9e brute, en acc\xe9dant directement au ",(0,i.jsx)(n.em,{children:"data lake"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les data scientistes vont en g\xe9n\xe9ral vouloir y acc\xe9der via le slow storage."}),"\n",(0,i.jsxs)(n.li,{children:["Et l'acc\xe8s via le fast storage va plut\xf4t int\xe9resser les applications qui s'abonnent en mode streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple un syst\xe8me de recommandation ecommerce en temps r\xe9el."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"6.1 - Orchestration overlay layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Coordonner l'ex\xe9cution de jobs, sous la forme d'un graphe de d\xe9pendance."}),"\n",(0,i.jsx)(n.li,{children:"G\xe9rer les \xe9checs et les retries."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["C'est un peu le compl\xe9ment du ",(0,i.jsx)(n.em,{children:"technical metadata layer"})," pour permettre le faible couplage entre les layers."]}),"\n",(0,i.jsxs)(n.li,{children:["L'outil le plus connu d'orchestration est ",(0,i.jsx)(t.U,{children:"Apache Airflow"}),", adopt\xe9 par Google Cloud Platform sous le nom de ",(0,i.jsx)(t.U,{children:"Google Composer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"AWS et Azure ont quant \xe0 eux choisi d'inclure des fonctionnalit\xe9s d'orchestration dans leur outil d'ETL."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"6.2 - ETL overlay layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son but est de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Prendre en charge les fonctionnalit\xe9s de certains layers (ingestion, processing, metadata, orchestration) ",(0,i.jsx)(n.strong,{children:"avec peu ou pas de code"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On pourrait faire l'ensemble de notre pipeline avec cet outil ETL, la question \xe0 se poser c'est : ",(0,i.jsx)(n.strong,{children:"\xe0 quel point il est ouvert \xe0 l'extension ?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut vouloir \xe0 l'avenir par exemple utiliser un autre outil de processing, ou s'interfacer avec un outil open source."}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 il y a une incompatibilit\xe9 avec un usage qu'on a, on peut toujours l'impl\xe9menter \xe0 part de l'outil ETL.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le probl\xe8me c'est qu'au bout d'un moment, les usages \xe0 c\xf4t\xe9 deviennent aussi complexes que la solution enti\xe8re sans l'outil ETL, mais avec une ",(0,i.jsx)(n.strong,{children:"architecture spaghetti"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les outils ETL il y a ",(0,i.jsx)(t.U,{children:"AWS Glue"}),", ",(0,i.jsx)(t.U,{children:"Azure Data Factory"})," et ",(0,i.jsx)(t.U,{children:"Google Cloud Data Fusion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il existe des solutions commerciales non cloud-natives comme ",(0,i.jsx)(t.U,{children:"Talend"})," et ",(0,i.jsx)(t.U,{children:"Informatica"}),", mais ce livre se limite au cloud-native et aux outils open source."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"couches"})," doivent \xeatre bien ",(0,i.jsx)(n.strong,{children:"s\xe9par\xe9es et d\xe9coupl\xe9es"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Une premi\xe8re raison est de pouvoir utiliser les outils les plus adapt\xe9s aux besoins de chaque couche.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le cloud bougeant tr\xe8s vite, on voudra sans doute pouvoir changer seulement l'un d'entre eux quand on a une meilleure alternative pour une couche en particulier."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une autre raison est qu'on peut avoir plusieurs \xe9quipes en charge de la data platform, et il vaut mieux qu'elles ne se g\xeanent pas.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple, on voudra souvent avoir l'ingestion plut\xf4t centralis\xe9e, et le processing plut\xf4t en mode libre service pour chaque \xe9quipe qui en a besoin."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"outils"})," pouvant servir dans une des couches de notre plateforme sont class\xe9s en 4 cat\xe9gories (les auteurs les priorisent dans cet ordre) :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - Solutions ",(0,i.jsx)(n.strong,{children:"cloud-native PaaS"})," d'AWS, GCP ou Azure.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Leur avantage principal c'est le gain de temps : on n'a pas \xe0 se pr\xe9occuper de la compatibilit\xe9. On configure tr\xe8s facilement et c'est en prod."}),"\n",(0,i.jsx)(n.li,{children:"Par contre, c'est la solution qui va \xeatre la moins extensible : si par exemple un connecteur n'est pas support\xe9, on aura du mal \xe0 l'ajouter."}),"\n",(0,i.jsx)(n.li,{children:"Elle est aussi peu portable, vu qu'on n'a pas les m\xeames services d'un cloud provider \xe0 un autre."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Solutions ",(0,i.jsx)(n.strong,{children:"serverless"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit de pouvoir d\xe9ployer son code custom, mais sans avoir \xe0 se pr\xe9occuper des serveurs, de leur configuration, du scaling etc."}),"\n",(0,i.jsx)(n.li,{children:"C'est une solution interm\xe9diaire d'un point de vue trade-offs sur la flexibilit\xe9, la portabilit\xe9 et le gain de temps."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["3 - Solutions ",(0,i.jsx)(n.strong,{children:"open-source"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Leur avantage c'est c'est la flexibilit\xe9 et la portabilit\xe9 maximales, mais de l'autre c\xf4t\xe9 on a \xe0 g\xe9rer soi-m\xeame des VMs dans le cloud donc plus de travail d'Ops."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["4 - Solutions ",(0,i.jsx)(n.strong,{children:"SaaS commerciales"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Elles peuvent avoir un int\xe9r\xeat si elles ont une fonctionnalit\xe9 non disponible sous forme PaaS ou open source."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans les faits, on va utiliser un ",(0,i.jsx)(n.strong,{children:"mix de solutions des 4 cat\xe9gories"})," en fonction des layers et des besoins qu'on a.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On a de plus en plus d'entreprises qui utilisent des solutions de ",(0,i.jsx)(n.strong,{children:"plusieurs cloud providers"}),". Par exemple le gros des services sur AWS, et le use-case machine learning sur GCP."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Outils sur ",(0,i.jsx)(n.strong,{children:"AWS"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Glue"})," supporte l'ingestion \xe0 partir de ",(0,i.jsx)(t.U,{children:"AWS S3"}),", ou \xe0 partir d'une connexion JDBC."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Database Migration Service"})," sert \xe0 la base \xe0 transf\xe9rer ses DBs vers AWS, mais on peut l'utiliser comme ingestion layer."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS DMS"})," permet d'impl\xe9menter un m\xe9canisme de change data capture \xe0 partir d'une DB."]}),"\n",(0,i.jsxs)(n.li,{children:["Si aucune des solutions PaaS ne supporte notre data source, on peut utiliser la solution serverless ",(0,i.jsx)(t.U,{children:"AWS Lambda"})," o\xf9 il faudra \xe9crire et maintenir du code."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Kinesis"})," est un message broker pour lequel il faudra \xe9crire du code pour publier dedans. Il a malheureusement tr\xe8s peu de connecteurs entrants.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["En revanche il a des connecteurs sortants appel\xe9s ",(0,i.jsx)(t.U,{children:"Kinesis Firehose"}),", qui permettent par exemple d'envoyer la donn\xe9e de Kinesis dans un ",(0,i.jsx)(t.U,{children:"S3"})," sous format ",(0,i.jsx)(t.U,{children:"Parquet"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Managed Streaming for Apache Kafka (MSK)"})," est une version de ",(0,i.jsx)(t.U,{children:"Kafka"})," enti\xe8rement manag\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut l'utiliser \xe0 la place de ",(0,i.jsx)(t.U,{children:"Kinesis"}),", par exemple si on migre une application avec ",(0,i.jsx)(t.U,{children:"Kafka"})," vers AWS."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Storage.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS S3"})," permet de stocker de la donn\xe9e de mani\xe8re scalable, avec la possibilit\xe9 de choisir entre plusieurs formules avec des latences plus ou moins grandes."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Batch processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Elastic MapReduce (EMR)"})," est une version manag\xe9e de ",(0,i.jsx)(t.U,{children:"Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va en g\xe9n\xe9ral lire la donn\xe9e depuis ",(0,i.jsx)(t.U,{children:"S3"}),", faire le calcul, puis d\xe9truire le cluster ",(0,i.jsx)(t.U,{children:"EMR"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Kinesis Data Analytics"})," permet de se brancher sur ",(0,i.jsx)(t.U,{children:"Kinesis"}),", et de faire du processing en streaming."]}),"\n",(0,i.jsxs)(n.li,{children:["Si on utilise ",(0,i.jsx)(t.U,{children:"AWS MSK"}),", on peut brancher dessus ",(0,i.jsx)(t.U,{children:"Kafka Streams"})," pour le processing en streaming."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Data warehouse.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Redshift"})," est un ",(0,i.jsx)(n.em,{children:"data warehouse"})," distribut\xe9 sur plusieurs noeuds.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Redshift Spectrum"})," permet de faire des requ\xeates depuis ",(0,i.jsx)(t.U,{children:"Redshift"})," pour obtenir des donn\xe9es qui sont en fait sur ",(0,i.jsx)(t.U,{children:"S3"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faudra d\xe9finir des “tables externes”, et la performance de la query sera moins bonne, mais \xe7a permet d'\xe9conomiser de la place dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Direct access.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Athena"})," permet de faire une requ\xeate SQL distribu\xe9e en utilisant directement la donn\xe9e sur ",(0,i.jsx)(t.U,{children:"S3"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On lance l'instance le temps de la requ\xeate, puis on d\xe9truit l'instance."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["ETL overlay et metadata repository.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Glue"})," est un outil d'ETL complet.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est construit autour de ",(0,i.jsx)(t.U,{children:"Spark"}),", et poss\xe8de des templates pour faciliter de nombreuses transformations.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a aussi des add-ons ",(0,i.jsx)(t.U,{children:"Spark"})," non-standards, ce qui nuit \xe0 la portabilit\xe9 par rapport \xe0 un simple ",(0,i.jsx)(t.U,{children:"Spark"})," manag\xe9."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il maintient un ",(0,i.jsx)(n.em,{children:"Data Catalog"})," \xe0 partir des donn\xe9es disponibles sur ",(0,i.jsx)(t.U,{children:"S3"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Il maintient un ensemble de statistiques sur l'ex\xe9cution des jobs."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Step Functions"})," permet de cr\xe9er des workflows qui mettent en jeu diff\xe9rents services, y compris ceux qui ne seraient pas g\xe9r\xe9s par ",(0,i.jsx)(t.U,{children:"Glue"})," comme ",(0,i.jsx)(t.U,{children:"AWS Lambda"})," avec du code custom."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Consumers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour les outils comme ",(0,i.jsx)(t.U,{children:"Tableau"})," qui ont besoin d'une connexion JDBC/ODBC qui supporte SQL, elles peuvent se connecter \xe0 ",(0,i.jsx)(t.U,{children:"Redshift"})," ou ",(0,i.jsx)(t.U,{children:"Athena"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour du streaming avec faible latence, on peut envoyer la donn\xe9e dans un key/value store comme ",(0,i.jsx)(t.U,{children:"DynamoDB"}),", ou dans une DB comme ",(0,i.jsx)(t.U,{children:"AWS RDS"})," ou ",(0,i.jsx)(t.U,{children:"AWS Aurora"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Outils sur ",(0,i.jsx)(n.strong,{children:"GCP"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Data Fusion"})," est un ETL overlay qui permet d'ing\xe9rer des donn\xe9es depuis une DB relationnelle avec JDBC, des fichiers depuis ",(0,i.jsx)(t.U,{children:"Google Cloud Storage"}),", et m\xeame depuis un FTP ou depuis ",(0,i.jsx)(t.U,{children:"AWS S3"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il est bas\xe9 sur un projet open source, et donc supporte des connecteurs custom."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"BigQuery Data Transfer Service"})," permet d'ing\xe9rer de la donn\xe9e depuis les services SaaS de Google, et depuis des centaines d'autres services SaaS connus gr\xe2ce \xe0 un partenariat avec Fivetran.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par contre, la donn\xe9e va directement dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", ce qui ne permet pas vraiment l'architecture modulaire qu'on vise."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Functions"})," repr\xe9sente l'\xe9quivalent d'",(0,i.jsx)(t.U,{children:"AWS Lambda"}),", avec le d\xe9savantage d'avoir une limite de temps d'ex\xe9cution des fonctions serverless."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Stream ingrestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Pub/Sub"})," est un broker \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS Kinesis"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Storage.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Google Cloud Storage"})," est un \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS S3"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Batch processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Dataproc"})," est un ",(0,i.jsx)(t.U,{children:"Spark"})," manag\xe9 \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS EMR"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Dataflow"})," est un ",(0,i.jsx)(t.U,{children:"Apache Beam"})," manag\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Beam"})," a l'avantage d'offrir une m\xeame API pour le batch processing et le streaming processing, l\xe0 o\xf9 ",(0,i.jsx)(t.U,{children:"Spark"})," ne supporte que le batch mais est une techno plus mature."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Dataflow"})," repr\xe9sente la mani\xe8re cloud-native de faire du streaming sur GCP."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Dataproc"})," avec ",(0,i.jsx)(t.U,{children:"Spark Streaming"})," peut repr\xe9senter une alternative, mais il s'agit en fait de micro-batch et non pas de traiter les messages un par un.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs conseillent ",(0,i.jsx)(t.U,{children:"Beam"}),", sauf si on a d\xe9j\xe0 investi en temps ou connaissances sur ",(0,i.jsx)(t.U,{children:"Spark"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Data warehouse.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"BigQuery"})," est un \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS Redshift"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il a l'avantage de scaler le nombre de nœuds tout seul."}),"\n",(0,i.jsx)(n.li,{children:"Par contre il a un mod\xe8le de facturation bas\xe9 sur la donn\xe9e lue par chaque requ\xeate, ce qui peut rendre les co\xfbts difficiles \xe0 pr\xe9dire."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Direct access.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["GCP ne propose pas de services pour acc\xe9der au ",(0,i.jsx)(n.em,{children:"data lake"})," directement avec du SQL.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut \xe9ventuellement cr\xe9er des tables vers de la donn\xe9e externe (donc dans le ",(0,i.jsx)(n.em,{children:"data lake"}),") \xe0 partir de ",(0,i.jsx)(t.U,{children:"BigQuery"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut aussi utiliser ",(0,i.jsx)(t.U,{children:"Spark SQL"})," pour identifier et lire de la donn\xe9e sur le ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["ETL overlay et metadata repository.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Data Fusion"})," est un ETL overlay \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS Glue"}),". Il fournit une UI qui permet de configurer la pipeline.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il met \xe0 disposition un moyen d'analyser quelle partie de la pipeline peut affecter telle ou telle donn\xe9e."}),"\n",(0,i.jsx)(n.li,{children:"Il met aussi \xe0 disposition des statistiques sur l'ex\xe9cution des jobs."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Composer"})," permet de cr\xe9er des flows d'orchestration entre jobs.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est bas\xe9 sur ",(0,i.jsx)(t.U,{children:"Apache Airflow"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Consumers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"BigQuery"})," n'a pas de connexion JDBC/ODBC pour y connecter un outil BI par exemple.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il a une API REST, et il est directement compatible avec certains outils BI."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Si on veut consommer la donn\xe9e avec une faible latence, on peut la mettre dans le key/value store ",(0,i.jsx)(t.U,{children:"Cloud Bigtable"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Outils sur ",(0,i.jsx)(n.strong,{children:"Azure"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Data Factory"})," est un ETL overlay permettant de faire de l'ingestion depuis diverses sources (DB, SaaS externes, ",(0,i.jsx)(t.U,{children:"S3"}),", ",(0,i.jsx)(t.U,{children:"GCS"})," etc.).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est celui qui a le plus de connecteurs compar\xe9 \xe0 ",(0,i.jsx)(t.U,{children:"AWS Glue"})," et ",(0,i.jsx)(t.U,{children:"Cloud Data Fusion"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Functions"})," est l'\xe9quivalent d'",(0,i.jsx)(t.U,{children:"AWS Lambda"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il ne supporte que Java et Python."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Event Hubs"})," est \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS Kinesis"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a la particularit\xe9 d'\xeatre compatible avec ",(0,i.jsx)(t.U,{children:"Apache Kafka"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Storage.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Blob Storage"})," est \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS S3"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Data Lake Storage"})," est une version am\xe9lior\xe9e qui supporte mieux le calcul distribu\xe9 avec de grandes quantit\xe9s de donn\xe9es."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Batch processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour le batch processing, Azure a choisi de miser sur un partenariat avec ",(0,i.jsx)(t.U,{children:"Databricks"}),", qui est un service cr\xe9\xe9 par les cr\xe9ateurs de ",(0,i.jsx)(t.U,{children:"Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La version manag\xe9e de ",(0,i.jsx)(t.U,{children:"Databricks"})," est disponible sur AWS et Azure, mais elle est celle par d\xe9faut sur Azure, donc mieux support\xe9e par son \xe9cosyst\xe8me."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Stream Analytics"})," se branche sur ",(0,i.jsx)(t.U,{children:"Event Hubs"})," et permet de faire du streaming processing."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Data warehouse.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Synapse"})," est le ",(0,i.jsx)(n.em,{children:"data warehouse"})," d'Azure.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est entre ",(0,i.jsx)(t.U,{children:"AWS Redshift"})," et ",(0,i.jsx)(t.U,{children:"Google BigQuery"})," dans la mesure o\xf9 il n\xe9cessite de choisir la capacit\xe9 de calcul, mais il scale l'espace disque tout seul."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Direct access.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Databricks"})," est la mani\xe8re privil\xe9gi\xe9e d'acc\xe9der \xe0 la donn\xe9e sur le ",(0,i.jsx)(n.em,{children:"data lake"}),", soit par l'API native de ",(0,i.jsx)(t.U,{children:"Spark"}),", soit en SQL avec ",(0,i.jsx)(t.U,{children:"Spark SQL"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["ETL overlay et metadata repository.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Data Factory"})," est \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"AWS Glue"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s'int\xe8gre parfaitement avec ",(0,i.jsx)(t.U,{children:"Databricks"})," pour les transformations complexes."]}),"\n",(0,i.jsx)(n.li,{children:"Il fournit des m\xe9triques sur la data pipeline."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La partie orchestration des jobs est prise en charge par ",(0,i.jsx)(t.U,{children:"Azure Data Factory"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Consumers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Synapse"})," fournit une connexion JDBC/ODBC pour connecter les outils de BI.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Databricks"})," fournit la m\xeame chose, mais il faut un cluster ",(0,i.jsx)(t.U,{children:"Spark"})," toujours allum\xe9, ce qui peut co\xfbter cher."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cosmos DB"})," est une DB orient\xe9e document o\xf9 on peut stocker les r\xe9sultats de processings pour un acc\xe8s faible latence."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Alternatives ",(0,i.jsx)(n.strong,{children:"commerciales"})," ou ",(0,i.jsx)(n.strong,{children:"open source"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Certains logiciels open source sont trop difficiles \xe0 mettre en place, par exemple un ",(0,i.jsx)(n.em,{children:"data warehouse"})," distribu\xe9 comme ",(0,i.jsx)(t.U,{children:"Apache Druid"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Batch ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il existe pas mal d'outils open source et commerciaux qui permettent d'ing\xe9rer des donn\xe9es, leur valeur ajout\xe9e \xe9tant en g\xe9n\xe9ral le grand nombre de sources support\xe9es."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Apachi NiFi"})," est une solution open source qui supporte de nombreuses sources, et permet d'en ajouter soi-m\xeame en Java."]}),"\n",(0,i.jsxs)(n.li,{children:["Il existe de nombreux outils SaaS commerciaux qui g\xe8rent l'ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ces outils vont souvent envoyer la donn\xe9e directement dans un ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Il faut bien r\xe9fl\xe9chir \xe0 la probl\xe9matique de la s\xe9curit\xe9."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Streaming ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Apache Kafka"})," est le principal outil utilis\xe9 en dehors d'une solution manag\xe9e de streaming.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a l'avantage de pouvoir se connecter \xe0 de nombreuses sources avec ",(0,i.jsx)(t.U,{children:"Kafka Connect"}),", et il a un moyen d'impl\xe9menter des applications de streaming avec ",(0,i.jsx)(t.U,{children:"Kafka Streams"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les raisons de choisir ",(0,i.jsx)(t.U,{children:"Kafka"})," plut\xf4t qu'une solution cloud-native peuvent \xeatre l'investissement qu'on a d\xe9j\xe0 dans ",(0,i.jsx)(t.U,{children:"Kafka"})," (par exemple connaissances), ou le besoin de performance n\xe9cessitant le fine-tuning du serveur ",(0,i.jsx)(t.U,{children:"Kafka"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Apache Airflow"})," est le principal outil utilis\xe9 en dehors d'une solution manag\xe9e d'orchestration.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La raison de l'utiliser en mode non manag\xe9 peut \xeatre de profiter de sa flexibilit\xe9, avec ses fichiers en Python."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"4---getting-data-into-the-platform",children:"4 - Getting data into the platform"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le layer d'ingestion peut avoir besoin d'ing\xe9rer diff\xe9rents types de donn\xe9es :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Les bases de donn\xe9es relationnelles"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Leurs donn\xe9es sont organis\xe9es en colonnes et typ\xe9es, mais chaque vendor a des types \xe0 lui.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il y a donc un ",(0,i.jsx)(n.strong,{children:"mapping"})," \xe0 faire entre le type de chaque colonne et notre mod\xe8le."]}),"\n",(0,i.jsx)(n.li,{children:"Ce mapping va changer r\xe9guli\xe8rement en fonction des \xe9volutions fonctionnelles des applications qui poss\xe8dent ces DBs."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Vu que la donn\xe9e est normalis\xe9e, elle se trouve dans des centaines de tables.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faudra donc ",(0,i.jsx)(n.strong,{children:"automatiser le mapping"})," pour \xe9viter de le faire \xe0 la main."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La donn\xe9e change r\xe9guli\xe8rement dans la DB, pour refl\xe9ter l'\xe9tat de l'application, elle est ",(0,i.jsx)(n.strong,{children:"volatile"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faudra donc aller chercher r\xe9guli\xe8rement les derniers changements."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Les fichiers."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les fichiers sont structur\xe9s selon divers types de format texte ou binaire (CSV, JSON XML, Avro, Protobuf etc.) qui ne contiennent pas d'information de type.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut donc pouvoir ",(0,i.jsx)(n.strong,{children:"supporter le parsing de tous ces formats"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les fichiers ne garantissent aucun sch\xe9ma, et on voit beaucoup plus souvent des changements dans celui-ci que pour les DB relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut donc g\xe9rer les ",(0,i.jsx)(n.strong,{children:"changements de sch\xe9ma fr\xe9quents"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les fichiers repr\xe9sentent en g\xe9n\xe9ral de la ",(0,i.jsx)(n.strong,{children:"donn\xe9e fig\xe9e"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La nouvelle donn\xe9e est \xe9crite dans un autre fichier, donc on se retrouve \xe0 devoir ing\xe9rer ",(0,i.jsx)(n.strong,{children:"de nombreux fichiers"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - La donn\xe9e SaaS via API."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les donn\xe9es SaaS sont en g\xe9n\xe9ral disponibles via une API REST, qui renvoie du JSON."}),"\n",(0,i.jsxs)(n.li,{children:["Chaque provider a sa propre API, et son propre format. Il faudra donc ",(0,i.jsx)(n.strong,{children:"impl\xe9menter la partie ingestion pour chacun des providers"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faudra faire la validation du sch\xe9ma \xe0 chaque fois."}),"\n",(0,i.jsx)(n.li,{children:"Il faudra la tenir \xe0 jour en fonction des changements d'API."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - Les streams"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les m\xeames donn\xe9es peuvent arriver plusieurs fois, donc il faut que notre pipeline puisse ",(0,i.jsx)(n.strong,{children:"g\xe9rer les duplicatas"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les events des streams sont immutables, et peuvent \xeatre corrig\xe9s en ajoutant un autre message modifi\xe9 au stream.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Donc il faut que notre pipeline ",(0,i.jsx)(n.strong,{children:"g\xe8re la r\xe9conciliation entre plusieurs versions d'un m\xeame message"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les donn\xe9es de streaming ont en g\xe9n\xe9ral un ",(0,i.jsx)(n.strong,{children:"grand volume"}),", donc il faut une infrastructure qui le supporte."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le cas des ",(0,i.jsx)(n.strong,{children:"bases de donn\xe9es relationnelles"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il y a deux moyens d'ing\xe9rer de la donn\xe9e depuis une DB relationnelle :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - L'utilisation de requ\xeates SQL"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s'agit d'avoir un composant qui va :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - Ex\xe9cuter la requ\xeate vers la DB concern\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ca peut \xeatre un simple :","\n",(0,i.jsx)(n.pre,{"data-language":"sql","data-theme":"default",children:(0,i.jsx)(n.code,{"data-language":"sql","data-theme":"default",children:(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"SELECT"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"*"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"FROM"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"table"})]})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"2 - R\xe9cup\xe9rer la donn\xe9e sous un format qu'il comprend."}),"\n",(0,i.jsxs)(n.li,{children:["3 - Mapper la donn\xe9e dans le bon format pour la stocker sur le ",(0,i.jsx)(n.em,{children:"storage layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Il y a donc ",(0,i.jsx)(n.strong,{children:"2 mappings"})," qui se produisent pendant l'op\xe9ration."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Alors que la donn\xe9e op\xe9rationnelle s'int\xe9resse \xe0 l'\xe9tat actuel (“Quels sont les articles dans le panier ?”), ",(0,i.jsx)(n.strong,{children:"la donn\xe9e analytique s'int\xe9resse \xe0 l'\xe9volution de l'\xe9tat dans le temps"})," (“Quels articles ont \xe9t\xe9 ajout\xe9s ou enlev\xe9s et dans quel ordre ?”).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut donc un moyen pour capturer l'\xe9volution de la donn\xe9e dans le temps."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une 1\xe8re solution pour garder l'\xe9volution dans le temps est de faire une ",(0,i.jsx)(n.strong,{children:"full table ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va r\xe9cup\xe9rer l'ensemble des donn\xe9es d'une table \xe0 intervals r\xe9guliers, sauver ces snapshots dans le ",(0,i.jsx)(n.em,{children:"data lake"}),", et les charger dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour en tirer quelque chose, il faut ",(0,i.jsx)(n.strong,{children:"superposer les rows des snapshots"})," dans la m\xeame table du ",(0,i.jsx)(n.em,{children:"data warehouse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour diff\xe9rencier les rows de chaque snapshot, on peut ajouter une colonne ",(0,i.jsx)(n.code,{children:"INGEST_DATE"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut directement utiliser du SQL pour obtenir les donn\xe9es qu'on veut, mais pour certains usages on aura besoin de faire une transformation dans le ",(0,i.jsx)(n.em,{children:"processing layer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les donn\xe9es d\xe9riv\xe9es qu'on voudra cr\xe9er, il peut y avoir :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Cr\xe9er une ",(0,i.jsx)(n.em,{children:"view"})," qui ne montre que les rows du dernier snapshot."]}),"\n",(0,i.jsxs)(n.li,{children:["De la donn\xe9e qui ",(0,i.jsx)(n.strong,{children:"identifie les suppressions"}),", en identifiant les rows qui existaient dans un snapshot et n'existaient plus dans le suivant."]}),"\n",(0,i.jsx)(n.li,{children:"Une version “compact\xe9e”, qui \xe9limine les rows qui n'ont pas chang\xe9 par rapport au snapshot pr\xe9c\xe9dent."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le probl\xe8me de la full table ingestion, c'est la charge sur la machine de DB, et l'",(0,i.jsx)(n.strong,{children:"\xe9norme quantit\xe9 de donn\xe9es"})," qu'on finit par avoir."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une autre solution peut \xeatre l'",(0,i.jsx)(n.strong,{children:"incremental table ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s'agit toujours de r\xe9cup\xe9rer des snapshots \xe0 intervalles r\xe9guliers, mais ",(0,i.jsx)(n.strong,{children:"seulement de la donn\xe9e qui a chang\xe9"})," depuis le pr\xe9c\xe9dent snapshot."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour savoir quelle donn\xe9e a chang\xe9 :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La table d'origine doit avoir un champ ",(0,i.jsx)(n.code,{children:"LAST_MODIFIED"}),", mis \xe0 jour automatiquement par la DB."]}),"\n",(0,i.jsxs)(n.li,{children:["En retenant le ",(0,i.jsx)(n.code,{children:"MAX(LAST_MODIFIED)"})," du dernier run d'ingestion (qu'on appelle le ",(0,i.jsx)(n.em,{children:"highest watermark"}),"), on peut construire une query qui r\xe9cup\xe8re uniquement les nouvelles donn\xe9es :","\n",(0,i.jsx)(n.pre,{"data-language":"sql","data-theme":"default",children:(0,i.jsx)(n.code,{"data-language":"sql","data-theme":"default",children:(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"SELECT"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"*"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"FROM"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" subscriptions "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"WHERE"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" LAST_MODIFIED "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:">"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"2019-05-01 17:01:00"'})]})})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On pourra mettre le ",(0,i.jsx)(n.em,{children:"highest watermark"})," dans le ",(0,i.jsx)(n.em,{children:"technical metadata layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Glue"})," g\xe8re nativement le stockage de ce genre de donn\xe9es, mais on peut le mettre dans une DB manag\xe9e comme ",(0,i.jsx)(t.U,{children:"Google Cloud SQL"})," ou ",(0,i.jsx)(t.U,{children:"Azure SQL Database"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Cette ",(0,i.jsx)(n.em,{children:"incremental table ingestion"})," permet d'ing\xe9rer moins de donn\xe9es dupliqu\xe9es, mais elle a encore des inconv\xe9nients :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut faire du processing pour faire appara\xeetre les donn\xe9es supprim\xe9es, en comparant les snapshots entre eux."}),"\n",(0,i.jsxs)(n.li,{children:["Les donn\xe9es qui sont ins\xe9r\xe9es puis supprim\xe9es entre deux snapshots ne seront pas captur\xe9es par ce m\xe9canisme, donc ",(0,i.jsx)(n.strong,{children:"on perd des informations"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Le Change Data Capture (CDC)"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le CDC permet de r\xe9cup\xe9rer ",(0,i.jsx)(n.strong,{children:"l'ensemble des op\xe9rations"})," qui ont lieu sur la table, ",(0,i.jsx)(n.strong,{children:"sans aucun doublon"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Il s'agit de lire le log de changements cr\xe9\xe9 par la DB, \xe0 l'aide d'une application qui sait le faire.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'application peut \xeatre fournie par la DB, ou une application cloud-native comme ",(0,i.jsx)(t.U,{children:"AWS Database Migration Service"}),", ou une application open source comme ",(0,i.jsx)(t.U,{children:"Debezium"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Etant donn\xe9 que les DBs ne gardent pas longtemps leur log de changements, ",(0,i.jsx)(n.strong,{children:"le CDC n\xe9cessite une infrastructure de type streaming"})," pour \xeatre r\xe9cup\xe9r\xe9."]}),"\n",(0,i.jsxs)(n.li,{children:["Le format des messages r\xe9cup\xe9r\xe9s depuis le log de changements contient la valeur du row avant, sa valeur apr\xe8s l'op\xe9ration, le type d'op\xe9ration, et des metadata.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va vouloir mettre dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})," uniquement la valeur apr\xe8s l'op\xe9ration et le type d'op\xe9ration."]}),"\n",(0,i.jsxs)(n.li,{children:["La table dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})," ressemble du coup au cas de l'",(0,i.jsx)(n.em,{children:"incremental table ingestion"})," : on a une entr\xe9e par changement."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(t.U,{children:"Oracle"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Oracle fournit ",(0,i.jsx)(t.U,{children:"Oracle GoldenGate"}),", une application qui permet de lire son log de changement et de le transf\xe9rer vers diverses plateformes.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut acheter la licence pour pouvoir l'utiliser."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On peut mettre en place ",(0,i.jsx)(t.U,{children:"Debezium"})," qui est open source, mais il faudra qu'il puisse se connecter \xe0 ",(0,i.jsx)(t.U,{children:"Oracle XStream API"}),", qui lui-m\xeame n\xe9cessite quand m\xeame une licence ",(0,i.jsx)(t.U,{children:"GoldenGate"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Oracle fournit un outil d'analyse qui s'appelle ",(0,i.jsx)(n.strong,{children:"LogMiner"}),", qui est consid\xe9r\xe9 comme pas 100% fiable.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Certains outils comme ",(0,i.jsx)(t.U,{children:"AWS Database Migration Service"})," l'utilisent malgr\xe9 tout."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une alternative moins ch\xe8re \xe0 ",(0,i.jsx)(t.U,{children:"GoldenGate"})," peut \xeatre ",(0,i.jsx)(t.U,{children:"SharePlex"}),", un produit fait par Quest."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(t.U,{children:"MySQL"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"MySQL"})," \xe9crit les changements dans un log servant principalement \xe0 la r\xe9plication, pour ajouter des DBs followers."]}),"\n",(0,i.jsxs)(n.li,{children:["Vu que c'est une DB open source, il existe de nombreux outils pour servir d'application CDC \xe0 partir de ce log, par exemple : ",(0,i.jsx)(t.U,{children:"Debezium"})," et ",(0,i.jsx)(t.U,{children:"Apache NiFi"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(t.U,{children:"MS SQL Server"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"MS SQL Server"})," fournit la possibilit\xe9 de rediriger le log de changements d'une table vers une table cr\xe9\xe9e sp\xe9cialement pour \xe7a.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut donc facilement impl\xe9menter un outil CDC qui n'a qu'\xe0 utiliser SQL pour lire cette nouvelle table r\xe9guli\xe8rement."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les outils qui supportent le CDC sur ",(0,i.jsx)(t.U,{children:"MS SQL Server"}),", il y a par exemple: ",(0,i.jsx)(t.U,{children:"Debezium"}),", ",(0,i.jsx)(t.U,{children:"Apache NiFi"})," et ",(0,i.jsx)(t.U,{children:"AWS Database Migration Service"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le CDC sur une DB ",(0,i.jsx)(t.U,{children:"PostgreSQL"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"PostgreSQL"})," supporte le fait de fournir son log de changements sous un format Protobuf ou JSON, ce qui facilite le travail des applications CDC."]}),"\n",(0,i.jsxs)(n.li,{children:["Il existe de nombreux outils qui savent lire ces donn\xe9es, par exemple : ",(0,i.jsx)(t.U,{children:"Debezium"})," et ",(0,i.jsx)(t.U,{children:"AWS Database Migration Service"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le ",(0,i.jsx)(n.strong,{children:"mapping des donn\xe9es"})," depuis la DB vers le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", il va falloir faire une ",(0,i.jsx)(n.strong,{children:"analyse pour v\xe9rifier la compatibilit\xe9"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - On pr\xe9pare une liste des types de donn\xe9es support\xe9es par la DB dont on veut capturer les donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il vaut mieux prendre l'ensemble des types, en pr\xe9vision d'ajout de colonnes avec des types qui n'\xe9taient pas utilis\xe9s jusque l\xe0 par l'application."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - On pr\xe9pare une liste des types support\xe9s par le ",(0,i.jsx)(n.em,{children:"data warehouse"})," de destination, et on identifie les diff\xe9rences avec la pr\xe9c\xe9dente."]}),"\n",(0,i.jsxs)(n.li,{children:["3 - On identifie les types qui ne correspondent pas exactement, mais permetteront une conversion sans perte d'information.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple un type ",(0,i.jsx)(n.code,{children:"SMALLINT"})," sur ",(0,i.jsx)(t.U,{children:"MySQL"})," comme source, et le seul entier disponible sur ",(0,i.jsx)(t.U,{children:"Google BigQuery"})," qui est l'\xe9quivalent d'un ",(0,i.jsx)(n.code,{children:"BIGINT"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["4 - On identifie les types qui n'ont pas de correspondance satisfaisante, et pourraient mener \xe0 une perte d'information.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On essaye de voir si on ne peut pas trouver un workaround, par exemple transformer des donn\xe9es g\xe9ospatiales en string, puis utiliser du processing pour les parser au moment de la lecture."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["5 - Si on est devant une impasse, on essaye de voir s'il n'y a pas un outil de ",(0,i.jsx)(n.em,{children:"data warehouse"})," plus adapt\xe9."]}),"\n",(0,i.jsxs)(n.li,{children:["6 - Dans le cas o\xf9 notre application d'ingestion n'est pas faite \xe0 la main, on v\xe9rifie les types qu'elle supporte, et leur compatibilit\xe9 avec la source et la destination.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les auteurs conseillent de faire plusieurs PoC, et disent de ne pas faire confiance aux documentations de ces outils."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"7 - Si on \xe9crit l'application d'ingestion \xe0 la main, il faut v\xe9rifier les types support\xe9s par le driver qui nous permet d'acc\xe9der \xe0 la DB. Par exemple le driver JDBC."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les DBs ",(0,i.jsx)(n.strong,{children:"NoSQL"})," sont \xe0 traiter diff\xe9remment des DBs relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Parmi les solutions courantes :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut utiliser un outil SaaS commercial qui supporte notre DB NoSQL : dans ce cas rien de plus \xe0 faire."}),"\n",(0,i.jsx)(n.li,{children:"Impl\xe9menter l'application d'ingestion \xe0 la main, en utilisant l'API de notre DB NoSQL directement pour acc\xe9der aux donn\xe9es."}),"\n",(0,i.jsxs)(n.li,{children:["Utiliser une application CDC si c'est disponible.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple ",(0,i.jsx)(t.U,{children:"Debezium"})," supporte MongoDB."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On peut utiliser l'outil d'export de donn\xe9es de notre DB NoSQL, et le faire tourner r\xe9guli\xe8rement pour avoir un snapshot des donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"MongoDB"})," permet d'obtenir les donn\xe9es sous un format CSV ou JSON, et l'outil permet d'ajouter des requ\xeates, donc on peut avoir une colonne qui a la date de la derni\xe8re modification, et faire un ",(0,i.jsx)(n.em,{children:"incremental table ingestion"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cassandra"})," permet d'obtenir les donn\xe9es sous un format CSV, mais uniquement en mode ",(0,i.jsx)(n.em,{children:"full table ingestion"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant les ",(0,i.jsx)(n.strong,{children:"metadata li\xe9es \xe0 l'ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut sauvegarder un certain nombre de statistiques pour pouvoir ensuite faire des v\xe9rifications sur la ",(0,i.jsx)(n.strong,{children:"qualit\xe9 des donn\xe9es"})," ing\xe9r\xe9es, et du ",(0,i.jsx)(n.strong,{children:"monitoring"})," de l'ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va mettre tout \xe7a dans notre ",(0,i.jsx)(n.em,{children:"technical metadata layer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les ",(0,i.jsx)(n.strong,{children:"statistiques"})," qu'on veut :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le nom et l'adresse IP du serveur de DB."}),"\n",(0,i.jsx)(n.li,{children:"Le nom de la base de donn\xe9es ou du sch\xe9ma."}),"\n",(0,i.jsx)(n.li,{children:"Le nom de la table."}),"\n",(0,i.jsx)(n.li,{children:"Le type de DB dans le cas o\xf9 on en g\xe8re plusieurs."}),"\n",(0,i.jsxs)(n.li,{children:["Pour de l'ingestion en batch, le nombre de rows ing\xe9r\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On pourra \xe0 partir de \xe7a v\xe9rifier que l'ensemble des donn\xe9es sont arriv\xe9es \xe0 destination."}),"\n",(0,i.jsx)(n.li,{children:"On peut monitorer ce chiffre pour \xeatre alert\xe9 dans le cas d'une variation anormale."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La dur\xe9e de chaque job d'ingestion, de m\xeame que le d\xe9but et la fin de l'ingestion.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"C'est un moyen de monitorer la sant\xe9 de la pipeline."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour de l'ingestion en streaming, on prend les statistiques par ",(0,i.jsx)(n.strong,{children:"fen\xeatre temporelle"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple ins\xe9rer un row toutes les 5 mn dans notre DB de technical metadata. Plus on a besoin de r\xe9agir vite, et plus on va choisir une fen\xeatre petite."}),"\n",(0,i.jsx)(n.li,{children:"On peut aussi ajouter le nombre d'inserts, updates, deletes etc. pour chaque fen\xeatre."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Les changements dans le sch\xe9ma de la DB source, ce qui nous permettra d'\xeatre alert\xe9 et d'adapter la pipeline."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le cas des ",(0,i.jsx)(n.strong,{children:"fichiers"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les fichiers (par exemple CSV, JSON) permettent un bon d\xe9couplage entre deux syst\xe8mes."}),"\n",(0,i.jsxs)(n.li,{children:["On a en g\xe9n\xe9ral deux moyens de mettre \xe0 disposition des fichiers :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Via un serveur d\xe9di\xe9 qui expose un protocole ",(0,i.jsx)(n.strong,{children:"FTP"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Via le service de ",(0,i.jsx)(n.strong,{children:"storage d'un cloud provider"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les avantages sont l'aspect ",(0,i.jsx)(n.em,{children:"elastic"}),", et les m\xe9canismes de s\xe9curit\xe9 pr\xe9-configur\xe9s."]}),"\n",(0,i.jsx)(n.li,{children:"Le d\xe9savantage principal c'est que c'est cloud provider met en place des co\xfbts pour faire sortir la donn\xe9e de son infrastructure."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les fichiers sont ",(0,i.jsx)(n.strong,{children:"immutables"})," une fois qu'ils sont \xe9crits, ce qu'on aura besoin de tracker c'est ",(0,i.jsx)(n.strong,{children:"quels fichiers ont d\xe9j\xe0 \xe9t\xe9 ing\xe9r\xe9s"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - Une approche recommand\xe9e par les auteurs c'est d'avoir ",(0,i.jsx)(n.strong,{children:"deux dossiers"})," dans le syst\xe8me source qui met \xe0 disposition les fichiers : ",(0,i.jsx)(n.strong,{children:"incoming"})," et ",(0,i.jsx)(n.strong,{children:"processed"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'application d'ingestion va ing\xe9rer un fichier depuis ",(0,i.jsx)(n.em,{children:"incoming"}),", puis une fois que l'ingestion est termin\xe9e, elle va le copier dans ",(0,i.jsx)(n.em,{children:"processed"})," et le supprimer d'",(0,i.jsx)(n.em,{children:"incoming"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On le laisse dans ",(0,i.jsx)(n.em,{children:"processed"})," pendant quelques jours dans un but de d\xe9bug et de replay, avant de le supprimer."]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les avantages :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On n'a pas besoin de tracker quels fichiers ont \xe9t\xe9 trait\xe9s : il suffit de traiter ceux du dossier ",(0,i.jsx)(n.em,{children:"incoming"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut facilement rejouer l'ingestion en repla\xe7ant le fichier depuis ",(0,i.jsx)(n.em,{children:"processed"})," vers ",(0,i.jsx)(n.em,{children:"incoming"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Dans le cas o\xf9 l'approche des deux dossiers n'est pas possible, parce que le syst\xe8me source veut organiser autrement ses fichiers, ou qu'on n'a pas la possibilit\xe9 de les modifier, on peut mettre en place l'approche des ",(0,i.jsx)(n.strong,{children:"timestamps"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Chaque fichier va avoir un timestamp de la derni\xe8re fois qu'il a \xe9t\xe9 modifi\xe9, et on va devoir garder le timestamp le plus r\xe9cent dont on a ing\xe9r\xe9 un fichier dans le ",(0,i.jsx)(n.em,{children:"technical metadata layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Vu que le filesystem ne fournit pas de syst\xe8me d'indexation, on va devoir lire \xe0 chaque fois les metadata de l'ensemble des fichiers pour savoir s'ils sont plus r\xe9cents ou moins r\xe9cents que notre timestamp sauvegard\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut se retrouver face \xe0 un probl\xe8me de performance, surtout avec les stockages cloud de masse."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Cette m\xe9thode rend plus compliqu\xe9 le replay des fichiers : on devra possiblement modifier notre dernier timestamp sauvegard\xe9, et on aura du mal avec les fichiers qui ont le m\xeame timestamp de modification."}),"\n",(0,i.jsxs)(n.li,{children:["Certains outils comme ",(0,i.jsx)(t.U,{children:"Apache NiFi"})," impl\xe9mentent d\xe9j\xe0 ce m\xe9canisme.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faudra faire attention \xe0 faire un backup de ces donn\xe9es pour ne pas avoir \xe0 reprocesser tous les fichiers."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["3 - Une variante de l'approche des timestamps consiste \xe0 organiser les fichiers source dans une ",(0,i.jsx)(n.strong,{children:"arborescence de dossiers repr\xe9sentant la date d'ajout"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Exemple :","\n",(0,i.jsx)(n.pre,{"data-language":"bash","data-theme":"default",children:(0,i.jsx)(n.code,{"data-language":"bash","data-theme":"default",children:(0,i.jsx)(n.span,{className:"line",children:(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"/ftp/inventory_data/incoming/2019/05/28/sales_1_081232"})})})}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"On peut s'en servir pour ne lire que les metadata des fichiers qui sont \xe0 la date qu'on veut ing\xe9rer."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["4 - Des ",(0,i.jsx)(n.strong,{children:"outils cloud-native"})," existent pour copier des fichiers d'un storage \xe0 un autre, et de ne copier que les nouveaux fichiers \xe0 chaque fois qu'il y en a.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"gsutil"})," permet de le faire chez Google Cloud, ",(0,i.jsx)(t.U,{children:"blobxfer"})," chez Azure, et ",(0,i.jsx)(t.U,{children:"s3 sync"})," chez AWS."]}),"\n",(0,i.jsx)(n.li,{children:"Il est compliqu\xe9 de faire du replay avec ces outils, parce qu'il n'y a pas de dernier timestamp stock\xe9 \xe0 modifier."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant les ",(0,i.jsx)(n.strong,{children:"metadata techniques \xe0 garder"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On ne va pas \xe0 ce stade r\xe9cup\xe9rer de statistiques sur le nombre de rows dans le fichier, parce que ce serait techniquement co\xfbteux pour l'ingestion layer.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On le fait pour les DBs parce que c'est pas cher."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les ",(0,i.jsx)(n.strong,{children:"statistiques \xe0 r\xe9cup\xe9rer"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Nom permettant d'identifier la source."}),"\n",(0,i.jsx)(n.li,{children:"Taille du fichier."}),"\n",(0,i.jsx)(n.li,{children:"Dur\xe9e de l'ingestion."}),"\n",(0,i.jsx)(n.li,{children:"Le nom du fichier et le path o\xf9 il \xe9tait (peut contenir des infos importantes)."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le cas des ",(0,i.jsx)(n.strong,{children:"streams"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit ici de lire de la donn\xe9e disponible dans Kafka, ou encore dans un \xe9quivalent cloud-native."}),"\n",(0,i.jsxs)(n.li,{children:["On parle ici seulement d'",(0,i.jsx)(n.strong,{children:"ingestion en mode streaming"}),", c'est-\xe0-dire que la donn\xe9e est disponible dans la plateforme d\xe8s que possible, mais elle sera exploit\xe9e plus tard."]}),"\n",(0,i.jsxs)(n.li,{children:["Les \xe9tapes \xe0 mettre en place sont :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - La 1\xe8re \xe9tape est de ",(0,i.jsxs)(n.strong,{children:["lire le stream source, et de l'\xe9crire dans le ",(0,i.jsx)(n.em,{children:"fast storage"})]})," de notre ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", qui est aussi un stream.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut faire \xe7a avec ",(0,i.jsx)(t.U,{children:"Kafka Connect"}),", qui permet de lire et \xe9crire entre deux topics ",(0,i.jsx)(t.U,{children:"Kafka"}),", mais aussi de lire depuis ",(0,i.jsx)(t.U,{children:"Kafka"})," et \xe9crire dans une solution de streaming cloud-native, ou l'inverse."]}),"\n",(0,i.jsxs)(n.li,{children:["On peut aussi faire notre propre application ",(0,i.jsx)(n.strong,{children:"consumer Kafka \xe0 la main"}),", mais il faudra alors s'occuper de la gestion des erreurs, du logging, et du scaling de notre consumer. Les auteurs le ",(0,i.jsx)(n.strong,{children:"d\xe9conseillent"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - On va ensuite ",(0,i.jsxs)(n.strong,{children:["l'\xe9crire dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})]}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs conseillent fortement d'utiliser une solution ",(0,i.jsx)(n.strong,{children:"cloud-native"})," pour \xe7a, en fonction de notre fast storage :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Stream Analytics"})," qui lit depuis ",(0,i.jsx)(t.U,{children:"Azure Event Hubs"})," pour \xe9crire dans ",(0,i.jsx)(t.U,{children:"Azure SQL Warehouse"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Google Cloud Dataflow"})," qui lit depuis ",(0,i.jsx)(t.U,{children:"Cloud Pub/Sub"})," pour \xe9crire dans ",(0,i.jsx)(t.U,{children:"BigQuery"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Kinesis Data Firehose"})," qui lit depuis ",(0,i.jsx)(t.U,{children:"AWS Kinesis"})," pour \xe9crire dans ",(0,i.jsx)(t.U,{children:"Redshift"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour ",(0,i.jsx)(t.U,{children:"BigQuery"})," on peut ing\xe9rer dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})," en streaming, mais pour les deux autres, il faudra faire de petits batchs."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["3 - L'autre chose \xe0 faire en parall\xe8le c'est d'\xe9crire la donn\xe9e ",(0,i.jsxs)(n.strong,{children:["depuis le ",(0,i.jsx)(n.em,{children:"fast storage"})," vers le ",(0,i.jsx)(n.em,{children:"slow storage"})]}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut l\xe0 aussi utiliser les solutions cloud-natives."}),"\n",(0,i.jsxs)(n.li,{children:["Il va falloir \xe9crire la donn\xe9e par batchs pour des raisons de performance. Les auteurs recommandent des ",(0,i.jsx)(n.strong,{children:"batchs de plusieurs centaines de MB"})," si c'est possible."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Kafka"})," (et les solutions cloud-natives similaires) doit faire le commit de son offset, et en g\xe9n\xe9ral il le fait apr\xe8s avoir trait\xe9 plusieurs messages pour des raisons de performance;","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\xc7a veut dire que si il y a un crash, les message trait\xe9s mais non commit\xe9s seront trait\xe9s \xe0 nouveau. Donc il faut ",(0,i.jsx)(n.strong,{children:"g\xe9rer la duplication"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Un des moyens de le faire c'est d'avoir un identifiant unique par message, et ensuite d'enlever les doublons dans la phase de processing."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Kafka"})," a en g\xe9n\xe9ral une ",(0,i.jsx)(n.em,{children:"cleanup policy"})," qui est de l'ordre de la semaine, ce qui fait que pour ",(0,i.jsx)(n.strong,{children:"rejouer de la donn\xe9e"}),", il faut pr\xe9voir une \xe9tape qui va la chercher dans le ",(0,i.jsx)(n.em,{children:"slow storage"}),", et la remet dans le ",(0,i.jsx)(n.em,{children:"fast storage"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant les ",(0,i.jsx)(n.strong,{children:"metadata techniques \xe0 garder"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les metadata \xe0 garder ressemblent \xe0 ceux du cas CDC depuis les DBs relationnelles."}),"\n",(0,i.jsx)(n.li,{children:"On mesure le nombre de messages ing\xe9r\xe9s par fen\xeatre de temps (dont la taille d\xe9pendra du type de donn\xe9es ing\xe9r\xe9es)."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le cas des ",(0,i.jsx)(n.strong,{children:"applications SaaS"})," qui fournissent de la donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les applications SaaS vont en g\xe9n\xe9ral exposer leurs donn\xe9es via une API REST, le contenu \xe9tant format\xe9 en JSON ou parfois en XML.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut d'abord s'authentifier, souvent avec OAuth."}),"\n",(0,i.jsx)(n.li,{children:"Et ensuite il faut \xe9tudier la documentation du provider SaaS pour savoir quel call faire."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il y a un certain nombre de difficult\xe9s.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Chaque provider va ",(0,i.jsx)(n.strong,{children:"designer son API selon ses contraintes"}),". Et donc si on veut supporter de nombreux providers, il va falloir adapter l'",(0,i.jsx)(n.em,{children:"ingestion layer"})," pour chacun d'entre eux."]}),"\n",(0,i.jsxs)(n.li,{children:["Chaque provider va fournir soit du ",(0,i.jsx)(n.strong,{children:"full data export"})," soit de l'",(0,i.jsx)(n.strong,{children:"incremental data export"}),", et parfois les deux.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.em,{children:"full data export"})," consiste \xe0 obtenir une liste d'objets, puis \xe0 aller chercher les donn\xe9es pour chacun d'entre eux."]}),"\n",(0,i.jsxs)(n.li,{children:["L'",(0,i.jsx)(n.em,{children:"incremental data export"})," consiste \xe0 obtenir une liste d'objets qui ont chang\xe9 entre deux timestamps qu'on fournit, pour ensuite aller chercher leurs donn\xe9es r\xe9centes uniquement."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(t.U,{children:"JSON"})," re\xe7u est en g\xe9n\xe9ral ",(0,i.jsx)(n.strong,{children:"imbriqu\xe9 sur plusieurs niveaux"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Certains ",(0,i.jsx)(n.em,{children:"data warehouses"})," g\xe8rent les donn\xe9es imbriqu\xe9es, mais ce n'est pas le cas de ",(0,i.jsx)(t.U,{children:"Redshift"})," pour lequel il faudra faire une \xe9tape de processing pour mettre ces donn\xe9es \xe0 plat."]}),"\n",(0,i.jsx)(n.li,{children:"De mani\xe8re g\xe9n\xe9rale, mettre les donn\xe9es \xe0 plat dans plusieurs tables plus petites est plus pratique pour les data scientists."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Etant donn\xe9 la difficult\xe9 \xe0 impl\xe9menter et maintenir une pipeline ing\xe9rant de la donn\xe9e de sources SaaS, les auteurs conseillent de ",(0,i.jsx)(n.strong,{children:"bien r\xe9fl\xe9chir \xe0 l'impl\xe9menter soi-m\xeame"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"S'il s'agit d'une source pas trop compliqu\xe9e, \xe7a peut passer."}),"\n",(0,i.jsxs)(n.li,{children:["Si par contre il s'agit de nombreuses sources, alors il nous faudra une grande quantit\xe9 de code et de maintenance.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs conseillent plut\xf4t une solution off-the-shelf comme ",(0,i.jsx)(n.strong,{children:"Fivetran"})," qui supporte la plupart des sources SaaS connues."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant les ",(0,i.jsx)(n.strong,{children:"metadata techniques \xe0 garder"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit du m\xeame type de metadata que pour les sources en batch comme les DBs ou les fichiers."}),"\n",(0,i.jsxs)(n.li,{children:["On voudra notamment :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Le nom de la source."}),"\n",(0,i.jsx)(n.li,{children:"Le nom de l'objet qu'on va chercher dans la source."}),"\n",(0,i.jsx)(n.li,{children:"Les temps de d\xe9but et fin d'ingestion."}),"\n",(0,i.jsx)(n.li,{children:"Le nombre de rows qu'on r\xe9cup\xe8re."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour des questions de ",(0,i.jsx)(n.strong,{children:"s\xe9curit\xe9"}),", il est pr\xe9f\xe9rable d'encapsuler notre cloud data platform dans un ",(0,i.jsx)(n.strong,{children:"virtual private cloud (VPC)"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour faire le lien entre la plateforme dans le VPC et la donn\xe9e qu'on veut aller chercher, on peut utiliser un ",(0,i.jsx)(n.strong,{children:"VPN Gateway"}),", qui permet de passer par internet de mani\xe8re s\xe9curis\xe9e."]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas des SaaS comme source, ils fournissent des APIs s\xe9curis\xe9es par HTTPS, et disponibles globalement sur internet, donc il n'est pas n\xe9cessaire d'\xe9tablir une connexion via ",(0,i.jsx)(n.em,{children:"VPN Gateway"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 on veut transf\xe9rer des centaines de GB par jour, il vaut mieux mettre en place une ",(0,i.jsx)(n.strong,{children:"connexion directe"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les solutions cloud-natives ont leur outil de connexion directe : ",(0,i.jsx)(t.U,{children:"AWS Direct Connect"}),", ",(0,i.jsx)(t.U,{children:"Azure ExpressRoute"}),", ",(0,i.jsx)(t.U,{children:"Google Cloud Interconnect"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"5---organizing-and-processing-data",children:"5 - Organizing and processing data"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les architectes de l'ancienne \xe9cole ont encore tendance \xe0 recommander de ",(0,i.jsx)(n.strong,{children:"faire le processing dans le data warehouse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs du livre sugg\xe8rent que la mani\xe8re moderne est de le faire ",(0,i.jsx)(n.strong,{children:"sur des machines \xe0 part"}),", par exemple avec ",(0,i.jsx)(t.U,{children:"Spark"}),", qui lirait et \xe9crirait dans le ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les arguments sont les suivants ((1) pour faire le calcul dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", et (2) pour utiliser la ",(0,i.jsx)(n.em,{children:"layered architecture"}),") :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Flexibility"})," : avec la (1) le r\xe9sultat du processing n'est utilisable que dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", avec la (2) on peut facilement le rediriger ailleurs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Developer productivity"})," : il y a plus de personnes qui connaissent le SQL, donc le (1) a un avantage court terme, bien que ",(0,i.jsx)(t.U,{children:"Spark"})," soit plus puissant, il faut souvent former les devs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data governance"})," : la source principale \xe9tant le ",(0,i.jsx)(n.em,{children:"data lake"}),", faire les transformations au m\xeame endroit permet d'\xeatre s\xfbr d'avoir toutes les versions align\xe9es. Dans le cas o\xf9 on fait \xe7a dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", il est pr\xe9f\xe9rable de ne pas le faire dans le ",(0,i.jsx)(n.em,{children:"data lake"})," pour ne pas avoir de divergence."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cross-platform portability"})," : changer de cloud vendor est bien plus simple avec ",(0,i.jsx)(t.U,{children:"Spark"})," qu'avec du code SQL qu'il faudra au moins tester."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance"})," : avec la (1) le processing impacte le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", avec le (2) on fait le calcul compl\xe8tement \xe0 part et on n'impacte personne."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed of processing"})," : avec la (1) on peut faire du ",(0,i.jsx)(n.em,{children:"real time analytics"})," dans certains cas avec difficult\xe9, avec la (2) \xe7a marche facilement."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost"})," : tous les providers de ",(0,i.jsx)(n.em,{children:"data warehouse"})," ne le font pas (mais ils vont finir par le faire), mais pour ceux qui font payer le processing \xe7a revient plus cher que de faire le processing sur des machines compl\xe8tement \xe0 part."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reusability"})," : avec la (1) on peut parfois utiliser des ",(0,i.jsx)(n.em,{children:"stored procedures"}),", avec la (2) on a du code qu'on peut directement r\xe9utiliser."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le processing se d\xe9compose en ",(0,i.jsx)(n.strong,{children:"stages"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Chaque ",(0,i.jsx)(n.em,{children:"stage"})," contient : une ",(0,i.jsx)(n.em,{children:"area"})," de stockage dans le ",(0,i.jsx)(n.em,{children:"data lake"}),", et un job de calcul distribu\xe9 (par exemple avec ",(0,i.jsx)(t.U,{children:"Spark"}),"), qui va cr\xe9er la donn\xe9e pour l'\xe9tape suivante.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les jobs sont coordonn\xe9s par l'",(0,i.jsx)(n.em,{children:"orchestration layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les jobs peuvent \xeatre de deux types :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Common"})," data processing : les transformations communes, par exemple d\xe9dupliquer les messages, valider les dates etc."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Business logic specific"})," processing : les transformations sp\xe9cifiques \xe0 chaque use-case, qui vont par exemple filtrer les campagnes de marketing \xe0 succ\xe8s uniquement si le use-case c'est d'afficher les meilleures campagnes."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Avoir un ensemble de ",(0,i.jsx)(n.em,{children:"stages"})," standardis\xe9s est important pour que chacun puisse s'y retrouver malgr\xe9 le scale."]}),"\n",(0,i.jsxs)(n.li,{children:["Les \xe9tapes propos\xe9s par les auteurs sont :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Landing area"})," : c'est l\xe0 que la donn\xe9e arrive en premier, il ne s'agit pas d'un stockage long terme."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Staging area"})," : la donn\xe9e subit des checks basiques de qualit\xe9, et on v\xe9rifie qu'elle est conforme au sch\xe9ma attendu. Elle est stock\xe9e sous format ",(0,i.jsx)(t.U,{children:"Avro"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - Archive area"})," : la donn\xe9e est copi\xe9e depuis la ",(0,i.jsx)(n.em,{children:"landing area"})," vers l'",(0,i.jsx)(n.em,{children:"archive area"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Cette op\xe9ration n'est effectu\xe9e qu'apr\xe8s que la donn\xe9e ait pu aller vers la ",(0,i.jsx)(n.em,{children:"staging area"})," avec succ\xe8s."]}),"\n",(0,i.jsxs)(n.li,{children:["On pourra refaire le processing de la donn\xe9e simplement en la copiant depuis l'",(0,i.jsx)(n.em,{children:"archive area"})," vers la ",(0,i.jsx)(n.em,{children:"landing area"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - Production area"})," : la donn\xe9e subit la transformation business n\xe9cessaire pour un use-case particulier avant d'aller l\xe0.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Elle est aussi transform\xe9e du format ",(0,i.jsx)(t.U,{children:"Avro"})," vers ",(0,i.jsx)(t.U,{children:"Parquet"}),", qui est plus adapt\xe9 pour faire de l'analytics."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4.1 - Pass-through job"})," : il s'agit d'un job qui copie la donn\xe9e de la ",(0,i.jsx)(n.em,{children:"staging area"})," vers la ",(0,i.jsx)(n.em,{children:"production area"})," sans transformation autre que le format ",(0,i.jsx)(t.U,{children:"Parquet"}),", et ensuite la copie dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ce use-case “basique” est utile pour d\xe9bugguer les autres use-cases."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4.2 - Cloud data warehouse and production area"})," : les use-cases qui ont besoin de la donn\xe9e dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})," passent d'abord par le processing de la ",(0,i.jsx)(n.em,{children:"staging area"})," vers la ",(0,i.jsx)(n.em,{children:"production area"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"5 - Failed area"})," : chaque \xe9tape peut faire face \xe0 des erreurs, qu'elles soient li\xe9es \xe0 la donn\xe9e ou \xe0 des \xe9checs temporaires de la pipeline.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les messages qui n'ont pas r\xe9ussi une \xe9tape vont dans cette area o\xf9 on pourra les examiner et voir ce qu'il faut corriger."}),"\n",(0,i.jsx)(n.li,{children:"Une fois la correction faite, il suffit de les copier dans l'area de l'\xe9tape o\xf9 ils ont \xe9chou\xe9s."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Chaque ",(0,i.jsx)(n.strong,{children:"area"})," doit \xeatre dans un ",(0,i.jsx)(n.strong,{children:"container"})," du service de stockage de notre cloud provider.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"containers"})," contiennent des ",(0,i.jsx)(n.em,{children:"folders"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Ils sont appel\xe9s ",(0,i.jsx)(n.em,{children:"buckets"})," chez AWS et GCP."]}),"\n",(0,i.jsxs)(n.li,{children:["C'est au niveau de ces containers qu'on peut configurer les droits d'acc\xe8s, et choisir le prix qu'on paye pour les performances qu'on aura (hot / cold / archive storage).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Parmi nos 5 areas, toutes sont de type ",(0,i.jsx)(n.em,{children:"hot"}),", sauf l'archive area qui peut \xeatre ",(0,i.jsx)(n.em,{children:"cold"})," / ",(0,i.jsx)(n.em,{children:"archive"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On a besoin d'une ",(0,i.jsx)(n.strong,{children:"organisation des folders"})," claire dans chaque ",(0,i.jsx)(n.em,{children:"area"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les \xe9l\xe9ments communs sont :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"namespace"})," repr\xe9sente la cat\xe9gorisation la plus high level, pour les petites organisations \xe7a peut \xeatre juste le nom de l'organisation, mais pour les plus grandes \xe7a peut \xeatre le nom du d\xe9partement."]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"pipeline name"})," repr\xe9sente le nom d'un job en particulier. Il faut qu'il soit clair par rapport \xe0 ce que fait le job, et utilis\xe9 partout pour parler de lui."]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"data source name"})," identifie une source. C'est l'",(0,i.jsx)(n.em,{children:"ingestion layer"})," qui choisit ce nom et le note dans le ",(0,i.jsx)(n.em,{children:"metadata layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"batchId"})," repr\xe9sente l'identifiant de chaque batch de donn\xe9e \xe9crit dans la ",(0,i.jsx)(n.em,{children:"landing area"})," par l'",(0,i.jsx)(n.em,{children:"ingestion layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut utiliser un UUID pour le repr\xe9senter, ou encore un ULID, qui a la particularit\xe9 d'\xeatre plus court et de permettre de savoir facilement si un autre ULID est plus grand ou plus petit."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la ",(0,i.jsx)(n.em,{children:"landing area"}),", les auteurs proposent la folder structure :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"landing/NAMESPACE/PIPELINE/SOURCE_NAME/BATCH_ID/"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"landing"})," repr\xe9sente le nom du container."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Exemple : ",(0,i.jsx)(n.code,{children:"/landing/my_company/sales_oracle_ingest/customers/01DFTQ028FX89YDFAXREPJTR94/"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la ",(0,i.jsx)(n.em,{children:"staging area"}),", de m\xeame que pour les autres ",(0,i.jsx)(n.em,{children:"areas"}),", il s'agit de stocker la donn\xe9e sur le long terme, donc on aimerait une structure qui fasse appara\xeetre le ",(0,i.jsx)(n.strong,{children:"temps"}),", avec 3 folders suppl\xe9mentaires :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s'agit d'ajouter 3 folders suppl\xe9mentaires qui viennent de la convention de ",(0,i.jsx)(t.U,{children:"Hadoop"})," : ",(0,i.jsx)(n.code,{children:"year=YYYY/month=MM/day=DD"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Exemple : ",(0,i.jsx)(n.code,{children:"/staging/my_company/sales_oracle_ingest/customers/year=2019/month=07/day=03/01DFTQ028FX89YDFAXREPJTR94/"})]}),"\n",(0,i.jsxs)(n.li,{children:["De nombreux outils (y compris ",(0,i.jsx)(t.U,{children:"Spark"}),") vont reconna\xeetre ce format, et si notre batchId est un ULID, les folders les plus r\xe9cents seront pr\xe9sent\xe9s en premier."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la ",(0,i.jsx)(n.em,{children:"production area"}),", on ne peut pas vraiment reporter les sources qui ont servi \xe0 la donn\xe9e dans le folder name - il y en a potentiellement des dizaines.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On va donc plut\xf4t cr\xe9er des ",(0,i.jsx)(n.strong,{children:"sources d\xe9riv\xe9es"})," dont on mettra le nom \xe0 la place de la source, et on documentera ces sources d\xe9riv\xe9es dans le ",(0,i.jsx)(n.em,{children:"metadata layer"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La donn\xe9e qui arrive en ",(0,i.jsx)(n.strong,{children:"streaming"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Va passer directement vers la version streaming du ",(0,i.jsx)(n.em,{children:"processing layer"})," sans \xeatre stock\xe9e d'abord dans le ",(0,i.jsx)(n.em,{children:"slow storage"}),". C'est trait\xe9 au chapitre 6."]}),"\n",(0,i.jsxs)(n.li,{children:["Mais on va quand m\xeame l'envoyer dans le slow storage en parall\xe8le pour un but d'archivage et rejeu si besoin.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Un job va lire depuis le fast storage o\xf9 arrive la donn\xe9e en streaming, par batchs suffisamment gros, et va \xe9crire \xe7a dans la ",(0,i.jsx)(n.em,{children:"landing area"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Les fichiers seront ensuite pass\xe9s de stage en stage jusqu'\xe0 la production area."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les ",(0,i.jsx)(n.strong,{children:"common processing steps"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"File format conversion."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'approche ",(0,i.jsx)(n.em,{children:"data lake"})," traditionnelle consiste \xe0 laisser les donn\xe9es telles quelles, et laisser chaque pipeline parser elle-m\xeame la donn\xe9e et faire les traitements dont elle a besoin.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Mais cette approche a du mal \xe0 scaler."}),"\n",(0,i.jsxs)(n.li,{children:["Dans la ",(0,i.jsx)(n.em,{children:"cloud data platform architecture"}),", on choisit de faire certains traitements en amont, pour \xe9viter d'avoir \xe0 tester et maintenir du code qui fait \xe7a dans chaque pipeline."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Avro"})," et ",(0,i.jsx)(t.U,{children:"Parquet"})," sont des formats binaires int\xe9grant un sch\xe9ma.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ils permettent de ne pas r\xe9p\xe9ter le nom des champs, et donc d'\xe9conomiser de la place."}),"\n",(0,i.jsx)(n.li,{children:"Ils permettent de garantir le sch\xe9ma de la donn\xe9e."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Avro"})," est organis\xe9 en blocs de ",(0,i.jsx)(n.em,{children:"rows"}),", alors que ",(0,i.jsx)(t.U,{children:"Parquet"})," est organis\xe9 en blocs de ",(0,i.jsx)(n.em,{children:"columns"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les fichiers organis\xe9s en ",(0,i.jsx)(n.em,{children:"rows"})," sont utiles quand on lit la donn\xe9e de toutes les colonnes pour certains ",(0,i.jsx)(n.em,{children:"rows"})," donn\xe9s. La ",(0,i.jsx)(n.em,{children:"staging area"})," sert principalement \xe0 faire des transformations ou de l'exploration ad-hoc, donc ",(0,i.jsx)(t.U,{children:"Avro"})," est adapt\xe9."]}),"\n",(0,i.jsxs)(n.li,{children:["Les fichiers organis\xe9s en ",(0,i.jsx)(n.em,{children:"columns"})," sont utiles quand on ne veut traiter qu'une ",(0,i.jsx)(n.em,{children:"column"})," sur un grand nombre de ",(0,i.jsx)(n.em,{children:"rows"}),". La production area sert \xe0 faire des requ\xeates d'analytics, donc ",(0,i.jsx)(t.U,{children:"Parquet"})," est adapt\xe9."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour la conversion depuis le format initial vers ",(0,i.jsx)(t.U,{children:"Avro"}),", puis vers ",(0,i.jsx)(t.U,{children:"Parquet"}),", ",(0,i.jsx)(t.U,{children:"Spark"})," permet de lire et \xe9crire ces diff\xe9rents formats.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Exemple :","\n",(0,i.jsx)(n.pre,{"data-language":"python","data-theme":"default",children:(0,i.jsxs)(n.code,{"data-language":"python","data-theme":"default",children:[(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"clicks_df "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" spark"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"read"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"json"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(in_path)"})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"clicks_df "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" spark"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"write"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"format"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"avro"'}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"save"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(out_path)"})]})]})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data deduplication"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On s'int\xe9resse ici au fait d'avoir un attribut sur notre donn\xe9e qui soit unique dans l'ensemble des donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A partir du moment o\xf9 on n'a pas de garanties d'unicit\xe9, on peut se retrouver dans une situation de duplication, par exemple si le ",(0,i.jsx)(n.em,{children:"metadata repository"})," est corrompu, si la source envoie une donn\xe9e dupliqu\xe9e, ou encore si un dev rejoue certaines donn\xe9es qui avaient d\xe9j\xe0 march\xe9."]}),"\n",(0,i.jsxs)(n.li,{children:["Le probl\xe8me existe aussi avec ",(0,i.jsx)(t.U,{children:"Kafka"}),", o\xf9 des transactions existent si on lit un record et qu'on \xe9crit dans un topic ",(0,i.jsx)(t.U,{children:"Kafka"}),", mais pas si on \xe9crit sur un service de storage."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Spark"})," a une fonction int\xe9gr\xe9e ",(0,i.jsx)(n.code,{children:"dropDuplicates()"})," qui permet de d\xe9dupliquer en fonction d'une ou plusieurs colonnes.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut d\xe9dupliquer sur un batch qui arrive dans la ",(0,i.jsx)(n.em,{children:"landing area"})," pour pas cher :","\n",(0,i.jsx)(n.pre,{"data-language":"python","data-theme":"default",children:(0,i.jsxs)(n.code,{"data-language":"python","data-theme":"default",children:[(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"users_df "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" spark"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"read"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"format"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"csv"'}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"load"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(in_path)"})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"users_deduplicate_df "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"  users_df"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"dropDuplicates"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(["}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"user_id"'}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"])"})]})]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Si on veut vraiment d\xe9dupliquer s\xe9rieusement, il faut aussi joindre l'ensemble des donn\xe9es d\xe9j\xe0 pr\xe9sentes dans la ",(0,i.jsx)(n.em,{children:"staging area"})," au batch courant, et appliquer la d\xe9duplication dessus, par exemple avec du SQL qu'on passe \xe0 ",(0,i.jsx)(t.U,{children:"Spark"}),".","\n",(0,i.jsx)(n.pre,{"data-language":"python","data-theme":"default",children:(0,i.jsxs)(n.code,{"data-language":"python","data-theme":"default",children:[(0,i.jsx)(n.span,{className:"line",children:(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"incoming_users_df"})}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"  "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"createOrReplaceTempView"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"incomgin_users"'}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"staging"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"users_df"})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"  "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"createOrReplaceTempView"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"staging_users"'}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"users_deduplicate_df "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" spark"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"sql"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"  "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"SELECT * FROM incoming_users u1'})]}),"\n",(0,i.jsx)(n.span,{className:"line",children:(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"  LEFT JOIN staging_users u2"})}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"  ON u1.user_id "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" u2.user_id"})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"  WHERE u2.user_id IS NULL"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"'})]}),"\n",(0,i.jsx)(n.span,{className:"line",children:(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})})]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le probl\xe8me c'est que la d\xe9duplication \xe0 chaque fois avec l'ensemble des donn\xe9es co\xfbte cher. Donc il faut v\xe9rifier que notre use-case le n\xe9cessite d'un point de vue business.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut aussi d\xe9dupliquer avec seulement les fichiers dans le dossier de l'ann\xe9e actuelle, du mois actuel etc. depuis la ",(0,i.jsx)(n.em,{children:"staging area"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data quality checks"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Une v\xe9rification minimale de la qualit\xe9 de la donn\xe9e est en g\xe9n\xe9ral n\xe9cessaire pour la plupart des cas d'usages. Par exemple :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La longueur de certaines colonnes."}),"\n",(0,i.jsx)(n.li,{children:"La valeur num\xe9rique acceptable de certaines colonnes."}),"\n",(0,i.jsx)(n.li,{children:"Le fait d'avoir certaines colonnes “obligatoires”."}),"\n",(0,i.jsx)(n.li,{children:"Le fait d'avoir certaines colonnes respecter un pattern, par exemple l'email."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Spark"})," a la fonction ",(0,i.jsx)(n.code,{children:"filter()"})," qui permet d'obtenir les colonnes qui respectent une mauvaise condition.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On a aussi ",(0,i.jsx)(n.code,{children:"subtract()"})," qui permet d'enlever ces rows du batch, pour passer les rows valides \xe0 la ",(0,i.jsx)(n.em,{children:"production area"}),", et les rows invalides \xe0 la ",(0,i.jsx)(n.em,{children:"failed area"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Attention \xe0 la ",(0,i.jsx)(n.strong,{children:"consistance des donn\xe9es"}),", en fonction du contexte business, il peut \xeatre plus judicieux de laisser passer la donn\xe9e, et de simplement informer les data engineers du probl\xe8me."]}),"\n",(0,i.jsxs)(n.li,{children:["De mani\xe8re g\xe9n\xe9rale, il faut r\xe9fl\xe9chir \xe0 ",(0,i.jsx)(n.strong,{children:"la criticit\xe9 de chaque probl\xe8me de qualit\xe9"})," pour d\xe9cider quoi faire en cas de donn\xe9e malform\xe9e : filtrer la donn\xe9e, laisser passer et pr\xe9venir quelqu'un, ou annuler l'ingestion du batch entier."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Exemple :","\n",(0,i.jsx)(n.pre,{"data-language":"python","data-theme":"default",children:(0,i.jsxs)(n.code,{"data-language":"python","data-theme":"default",children:[(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"users_df "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" spark"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"read"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"format"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"csv"'}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"load"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(in_path)"})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"bad_user_rows "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"  users_df"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"filter"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("})]}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"    "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"length(email) > 100 OR username IS NULL"'})]}),"\n",(0,i.jsx)(n.span,{className:"line",children:(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"  )"})}),"\n",(0,i.jsxs)(n.span,{className:"line",children:[(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"users_df "}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" users_df"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"subtract"}),(0,i.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(bad_user_rows)"})]})]})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On peut cr\xe9er des ",(0,i.jsx)(n.strong,{children:"jobs configurables"})," : l'orchestration layer lance un job, en lui donnant d'abord la configuration contenant les sources \xe0 traiter, le sch\xe9ma \xe0 valider en fonction des sources, la folder structure o\xf9 ins\xe9rer les nouveaux fichiers etc.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\xc7a permet d'\xe9conomiser du code, au moins pour les jobs de transformation “common”."}),"\n",(0,i.jsxs)(n.li,{children:["Le bon endroit pour la configuration c'est le ",(0,i.jsx)(n.em,{children:"metadata layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour d\xe9clencher nos jobs, il faut qu'il y ait une forme de monitoring de la ",(0,i.jsx)(n.em,{children:"landing area"}),", soit avec du code qu'on \xe9crit nous-m\xeames, soit avec la fonctionnalit\xe9 de monitoring d'un outil d'orchestration cloud."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"6---real-time-data-processing-and-analytics",children:"6 - Real-time data processing and analytics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La notion de ",(0,i.jsx)(n.strong,{children:"real-time"})," (ou ",(0,i.jsx)(n.strong,{children:"streaming"}),") dans le contexte d'une pipeline data peut recouvrir deux choses diff\xe9rentes :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - real-time ingestion"})," : on ing\xe8re la donn\xe9e une par une avec un m\xe9canisme de message streaming, et on l'am\xe8ne jusqu'au ",(0,i.jsx)(n.em,{children:"data warehouse"}),". Mais la consommation de la donn\xe9e ne se fait pas en temps r\xe9el.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'aspect “real-time” ne concerne que l'",(0,i.jsx)(n.em,{children:"ingestion layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Le processing se fait \xe0 la demande, et peut prendre des secondes voire des minutes"}),", mais il se fait sur une donn\xe9e fra\xeeche."]}),"\n",(0,i.jsx)(n.li,{children:"Il peut se faire selon un schedule, ou \xe0 la demande des utilisateurs humains qui attendront un peu avant d'avoir un r\xe9sultat."}),"\n",(0,i.jsx)(n.li,{children:"Exemple : un data analyste veut pouvoir ex\xe9cuter une requ\xeate pour afficher un dashboard sur des donn\xe9es fra\xeeches quand il en a besoin. Le dashboard n'est pas mis \xe0 jour en continu mais juste \xe0 l'ex\xe9cution de cette requ\xeate."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - real-time processing"})," : on r\xe9cup\xe8re la donn\xe9e une par une, et on la redirige vers un autre syst\xe8me qui va r\xe9agir \xe0 chaque donn\xe9e qui arrive pour se mettre \xe0 jour.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.em,{children:"real-time processing"})," n\xe9cessite la ",(0,i.jsx)(n.em,{children:"real-time ingestion"}),". L'aspect “real-time” concerne donc l'",(0,i.jsx)(n.em,{children:"ingestion layer"})," et le ",(0,i.jsx)(n.em,{children:"processing layer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["On est dans un cas d'usage o\xf9 on a besoin que ",(0,i.jsx)(n.strong,{children:"le processing se fasse tr\xe8s vite et en continu"}),", en g\xe9n\xe9ral \xe0 destination d'un autre syst\xe8me."]}),"\n",(0,i.jsx)(n.li,{children:"Exemple : la donn\xe9e qui arrive dans la pipeline est ensuite mise \xe0 disposition d'un syst\xe8me de jeu vid\xe9o pour adapter le comportement du jeu en fonction de ce que fait le joueur en temps r\xe9el. Par exemple, ajuster la probabilit\xe9 de faire appara\xeetre un monstre."}),"\n",(0,i.jsxs)(n.li,{children:["La donn\xe9e est trait\xe9e par un ",(0,i.jsx)(n.strong,{children:"real-time job"})," qui tourne en permanence et ajuste les calculs en fonction des nouvelles donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Elle est ensuite mise \xe0 disposition d'un ",(0,i.jsx)(n.em,{children:"key/value store"})," ou \xe9ventuellement d'une DB relationnelle, pour un acc\xe8s rapide. Le ",(0,i.jsx)(n.em,{children:"data warehouse"})," est trop lent et est fait pour des requ\xeates \xe0 la demande sur de grandes quantit\xe9s de donn\xe9es."]}),"\n",(0,i.jsxs)(n.li,{children:["Elle peut aussi \xeatre post\xe9e dans le ",(0,i.jsx)(n.em,{children:"fast storage"}),", c'est-\xe0-dire comme event de streaming pour d\xe9clencher un autre processing."]}),"\n",(0,i.jsxs)(n.li,{children:["C'est parce que ce job tourne en continu avec des choses charg\xe9es en RAM qu'il donne un r\xe9sultat rapide, contrairement \xe0 une requ\xeate SQL dans un ",(0,i.jsx)(n.em,{children:"data warehouse"})," par exemple, qui ne se d\xe9clenche qu'au moment o\xf9 on la lance."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il est tr\xe8s important de ",(0,i.jsx)(n.strong,{children:"clarifier le besoin"})," : dans le cas o\xf9 on n'a besoin que de ",(0,i.jsx)(n.em,{children:"real-time ingestion"}),", la complexit\xe9 de mise en œuvre est beaucoup moins grande.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs conseillent d'utiliser la ",(0,i.jsx)(n.em,{children:"real-time ingestion"})," plut\xf4t que la ",(0,i.jsx)(n.em,{children:"batch ingestion"}),", sauf quand la source ne supporte pas le real time.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"real-time ingestion"})," implique moins de n\xe9cessit\xe9 d'orchestration et de monitoring."]}),"\n",(0,i.jsx)(n.li,{children:"Pour \xe9viter l'incoh\xe9rence pour les utilisateurs, il vaut mieux \xe9viter de mixer des donn\xe9es real-time avec des donn\xe9es qui viennent en batch."}),"\n",(0,i.jsxs)(n.li,{children:["On n'a en g\xe9n\xe9ral pas la possibilit\xe9 d'utiliser le m\xeame syst\xe8me pour traiter les donn\xe9es qui arrivent en real-time et les donn\xe9es qui arrivent par batch.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple ",(0,i.jsx)(t.U,{children:"Google Cloud Dataflow"})," le permet avec l'utilisation de ",(0,i.jsx)(t.U,{children:"Beam"}),", mais la plupart du temps on aura besoin de deux outils."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Selon les auteurs, la plupart du temps quand les utilisateurs demandent du “real-time”, ils veulent en fait juste de la ",(0,i.jsx)(n.em,{children:"real-time ingestion"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Quand des utilisateurs demandent \xe0 pouvoir afficher leur dashboard en “real-time” alors qu'il tourne une fois par jour, bien souvent avoir de la ",(0,i.jsx)(n.em,{children:"real-time ingestion"})," et faire tourner le processing du rapport toutes les heures ou toutes les 15 minutes leur suffira."]}),"\n",(0,i.jsx)(n.li,{children:"Parmi les cas d'usage qui pourraient n\xe9cessiter du real-time processing : les syst\xe8mes d'action in-game, les syst\xe8mes de recommandation, les syst\xe8mes de d\xe9tection de fraude."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Transiter une pipeline de la ",(0,i.jsx)(n.em,{children:"batch ingestion"})," \xe0 la real-time ingestion se fait sans trop de difficult\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Transiter du ",(0,i.jsx)(n.em,{children:"batch processing"})," au ",(0,i.jsx)(n.em,{children:"real-time processing"})," est bien plus complexe vu qu'il va falloir en g\xe9n\xe9ral changer d'outils, et donc il faut penser \xe7a en amont."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"fast storage"})," est compos\xe9 d'un syst\xe8me d'",(0,i.jsx)(n.strong,{children:"event streaming"})," du type ",(0,i.jsx)(t.U,{children:"Kafka"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les messages (qui font entre quelques KB et 1 MB) sont trait\xe9s un par un, et stock\xe9s dans des ",(0,i.jsx)(n.em,{children:"topics"}),". Ils sont identifiables par leur offset."]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"producers"})," \xe9crivent dans les topics, et les ",(0,i.jsx)(n.em,{children:"consumers"})," lisent depuis les topics. ",(0,i.jsx)(t.U,{children:"Kafka"})," a des m\xe9canismes qui leur permettent de publier et consommer de mani\xe8re fiable malgr\xe9 les fautes."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour permettre de scaler, le contenu des topics est s\xe9par\xe9 en ",(0,i.jsx)(n.em,{children:"partitions"}),", qui se trouvent sur des machines diff\xe9rentes, avec des copies pour plus de fiabilit\xe9."]}),"\n",(0,i.jsxs)(n.li,{children:["L\xe0 o\xf9 lire et \xe9crire dans ",(0,i.jsx)(t.U,{children:"S3"})," mettrait quelques centaines de ms, le faire dans Kafka en prend 10 fois moins, mais surtout ",(0,i.jsx)(t.U,{children:"Kafka"})," tient la charge avec une tr\xe8s grande quantit\xe9 de petits messages."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["De m\xeame que pour le ",(0,i.jsx)(n.em,{children:"slow storage"})," et le ",(0,i.jsx)(n.em,{children:"batch processing"}),", le ",(0,i.jsx)(n.em,{children:"fast storage"})," est ",(0,i.jsx)(n.strong,{children:"organis\xe9 en areas"})," qui servent \xe0 des stages de processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les \xe9tapes sont :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Landing area"})," : l'",(0,i.jsx)(n.em,{children:"ingestion layer"})," \xe9crit la donn\xe9e dans cet endroit."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Staging area"})," : la donn\xe9e subit des checks basiques de qualit\xe9, et on v\xe9rifie qu'elle est conforme au sch\xe9ma attendu."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - Archive area"})," : la donn\xe9e est copi\xe9e depuis la ",(0,i.jsx)(n.em,{children:"landing area"})," vers l'",(0,i.jsx)(n.em,{children:"archive area"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit d'espace de stockage cloud classique."}),"\n",(0,i.jsxs)(n.li,{children:["On pourra refaire le processing de la donn\xe9e simplement en la copiant depuis l'",(0,i.jsx)(n.em,{children:"archive area"})," vers la ",(0,i.jsx)(n.em,{children:"landing area"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - Production area"})," : la donn\xe9e subit la transformation business n\xe9cessaire pour un use-case particulier avant d'aller l\xe0.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4.1 - Pass-through job"})," : il s'agit d'un job qui copie la donn\xe9e de la ",(0,i.jsx)(n.em,{children:"staging area"})," vers la ",(0,i.jsx)(n.em,{children:"production area"})," sans transformation, et ensuite la copie dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ce use-case “basique” est utile pour d\xe9bugguer les autres use-cases."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4.2 - Staging to production"})," : des jobs lisent la donn\xe9e \xe0 partir de la ",(0,i.jsx)(n.em,{children:"staging area"})," dans un but de reporting/analytics, et cr\xe9ent un dataset dans la ",(0,i.jsx)(n.em,{children:"production area"}),", pour charger la donn\xe9e ensuite dans le ",(0,i.jsx)(n.em,{children:"data warehouse"})," ou dans une DB relationnelle ou NoSQL."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"5 - Failed area"})," : chaque \xe9tape peut faire face \xe0 des erreurs, qu'elles soient li\xe9es \xe0 la donn\xe9e ou \xe0 des \xe9checs temporaires de la pipeline.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les messages qui n'ont pas r\xe9ussi une \xe9tape vont dans cette area o\xf9 on pourra les examiner et voir ce qu'il faut corriger."}),"\n",(0,i.jsx)(n.li,{children:"Une fois la correction faite, il suffit de les copier dans l'area de l'\xe9tape o\xf9 ils ont \xe9chou\xe9s."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["C\xf4t\xe9 ",(0,i.jsx)(n.strong,{children:"organisation en topics"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les providers limitent en g\xe9n\xe9ral le nombre de topics \xe0 quelques milliers, et c'est l'abstraction principale qu'on a. Avec des centaines de tables par DB qu'on utilise comme source, les topics sont vite tr\xe8s nombreux."}),"\n",(0,i.jsxs)(n.li,{children:["Selon les auteurs, l'organisation la plus pertinente pour le cas g\xe9n\xe9ral serait d'utiliser ",(0,i.jsx)(n.strong,{children:"un topic par area"}),", et de faire la distinction entre sources avec un champ \xe0 l'int\xe9rieur des messages.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Mais dans le cas o\xf9 on a des sources qui donnent des messages structur\xe9s tr\xe8s diff\xe9remment, ou qui ne permettent pas d'utiliser des jobs de processings communs, on peut faire des topics diff\xe9rents par source."}),"\n",(0,i.jsx)(n.li,{children:"Une autre raison de s\xe9parer en topics par source peut \xeatre la limitation en termes de quotas par topic, de la part du provider."}),"\n",(0,i.jsx)(n.li,{children:"Une autre raison pour publier dans des topics diff\xe9rents peut \xeatre la structure interne des \xe9quipes, et les questions de s\xe9curit\xe9, pour restreindre certaines donn\xe9es \xe0 certaines \xe9quipes."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Pour rendre les jobs configurables, on peut faire en sorte qu'ils lisent le contenu du message et appellent une librairie qui va faire quelque chose de particulier en fonction de la valeur lue."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les transformations qu'on a couramment dans les syst\xe8mes real-time, il y a la ",(0,i.jsx)(n.strong,{children:"d\xe9duplication des messages"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les duplications sont courantes dans les syst\xe8mes real-time, elles ont deux origines :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"1 - Des duplications issues de la source, sur lesquelles on n'a pas de contr\xf4le."}),"\n",(0,i.jsxs)(n.li,{children:["2 - Des duplications qui sont dues au fonctionnement des syst\xe8mes real-time, et \xe0 leur nature distribu\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On peut par exemple avoir un producer qui envoie un message, mais ne re\xe7oit pas l'acknowledgement \xe0 cause d'un probl\xe8me r\xe9seau. Un autre broker sera \xe9lu master de la partition et on se retrouvera avec une duplication."}),"\n",(0,i.jsx)(n.li,{children:"C\xf4t\xe9 consumer, il suffit que l'un d'entre eux envoie un message et crash avant de commiter. Il va alors renvoyer le m\xeame message quand il reviendra \xe0 la vie."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La difficult\xe9 pour d\xe9dupliquer avec les syst\xe8mes real-time c'est qu'on a une donn\xe9e qui arrive en permanence, et qui est distribu\xe9e sur plusieurs machines.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Une solution peut \xeatre d'utiliser une ",(0,i.jsx)(n.strong,{children:"time window"})," : on choisit un d\xe9but et une fin de timestamp, et on r\xe9cup\xe8re tous les messages correspondants pour faire une d\xe9duplication parmi eux.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut avoir par exemple une ",(0,i.jsx)(n.em,{children:"sliding window"})," qui se d\xe9place dans le temps, ou ",(0,i.jsx)(n.em,{children:"tumbling window"})," qui va diviser le temps en tranches disjointes."]}),"\n",(0,i.jsx)(n.li,{children:"Le probl\xe8me c'est qu'on est limit\xe9s sur la tranche de temps qu'une machine peut traiter, et la d\xe9duplication ne se fait que pour les messages de cette tranche, et pas avec les autres tranches."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une autre solution est d'avoir un ",(0,i.jsx)(n.strong,{children:"key/value cache"})," dans lequel on met l'ID de chaque message trait\xe9, et qu'on r\xe9interroge \xe0 chaque fois pour \xe9viter de le retraiter encore.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La taille va rarement \xeatre un probl\xe8me : stocker 1 milliard de UUID fait ~15 Go."}),"\n",(0,i.jsxs)(n.li,{children:["Par contre il faut que le store soit ",(0,i.jsx)(n.em,{children:"highly available"})," et performant, donc une solution cloud est bien adapt\xe9e."]}),"\n",(0,i.jsxs)(n.li,{children:["Exemples de key/value stores : ",(0,i.jsx)(t.U,{children:"Azure Cosmos DB"}),", ",(0,i.jsx)(t.U,{children:"Google Cloud Bigtable"}),", ",(0,i.jsx)(t.U,{children:"AWS DynamoDB"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une 3\xe8me solution peut \xeatre de laisser les messages dupliqu\xe9s jusqu'au ",(0,i.jsx)(n.em,{children:"data warehouse"}),", et d\xe9dupliquer ensuite par ",(0,i.jsx)(n.strong,{children:"un job en mode batch"}),", soit dans le ",(0,i.jsx)(n.em,{children:"data lake"}),", soit dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ca permet d'avoir une real-time ingestion particuli\xe8rement rapide, mais il faut que la duplication soit OK dans un premier temps."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une autre transformation courante est la ",(0,i.jsx)(n.strong,{children:"conversion de format"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les messages dans le syst\xe8me real-time sont consomm\xe9s un par un, donc il est capital d'avoir des sch\xe9mas bien d\xe9finis entre producers et consumers. Le ",(0,i.jsx)(n.em,{children:"metadata layer"})," pourra nous aider \xe0 le stocker."]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant le format :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"JSON ne fournit pas de m\xe9canisme de gestion de sch\xe9ma, et est plus volumineux. Il peut \xeatre compress\xe9, mais ce serait surtout efficace avec plusieurs messages o\xf9 des noms de champ se r\xe9p\xe8tent par exemple."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Avro"})," permet de ",(0,i.jsx)(n.strong,{children:"minimiser la taille du message"}),", et permet une gestion du sch\xe9ma avec la possibilit\xe9 de le stocker dans un store."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Parquet"})," ",(0,i.jsx)(n.strong,{children:"n'apporte aucun avantage"})," dans un syst\xe8me real-time puisque son but est de permettre de lire de grandes quantit\xe9s de donn\xe9es pour faire du processing dessus, et qu'on est ici sur du message par message."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant les ",(0,i.jsx)(n.strong,{children:"quality checks"}),", on peut avoir un job qui v\xe9rifie la qualit\xe9 de chaque message avant de le placer dans l'area du stage suivant ou dans la ",(0,i.jsx)(n.em,{children:"failed area"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Dans le cas o\xf9 on a de nombreuses sources g\xe9r\xe9es par plusieurs \xe9quipes, la difficult\xe9 va surtout \xeatre dans la d\xe9finition de ce qu'est une donn\xe9e avec une qualit\xe9 suffisante."}),"\n",(0,i.jsxs)(n.li,{children:["Nos ",(0,i.jsx)(n.em,{children:"quality checks"})," peuvent impliquer de v\xe9rifier une caract\xe9ristique impliquant plusieurs messages, par exemple “pas plus de 10% des commandes avec le statut cancelled”.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faudra alors utiliser les techniques de ",(0,i.jsx)(n.em,{children:"windowing"})," comme avec la d\xe9duplication."]}),"\n",(0,i.jsx)(n.li,{children:"Si la dur\xe9e sur laquelle on veut faire les checks est trop grande par rapport \xe0 ce que supportent nos outils, il faudra faire passer le flow par le batch processing."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 on veut ",(0,i.jsx)(n.strong,{children:"combiner une source de donn\xe9es real-time et une source batch"}),", on peut :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"1 - Avoir le job real-time qui lit le message batch \xe0 combiner avec les donn\xe9es du message real-time, et qui le stocke dans sa RAM."}),"\n",(0,i.jsx)(n.li,{children:"2 - Puis ce job combine les deux pour les mettre dans la real-time production area."}),"\n",(0,i.jsx)(n.li,{children:"3 - Et il continue avec les messages suivants en utilisant le message batch qui est dans sa RAM, jusqu'\xe0 ce qu'il y en ait un nouveau."}),"\n",(0,i.jsx)(n.li,{children:"La limitation pourrait \xeatre la taille du message batch : s'il ne rentre pas dans la ram des VMs qui font le real-time processing, on peut fallback sur du batch processing."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"3 cloud vendors principaux"})," fournissent chacun deux outils pour le real-time processing : un outil de real-time storage type ",(0,i.jsx)(t.U,{children:"Kafka"}),", et un outil qui fait le real-time processing.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Kinesis Data Streams"})," est \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"Kafka"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il fournit des clients dans 5 langages, dont Node.js."}),"\n",(0,i.jsxs)(n.li,{children:["Il a l'\xe9quivalent des topics sous le nom de ",(0,i.jsx)(n.em,{children:"Data Streams"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Il a l'\xe9quivalent des partitions sous le nom de ",(0,i.jsx)(n.em,{children:"shard"}),", et limite la throughput \xe0 1 MB/s par shard."]}),"\n",(0,i.jsx)(n.li,{children:"Il supporte le “resharding” \xe0 la hausse ou \xe0 la baisse."}),"\n",(0,i.jsx)(n.li,{children:"Il limite la taille des records \xe0 1 MB."}),"\n",(0,i.jsx)(n.li,{children:"La r\xe9tention par d\xe9faut est d'un jour, et va jusqu'\xe0 une semaine."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Kinesis Data Analytics"})," est l'outil de processing real-time.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il fournit une API SQL pour cr\xe9er les jobs, mais c'est limit\xe9 \xe0 des records qui contiendront du CSV ou du JSON."}),"\n",(0,i.jsxs)(n.li,{children:["Il fournit aussi une API Java, qui utilise ",(0,i.jsx)(t.U,{children:"Apache Flink"})," et permet plus de flexibilit\xe9 sur le format des records."]}),"\n",(0,i.jsx)(n.li,{children:"Il ne fournit pas de m\xe9canisme de d\xe9duplication."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GCP"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Pub/Sub"})," est un peu diff\xe9rent de ",(0,i.jsx)(t.U,{children:"Kafka"})," et il abstrait plus de choses.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il fournit des clients dans 7 langages, dont Node.js."}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"topics"})," permettent de regrouper les records, mais il n'y a ",(0,i.jsx)(n.strong,{children:"pas de notion de partition"}),", ou en tout cas elle est abstraite derri\xe8re l'API."]}),"\n",(0,i.jsxs)(n.li,{children:["Les consumers peuvent faire une ",(0,i.jsx)(n.em,{children:"subscription"})," \xe0 un topic pour consommer les records.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ils peuvent aussi utiliser une ",(0,i.jsx)(n.em,{children:"subscription"})," pour recevoir de la donn\xe9e combin\xe9e de plusieurs ",(0,i.jsx)(n.em,{children:"topics"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"On se sert aussi des subscriptions pour scaler le throughput : on a le droit \xe0 1 MB/s par subscription."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Les records sont limit\xe9s \xe0 10 MB."}),"\n",(0,i.jsx)(n.li,{children:"La r\xe9tention des donn\xe9es maximale est d'une semaine."}),"\n",(0,i.jsxs)(n.li,{children:["Il ne fournit pas d'offsets pour les records, ce qui limite la possibilit\xe9 de rejouer certains messages particuliers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On peut faire des ",(0,i.jsx)(n.em,{children:"snapshots"})," pour pouvoir les rejouer, mais ils sont limit\xe9s \xe0 5000 par projet."]}),"\n",(0,i.jsx)(n.li,{children:"On a aussi la possibilit\xe9 de rejouer par timestamp, mais c'est peu pr\xe9cis."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Dataflow"})," est l'outil de processing real-time.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il fournit une API SQL pour cr\xe9er les jobs, mais c'est limit\xe9 \xe0 des records qui contiendront du JSON."}),"\n",(0,i.jsxs)(n.li,{children:["Il fournit aussi une API Java et Python, qui utilise ",(0,i.jsx)(t.U,{children:"Apache Beam"})," et permet plus de flexibilit\xe9 sur le format des records."]}),"\n",(0,i.jsx)(n.li,{children:"Il permet de d\xe9dupliquer les messages issus de probl\xe8mes techniques, et propose aussi une d\xe9duplication des messages par ID, sur une fen\xeatre de 10 minutes."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Azure"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Event Hubs"})," est \xe9quivalent \xe0 ",(0,i.jsx)(t.U,{children:"Kafka"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il fournit des clients en .NET et Python, mais des versions open source sont disponibles pour d'autres langages."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Il supporte 3 protocoles pour s'y int\xe9grer"})," en tant que producer ou consumer : HTTPS, AMQP et ",(0,i.jsx)(t.U,{children:"Kafka"}),". \xc7a permet de migrer vers Azure sans avoir \xe0 tout r\xe9\xe9crire."]}),"\n",(0,i.jsxs)(n.li,{children:["Il a l'\xe9quivalent des topics dans le cas de ",(0,i.jsx)(t.U,{children:"Kafka"}),", ou des hubs dans le cas d'AMQP."]}),"\n",(0,i.jsxs)(n.li,{children:["Il a l'\xe9quivalent des partitions, qu'il faut d\xe9finir \xe0 l'avance comme pour ",(0,i.jsx)(t.U,{children:"Kafka"}),", et \xe0 l'inverse de ",(0,i.jsx)(t.U,{children:"Kinesis Data Streams"})," pour lequel on peut “resharder”."]}),"\n",(0,i.jsx)(n.li,{children:"Le throughput est limit\xe9 \xe0 1 MB/s ou 1000 messages/s par partition."}),"\n",(0,i.jsx)(n.li,{children:"Les records ne peuvent pas d\xe9passer 1 MB."}),"\n",(0,i.jsx)(n.li,{children:"La p\xe9riode de r\xe9tention maximale est d'une semaine."}),"\n",(0,i.jsxs)(n.li,{children:["Contrairement \xe0 ",(0,i.jsx)(t.U,{children:"Kinesis Data Streams"})," qui stocke les offsets des consumers dans ",(0,i.jsx)(t.U,{children:"DynamoDB"}),", ou \xe0 ",(0,i.jsx)(t.U,{children:"Kafka"})," qui le stocke dans un topic interne, ",(0,i.jsx)(t.U,{children:"Event Hubs"})," laisse cette responsabilit\xe9 aux consumers."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Stream Analytics"})," est l'outil de processing real-time.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il ne propose qu'une API SQL, avec des fonctionnalit\xe9s avanc\xe9es de type windowing, recherche dans des dictionnaires etc."}),"\n",(0,i.jsxs)(n.li,{children:["Si on veut plus de flexibilit\xe9, on peut utiliser ",(0,i.jsx)(t.U,{children:"Spark"})," \xe0 travers ",(0,i.jsx)(t.U,{children:"Azure Databricks"}),", mais il s'agira de micro-batching et non pas de vrai streaming."]}),"\n",(0,i.jsx)(n.li,{children:"Il ne fournit pas de fonctionnalit\xe9s de d\xe9duplication."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"7---metadata-layer-architecture",children:"7 - Metadata layer architecture"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il existe deux types de metadata dans le cadre de la data.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - La ",(0,i.jsx)(n.strong,{children:"business metadata"})," permet de ",(0,i.jsx)(n.strong,{children:"donner du contexte"})," \xe0 la donn\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ca peut \xeatre par exemple : la source, le propri\xe9taire de la donn\xe9e, la date de sa cr\xe9ation, la taille de la donn\xe9e, le but de la donn\xe9e, son niveau de qualit\xe9 etc."}),"\n",(0,i.jsx)(n.li,{children:"\xc7a aide notamment \xe0 trouver la donn\xe9e qu'on cherche."}),"\n",(0,i.jsxs)(n.li,{children:["On appelle souvent l'outillage autour de la business metadata le ",(0,i.jsx)(n.strong,{children:"data catalog"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les cloud vendors ont chacun leur outil : ",(0,i.jsx)(t.U,{children:"Google Cloud Data Catalog"}),", ",(0,i.jsx)(t.U,{children:"Azure Data Catalog"}),", ",(0,i.jsx)(t.U,{children:"AWS Glue Data Catalog"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - La ",(0,i.jsx)(n.strong,{children:"data platform metadata"})," (ou ",(0,i.jsx)(n.em,{children:"pipeline metadata"}),") permet de rassembler des informations sur les pipelines de donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\xc7a peut \xeatre des informations sur les sources, sur le succ\xe8s ou l'\xe9chec de runs de pipelines, les erreurs qui ont eu lieu etc."}),"\n",(0,i.jsxs)(n.li,{children:["\xc7a permet notamment le ",(0,i.jsx)(n.strong,{children:"monitoring et la configuration des pipelines"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Cette metadata est plus align\xe9e avec la responsabilit\xe9 des data engineers, et c'est sur elle que se concentre ce livre."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Une seule pipeline simple peut \xeatre g\xe9r\xe9e avec du code, mais d\xe8s que le syst\xe8me de pipelines se complexifie, il faut ",(0,i.jsx)(n.strong,{children:"g\xe9rer cette complexit\xe9"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On a le choix de :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - ",(0,i.jsx)(n.strong,{children:"Dupliquer le code des pipelines"})," pour les rendre simples, mais alors il faudra refaire des modifications \xe0 plusieurs endroits \xe0 chaque fois qu'on voudra changer quelque chose qui concerne plusieurs pipelines."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - ",(0,i.jsx)(n.strong,{children:"Mettre du code en commun"})," pour \xe9viter de r\xe9\xe9crire trop de choses, mais alors la codebase se complexifie, et l'investigation des probl\xe8mes aussi."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les auteurs du livre conseillent de mettre le code en commun, et de ",(0,i.jsx)(n.strong,{children:"rendre les pipelines configurables"})," pour \xe9viter l'explosion de complexit\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On pourra par exemple mettre en commun l'ingestion de sources de type RDBMS, et celles de type file. Ou encore mettre en commun des jobs de ",(0,i.jsx)(n.em,{children:"data quality check"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Si la configuration se trouve dans un endroit s\xe9par\xe9, il devient facile de la changer sans avoir \xe0 toucher au code."}),"\n",(0,i.jsx)(n.li,{children:"Parmi les \xe9l\xe9ments de configuration, il peut y avoir par exemple : l'endroit d'o\xf9 on r\xe9cup\xe8re la donn\xe9e, l'endroit o\xf9 on l'envoie, les checks de qualit\xe9 et transformations qu'il faut faire sur chaque donn\xe9e etc."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"data platform metadata"})," a 3 fonctions :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - ",(0,i.jsx)(n.strong,{children:"Stocker les configurations des pipelines"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple, si un path d'input sur un serveur FTP change, il suffira d'aller changer la configuration de la pipeline dans le ",(0,i.jsx)(n.em,{children:"metadata layer"}),", sans toucher au code."]}),"\n",(0,i.jsx)(n.li,{children:"Pour conna\xeetre les inputs et outputs d'une pipeline, il suffira aussi de regarder sa configuration."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - ",(0,i.jsx)(n.strong,{children:"Monitorer l'ex\xe9cution et le statut des pipelines"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple, en cas d'erreur sur un pipeline, il suffira d'aller regarder dans le metadata layer pour avoir un statut d\xe9taill\xe9 de la pipeline, avec des statistiques d'\xe9chec, de nombre de duplicatas etc."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["3 - ",(0,i.jsx)(n.strong,{children:"Servir de schema repository"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cette partie sera plus d\xe9velopp\xe9e dans le chapitre 8."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il n'existe pas vraiment de standard concernant le ",(0,i.jsx)(n.strong,{children:"mod\xe8le d'un metadata layer"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs du livre en proposent un centr\xe9 autour de 4 domaines, contenant les aspects qu'ils pensent \xeatre suffisamment universels.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - La ",(0,i.jsx)(n.strong,{children:"Pipeline Metadata"})," contient les informations d'input, output et transformations de chaque pipeline.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["L'objet ",(0,i.jsx)(n.strong,{children:"Namespace"})," se trouve au plus haut niveau, et permet de s\xe9parer des groupes de pipelines.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit par exemple de pouvoir appliquer des droits d'acc\xe8s diff\xe9rents \xe0 des ensembles de pipelines."}),"\n",(0,i.jsx)(n.li,{children:"On pourra l'utiliser pour nommer les folders, ou les topics de notre syst\xe8me de slow et fast storage."}),"\n",(0,i.jsxs)(n.li,{children:["Sa structure est :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"ID"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Name"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Description"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Created At"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Updated At"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["L'objet ",(0,i.jsx)(n.strong,{children:"Pipeline"})," d\xe9crit un ensemble de jobs qui prend un ou plusieurs inputs, et \xe9crit dans une ou plusieurs destinations, avec d'\xe9ventuelles transformations.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les pipelines seront souvent li\xe9es : par exemple une pipeline d'ingestion qui \xe9crit dans le ",(0,i.jsx)(n.em,{children:"data lake"}),", puis une autre qui lit cette donn\xe9e, la combine avec une autre, et \xe9crit \xe0 nouveau dans le ",(0,i.jsx)(n.em,{children:"data lake"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Sa structure est :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"ID"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Name"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Description"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Type"})," : indique par exemple si c'est une pipeline d'ingestion ou de transformation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Velocity"})," : batch ou real-time."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Sources and Destinations"})," : liste les identifiants d'objets ",(0,i.jsx)(n.em,{children:"Source"})," desquels la pipeline lit, et ",(0,i.jsx)(n.em,{children:"Destination"})," vers lesquels la pipeline \xe9crit.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"En g\xe9n\xe9ral une pipeline d'ingestion aura une source et une destination, et une pipeline de transformation aura plusieurs sources et une destination."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Data Quality Checks IDs"})," : une liste d'identifiants de checks de qualit\xe9 \xe0 appliquer \xe0 l'ensemble des sources et destinations de la pipeline."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Created At"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Updated At"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Connectivity Details"})," : pour les pipelines d'ingestion, il s'agit d'avoir des informations sur les sources. Par exemple des URLs, adresses IP etc.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Attention \xe0 ne pas stocker de username / mots de passe dans ce layer. Il vaut mieux les mettre dans des outils s\xe9curis\xe9s comme ",(0,i.jsx)(t.U,{children:"Azure Key Vault"}),", ",(0,i.jsx)(t.U,{children:"AWS Secrets Manager"})," ou ",(0,i.jsx)(t.U,{children:"Google Cloud Secrets Manager"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["L'objet ",(0,i.jsx)(n.strong,{children:"Source"})," d\xe9crit un endroit dont on veut aller chercher de la donn\xe9e en entr\xe9e d'une pipeline.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Sa structure est :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"ID"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Name"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Schema ID"})," : un lien vers le ",(0,i.jsx)(n.em,{children:"schema registry"})," qui contient le sch\xe9ma de cette source."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Data Quality Checks IDs"})," : les checks de qualit\xe9 \xe0 appliquer \xe0 chaque fois que cette source est utilis\xe9e."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Type"})," : le type de source, par exemple “file”, “real-time topic”, “table”."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Created At"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Updated At"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["L'objet ",(0,i.jsx)(n.strong,{children:"Destination"})," est similaire \xe0 l'objet Source, mais les types peuvent \xeatre diff\xe9rents. Par exemple, on peut vouloir aussi mettre dans un ",(0,i.jsx)(n.em,{children:"key/value store"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Sa structure est :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"ID"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Name"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Schema ID"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Data Quality Checks IDs"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Type"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Created At"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Updated At"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Les ",(0,i.jsx)(n.strong,{children:"Data Quality Checks"})," permettent d'identifier les donn\xe9es qui posent probl\xe8me.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ils s'appliquent \xe0 des pipelines et sources ou destinations sans \xeatres sp\xe9cifiques \xe0 un namespace."}),"\n",(0,i.jsxs)(n.li,{children:["Il existe deux types de ",(0,i.jsx)(n.em,{children:"data quality checks"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"proactive"})," checks sont faits pour contr\xf4ler la donn\xe9e une par une, et s'assurer que la donn\xe9e de mauvaise qualit\xe9 ne rentre pas.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On va souvent v\xe9rifier le format de la donn\xe9e, ou le fait que certaines valeurs soient coh\xe9rentes. Par exemple 24h dans un jour, pas de dates n\xe9gatives etc."}),"\n",(0,i.jsx)(n.li,{children:"Ces checks ne peuvent pas \xeatre trop lourds pour ne pas bloquer la pipeline trop longtemps."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"retrospective"})," checks sont sch\xe9dul\xe9s r\xe9guli\xe8rement, et op\xe8rent sur de plus grandes quantit\xe9s de donn\xe9es, pour s'assurer qu'on garde une certaine consistance sur l'ensemble.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ca peut par exemple \xeatre de faire une jointure sur deux jeux de donn\xe9es de d\xe9partements et d'employ\xe9s, pour s'assurer qu'aucun d\xe9partement n'est sans employ\xe9."}),"\n",(0,i.jsx)(n.li,{children:"Ils produisent des rapports r\xe9guliers pour donner lieu \xe0 d'\xe9ventuelles actions pour am\xe9liorer la qualit\xe9 de la donn\xe9e."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["L'\xe9l\xe9ment principal du ",(0,i.jsx)(n.em,{children:"data quality check"})," est ",(0,i.jsx)(n.strong,{children:"la r\xe8gle"})," \xe0 faire respecter. Il existe de nombreuses options sur la mani\xe8re de l'impl\xe9menter.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\xc7a peut \xeatre une requ\xeate SQL, ou encore un Domain Specific Language (DSL)."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Leur structure est :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"ID"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Name"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Severity"})," : la gravit\xe9 du probl\xe8me si la r\xe8gle n'est pas respect\xe9e.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"info"})," indique qu'on laisse passer la donn\xe9e, qu'on log le probl\xe8me dans l'activity metadata, mais qu'on ne cr\xe9e pas d'alerte."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"warning"})," indique qu'on laisse passer la donn\xe9e, et qu'on cr\xe9e une alerte pour avertir un data engineer."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"critical"})," indique qu'on ne laisse pas passer la donn\xe9e et qu'on la met en quarantaine, avec aussi une alerte."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Rule"})," : en fonction de la mani\xe8re dont on g\xe8re nos r\xe8gles, cet attribut contiendra quelque chose de diff\xe9rent."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Created At"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Updated At"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["3 - Les ",(0,i.jsx)(n.strong,{children:"Pipeline Activities"})," contiennent des informations de succ\xe8s, \xe9checs, statistiques etc. sur l'ex\xe9cution r\xe9guli\xe8re des pipelines.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"On enregistre les informations de chaque pipeline qui tourne, et on ne supprime jamais ces donn\xe9es, pour pouvoir ensuite investiguer, ou faire des analyses dessus."}),"\n",(0,i.jsxs)(n.li,{children:["On pourra par exemple r\xe9pondre \xe0 des questions comme :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Quelle est la dur\xe9e moyenne d'une pipeline ?"}),"\n",(0,i.jsx)(n.li,{children:"Combien de rows lit en moyenne une pipeline ?"}),"\n",(0,i.jsx)(n.li,{children:"Combien de donn\xe9es on collecte en moyenne depuis une source donn\xe9e ?"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les \xe9l\xe9ments de structure que les auteurs ont trouv\xe9 utiles dans la plupart des contextes :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Activity ID"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Pipeline ID"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Start time"}),", ",(0,i.jsx)(n.em,{children:"Stop time"})," : d\xe9but et fin de l'ex\xe9cution de la pipeline."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Status"})," : succ\xe8s / \xe9chec."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Error Message"})," : en cas d'\xe9chec, mettre l'erreur dans ce champ fait gagner beaucoup de temps de recherche dans les logs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Source and Destination Ids"})," : la liste pr\xe9cise des sources et destinations qui ont \xe9t\xe9 utilis\xe9es par la pipeline."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Rows Read"})," : nombre de rows qui ont \xe9t\xe9 lues, dans le cas de fichiers \xe7a permet notamment de s'assurer qu'on a lu le fichier entier."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Rows Written"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Bytes Read"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Bytes Written"})," : on peut l'utiliser pour du monitoring, par exemple pour s'assurer que la valeur ne vaut pas 0 si ",(0,i.jsx)(n.em,{children:"Bytes Read"})," ne vaut pas 0."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Extra"})," : des infos additionnelles comme le path o\xf9 le fichier a \xe9t\xe9 \xe9crit sur le storage, le nom du topic et le window dans le cas de real-time."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas de real-time processing, c'est une bonne id\xe9e d'aligner le ",(0,i.jsx)(n.em,{children:"time window"})," avec la fr\xe9quence d'\xe9criture des messages dans le slow storage."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["4 - ",(0,i.jsx)(n.strong,{children:"Schema Registry"})," contient l'ensemble des versions des sch\xe9mas des donn\xe9es entrantes. Il est d\xe9taill\xe9 au chapitre suivant."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Selon l'exp\xe9rience des auteurs, il n'y a pas d'outil open source ou commercial qui permette de mettre en œuvre le metadata layer de mani\xe8re satisfaisante. Ils conseillent donc de ",(0,i.jsx)(n.strong,{children:"le coder soi-m\xeame"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - Une premi\xe8re solution simple est d'impl\xe9menter le ",(0,i.jsx)(n.em,{children:"metadata layer"})," avec des ",(0,i.jsx)(n.strong,{children:"fichiers"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit de la solution la plus simple, quand on a peu de sources et de pipelines."}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"pipeline metadata"})," peut \xeatre impl\xe9ment\xe9e avec des fichiers de configuration de type JSON ou YAML par exemple.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il s'agit d'avoir par exemple un fichier pour les namespaces, un pour les pipelines etc."}),"\n",(0,i.jsx)(n.li,{children:"Les IDs doivent \xeatre assign\xe9s \xe0 la main."}),"\n",(0,i.jsx)(n.li,{children:"Il s'agira de les mettre dans le gestionnaire de version avec le reste du code, et de les d\xe9ployer \xe0 chaque fois avec la pipeline de CI/CD."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"pipeline activities metadata"})," sont l'\xe9quivalent de fichiers logs o\xf9 la donn\xe9e afflue r\xe9guli\xe8rement.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour pouvoir chercher dedans, il faut un outil sp\xe9cialis\xe9 qui permette de le faire, il s'agit des ",(0,i.jsx)(n.strong,{children:"Cloud Log Aggregation Services"})," : ",(0,i.jsx)(t.U,{children:"Azure Monitor"})," avec ",(0,i.jsx)(t.U,{children:"Log Analytics"})," sur Azure, ",(0,i.jsx)(t.U,{children:"Cloud Logging"})," sur GCP, et ",(0,i.jsx)(t.U,{children:"Elasticsearch"})," sur AWS."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Un cran de complexit\xe9 au-dessus, on a l'utilisation d'une ",(0,i.jsx)(n.strong,{children:"base de donn\xe9es"})," pour stocker les fichiers de configuration (la ",(0,i.jsx)(n.em,{children:"pipeline metadata"}),").","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les fichiers de configuration sont toujours dans le gestionnaire de version, et servent de source de v\xe9rit\xe9 pour la configuration du ",(0,i.jsx)(n.em,{children:"metadata layer"}),". C'est n\xe9cessaire pour avoir un historique des changements."]}),"\n",(0,i.jsxs)(n.li,{children:["A chaque fois qu'un changement est fait dans ces fichiers, une migration sera faite sur la ",(0,i.jsx)(n.em,{children:"metadata database"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["L'avantage d'avoir cette DB, c'est qu'on va pouvoir faire des requ\xeates pour obtenir des informations sp\xe9cifiques qui existent \xe0 travers les fichiers de config. Par exemple : “Je veux voir toutes les sources qui utilisent ce ",(0,i.jsx)(n.em,{children:"data quality check"}),"”."]}),"\n",(0,i.jsxs)(n.li,{children:["La DB peut \xeatre une DB relationnelle ou une DB de document qui permettra plus de flexibilit\xe9 sur l'\xe9volution du sch\xe9ma.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Des exemples typiques peuvent \xeatre ",(0,i.jsx)(t.U,{children:"Google Cloud Datastore"}),", ",(0,i.jsx)(t.U,{children:"Azure Cosmos DB"})," et ",(0,i.jsx)(t.U,{children:"AWS DynamoDB"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["3 - Quand on a plusieurs \xe9quipes en charge des pipelines, il faut une solution qui puisse abstraire les d\xe9tails d'impl\xe9mentation expos\xe9s par la DB : on peut utiliser une ",(0,i.jsx)(n.strong,{children:"metadata API"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"L'id\xe9e c'est que le changement dans la structure de la DB n'impactera pas de nombreux outils maintenus par plusieurs \xe9quipes diff\xe9rentes. On pourra par exemple faire plusieurs versions de l'API."}),"\n",(0,i.jsxs)(n.li,{children:["La metadata API est en g\xe9n\xe9ral faite selon les principes REST.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Pour plus d'infos sur comment designer une API REST, il y a ",(0,i.jsx)(a.f,{children:"The Design of Web APIs"})," d'Arnaud Lauret."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Il faudra que l'ensemble des outils qui utilisaient la DB, y compris les pipelines, utilisent maintenant l'API pour acc\xe9der aux configurations."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les auteurs conseillent de ",(0,i.jsx)(n.strong,{children:"commencer par impl\xe9menter la solution la plus simple qui satisfait les besoins actuels"})," de la ",(0,i.jsx)(n.em,{children:"data platform"}),", avec la possibilit\xe9 de passer \xe0 la version un cran plus complexe d\xe8s que le besoin sera l\xe0.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Chaque solution se base sur la pr\xe9c\xe9dente en lui ajoutant quelque chose, donc \xe7a devrait \xeatre relativement facile de migrer."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les ",(0,i.jsx)(n.strong,{children:"outils qu'on peut trouver chez les cloud vendors"}),", qui se rapprochent le plus de ce qu'on recherche avec notre ",(0,i.jsx)(n.em,{children:"metadata layer"}),", il y a :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Glue Data Catalog"})," stocke des informations \xe0 propos des sources et destinations, et des statistiques sur les runs des pipelines, ce qui fait de cet outil le plus proche de ce qu'on recherche.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le d\xe9savantage majeur c'est sa flexibilit\xe9 : il faut impl\xe9menter les pipelines avec ",(0,i.jsx)(t.U,{children:"AWS Glue ETL"}),", ce qui veut dire n'avoir que des ",(0,i.jsx)(n.em,{children:"batch jobs"}),", et qui soient compatibles avec ",(0,i.jsx)(t.U,{children:"Glue"})," (donc pas de source REST par exemple)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Data Catalog"})," et ",(0,i.jsx)(t.U,{children:"Google Cloud Data Catalog"})," sont plus orient\xe9es ",(0,i.jsx)(n.em,{children:"business metadata"}),", et fournissent surtout de la ",(0,i.jsx)(n.strong,{children:"data discovery"})," : permettre aux utilisateurs de la donn\xe9e de faire une recherche dans une UI pour trouver la table qui les int\xe9resse."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Parmi les ",(0,i.jsx)(n.strong,{children:"outils open source"}),", qui se rapprochent le plus de ce qu'on recherche avec notre ",(0,i.jsx)(n.em,{children:"metadata layer"}),", il y a :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Apache Atlas"})," permet de faire de la ",(0,i.jsx)(n.em,{children:"data discovery"}),", mais aussi de g\xe9rer la configuration de pipelines de mani\xe8re ",(0,i.jsx)(n.strong,{children:"flexible"})," : on peut utiliser les ",(0,i.jsx)(n.em,{children:"Types"})," qu'il propose pour cr\xe9er la configuration des namespaces, des pipelines, sources, destinations etc. avec des liens entre les objets.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Son inconv\xe9nient principal est qu'il a \xe9t\xe9 cr\xe9\xe9 pour l'\xe9cosyst\xe8me de ",(0,i.jsx)(t.U,{children:"Hadoop"}),", et poss\xe8de de nombreuses fonctionnalit\xe9s qui lui sont d\xe9di\xe9es."]}),"\n",(0,i.jsxs)(n.li,{children:["Un autre inconv\xe9nient est que c'est un outil open source qui n\xe9cessite de faire tourner d'autres outils open sources difficiles \xe0 administrer : ",(0,i.jsx)(t.U,{children:"HBase"})," et ",(0,i.jsx)(t.U,{children:"Solr"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"DataHub"})," est similaire \xe0 Atlas, dans la mesure o\xf9 il est suffisamment flexible pour permettre d'impl\xe9menter le mod\xe8le d\xe9crit dans ce chapitre, et permet aussi la data discovery.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a aussi l'inconv\xe9nient de n\xe9cessiter de faire tourner des outils open source difficiles \xe0 administrer : ",(0,i.jsx)(t.U,{children:"Kafka"}),", ",(0,i.jsx)(t.U,{children:"MySQL"}),", ",(0,i.jsx)(t.U,{children:"Elasticsearch"})," et ",(0,i.jsx)(t.U,{children:"Neo4j"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Marquez"})," permet principalement de mettre \xe0 disposition des informations de ",(0,i.jsx)(n.strong,{children:"data lineage"}),", c'est-\xe0-dire des informations sur l'origine des donn\xe9es.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il n'est pas assez flexible pour impl\xe9menter le mod\xe8le pr\xe9sent\xe9 dans ce chapitre."}),"\n",(0,i.jsxs)(n.li,{children:["Il a l'avantage de ne n\xe9cessiter que ",(0,i.jsx)(t.U,{children:"PostgreSQL"})," comme d\xe9pendance \xe0 faire tourner, et on peut le faire comme service manag\xe9."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"8---schema-management",children:"8 - Schema management"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Certaines organisations ont une approche ",(0,i.jsx)(n.strong,{children:"proactive"}),", et planifient les cons\xe9quences des changements dans les DBs op\xe9rationnelles sur les \xe9quipes data.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["D'autres ont une approche ",(0,i.jsx)(n.strong,{children:"”do nothing and wait for things to break”"}),", et attendent simplement que la pipeline ETL casse pour que l'\xe9quipe data la r\xe9pare en prenant en compte le changement de sch\xe9ma."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans les ",(0,i.jsx)(n.strong,{children:"architectures data traditionnelles"})," bas\xe9es sur le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", les donn\xe9es arrivent dans une ",(0,i.jsx)(n.em,{children:"landing table"})," qui reproduit exactement leur sch\xe9ma, et donc quand elles changent, l'ingestion casse.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il existe une approche alternative ",(0,i.jsx)(n.strong,{children:"schema-on-read"})," o\xf9 il s'agit d'ing\xe9rer la donn\xe9e telle quelle dans un syst\xe8me de fichiers distribu\xe9s, et dans ce cas on repousse le probl\xe8me au ",(0,i.jsx)(n.em,{children:"processing layer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Coupler le ",(0,i.jsx)(n.em,{children:"schema-on-read"})," avec une approche ”",(0,i.jsx)(n.em,{children:"do nothing and wait for things to break"}),"” est plut\xf4t une mauvaise id\xe9e selon les auteurs. Comme alternatives, on a :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - ",(0,i.jsx)(n.strong,{children:"Le schema as a contract"})," o\xf9 il s'agit pour l'\xe9quipe de d\xe9veloppeurs d'enregistrer le sch\xe9ma de leur source de donn\xe9e dans le ",(0,i.jsx)(n.em,{children:"schema repository"}),", et d'en \xeatre ",(0,i.jsx)(n.strong,{children:"responsables"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ils doivent alors ne faire que des changements ",(0,i.jsx)(n.em,{children:"backward-compatibles"})," dans leur DB. Par exemple ajouter des colonnes mais pas en renommer."]}),"\n",(0,i.jsxs)(n.li,{children:["Pour que \xe7a marche, il faut deux choses :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Un grand niveau de maturit\xe9 dans les process de d\xe9veloppement, notamment d'un point de vue automatisation de du check de r\xe9trocompatibilit\xe9 dans la pipeline de CI."}),"\n",(0,i.jsx)(n.li,{children:"Un owner pour chaque source de donn\xe9e externe \xe0 l'organisation."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["De l'exp\xe9rience des auteurs, les organisations n'ont en g\xe9n\xe9ral pas la maturit\xe9 technique suffisante, et le besoin de sch\xe9ma versionn\xe9 venant apr\xe8s coup, il est difficile de convaincre les \xe9quipes op\xe9rationnelles de mettre en place le ",(0,i.jsx)(n.em,{children:"schema as a contract"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"NDLR : il s'agit de l'approche mise en avant par le Data Mesh."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["2 - ",(0,i.jsx)(n.strong,{children:"La gestion du sch\xe9ma dans la data platform"}),". Dans ce cas, la responsabilit\xe9 se trouve du c\xf4t\xe9 de l'\xe9quipe qui g\xe8re la data platform.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les auteurs trouvent que cette solution marche bien dans pas mal de contextes. Elle a l'avantage de permettre de ",(0,i.jsx)(n.strong,{children:"centraliser"})," au m\xeame endroit les sch\xe9mas des donn\xe9es qui viennent des \xe9quipes internes et ceux qui viennent de l'ext\xe9rieur.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cette centralisation permet ensuite d'avoir un catalogue de donn\xe9es dans lequel on peut fouiller."}),"\n",(0,i.jsx)(n.li,{children:"Ca permet d'avoir un historique des sch\xe9mas pour pouvoir utiliser n'importe quelle donn\xe9e archiv\xe9e, ou faire du debugging."}),"\n",(0,i.jsx)(n.li,{children:"Ca peut aussi permettre de d\xe9tecter et ajuster les changements de sch\xe9mas avant que la pipeline n'\xe9choue."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Une autre solution peut \xeatre de laisser aux \xe9quipes internes la responsabilit\xe9 du sch\xe9ma de leurs donn\xe9es, et de centraliser les sch\xe9mas des sources externes chez l'\xe9quipe responsable de la data platform."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 la gestion des sch\xe9mas se fait dans la data platform, elle doit \xeatre ajout\xe9e en tant que ",(0,i.jsx)(n.strong,{children:"1\xe8re \xe9tape du common data processing"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le module de ",(0,i.jsx)(n.em,{children:"schema-management"})," va d'abord v\xe9rifier si un sch\xe9ma existe d\xe9j\xe0 pour cette source.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["S'il n'existe pas, le module va inf\xe9rer un sch\xe9ma depuis les donn\xe9es, puis enregistrer ce sch\xe9ma dans le ",(0,i.jsx)(n.em,{children:"sch\xe9ma registry"})," en tant que V1."]}),"\n",(0,i.jsxs)(n.li,{children:["S'il existe, le module va r\xe9cup\xe9rer la derni\xe8re version depuis le ",(0,i.jsx)(n.em,{children:"schema registry"}),", puis inf\xe9rer le sch\xe9ma depuis les donn\xe9es, cr\xe9er un nouveau sch\xe9ma compatible avec les deux et l'enregistrer en tant que version actuelle."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"L'inf\xe9rence de sch\xe9ma"})," dont on est en train de parler se base sur ",(0,i.jsx)(t.U,{children:"Apache Spark"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Spark"})," est capable d'inf\xe9rer le sch\xe9ma de fichiers CSV, JSON, y compris s'il y a plusieurs records dedans."]}),"\n",(0,i.jsxs)(n.li,{children:["Il utilise un sample de records pour faire l'inf\xe9rence, par d\xe9faut 1000, et ce nombre est configurable.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"S'il est trop faible on risque d'avoir une inf\xe9rence fauss\xe9e qui ne permet pas de parser l'ensemble des donn\xe9es. Et s'il est trop grand on risque d'avoir des probl\xe8mes de performance."}),"\n",(0,i.jsx)(n.li,{children:"Pour une table d'une DB relationnelle par exemple, le nombre pourra \xeatre bas parce que la sch\xe9ma est garanti par la DB."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 la donn\xe9e est diff\xe9rente entre deux records, ",(0,i.jsx)(t.U,{children:"Spark"})," essayera de trouver un type qui englobe les deux. Par exemple, un nombre et un string vont donner un string."]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 un type commun n'est pas possible, les donn\xe9es minoritaires seront plac\xe9es dans le champ ",(0,i.jsx)(n.code,{children:"_corrupt_record"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Le sch\xe9ma inf\xe9r\xe9 par ",(0,i.jsx)(t.U,{children:"Spark"})," va d'abord \xeatre converti en sch\xe9ma ",(0,i.jsx)(t.U,{children:"Avro"})," avant d'\xeatre mis dans le ",(0,i.jsx)(n.em,{children:"schema registry"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Si on utilise un outil qui ne supporte pas l'inf\xe9rence de sch\xe9ma, comme par exemple ",(0,i.jsx)(t.U,{children:"Google Cloud Dataflow"})," bas\xe9 sur ",(0,i.jsx)(t.U,{children:"Apache Beam"}),", alors il faudra g\xe9rer les sch\xe9mas \xe0 la main."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Dans le cas d'une ",(0,i.jsx)(n.strong,{children:"real-time pipeline"}),", on ne peut pas utiliser l'inf\xe9rence \xe0 cause du probl\xe8me de performance et de la quantit\xe9 de sch\xe9mas qui seraient g\xe9n\xe9r\xe9s.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Dans ce cas, les auteurs conseillent de ",(0,i.jsx)(n.strong,{children:"laisser les d\xe9veloppeurs qui g\xe9n\xe8rent les donn\xe9es de streaming maintenir le sch\xe9ma"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Pour pouvoir avoir du ",(0,i.jsx)(n.strong,{children:"monitoring"})," sur les changements de sch\xe9mas, le module de ",(0,i.jsx)(n.em,{children:"schema-management"})," peut cr\xe9er un log dans la partie ",(0,i.jsx)(n.em,{children:"Pipeline Activities"})," du ",(0,i.jsx)(n.em,{children:"metadata layer"})," \xe0 chaque fois qu'il trouve des donn\xe9es avec un sch\xe9ma qui a chang\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["M\xeame si l'ingestion et les ",(0,i.jsx)(n.em,{children:"common data processing"})," steps peuvent se “r\xe9parer” automatiquement, la suite du processing peut ne pas donner le r\xe9sultat voulu. Par exemple un rapport qui n'a plus les valeurs d'une colonne qui a \xe9t\xe9 enlev\xe9e par la source."]}),"\n",(0,i.jsx)(n.li,{children:"Il vaut mieux \xeatre alert\xe9 du changement de sch\xe9ma, et pr\xe9venir les \xe9quipes qui utilisent les donn\xe9es de cette source, avant qu'ils ne s'aper\xe7oivent du probl\xe8me par eux-m\xeames."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["C\xf4t\xe9 ",(0,i.jsx)(n.strong,{children:"impl\xe9mentation"})," du ",(0,i.jsx)(n.em,{children:"schema registry"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Apache Avro"})," est l'option conseill\xe9e par les auteurs pour servir de format de base pour l'ensemble des donn\xe9es de la ",(0,i.jsx)(n.em,{children:"data platform"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Son sch\xe9ma peut \xeatre \xe9crit et maintenu \xe0 la main."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Spark"})," peut aussi transformer son sch\xe9ma inf\xe9r\xe9 en sch\xe9ma ",(0,i.jsx)(t.U,{children:"Avro"})," automatiquement."]}),"\n",(0,i.jsx)(n.li,{children:"Ces sch\xe9mas peuvent \xeatre repr\xe9sent\xe9s par du simple JSON, et donc n'importe quelle DB qui supporte \xe7a peut les h\xe9berger."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Avro"})," a un tr\xe8s bon syst\xe8me de gestion des versions des sch\xe9mas."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"solutions cloud-natives"})," de type ",(0,i.jsx)(n.em,{children:"data catalog"})," permettent d'impl\xe9menter un ",(0,i.jsx)(n.em,{children:"schema registry"}),", mais ont des limitations.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La plupart sont surtout orient\xe9s data discovery, et manquent de fonctionnalit\xe9s concernant la gestion des versions des sch\xe9mas et le support d'",(0,i.jsx)(t.U,{children:"Avro"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Confluent Schema Registry"})," offre les fonctionnalit\xe9s de gestion de version des sch\xe9mas et un bon support d'",(0,i.jsx)(t.U,{children:"Avro"}),", mais il n\xe9cessite d'utiliser ",(0,i.jsx)(t.U,{children:"Kafka"}),", ou un outil compatible avec ",(0,i.jsx)(t.U,{children:"Kafka"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Donc si on utilise par exemple ",(0,i.jsx)(t.U,{children:"Kinesis"}),", ou bien si on ne fait pas de real-time, on ne pourra pas utiliser leur ",(0,i.jsx)(n.em,{children:"schema registry"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"solution maison"})," propos\xe9e par les auteurs consiste \xe0 avoir soit une DB, soit une API avec une DB derri\xe8re.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Le solution pure texte stock\xe9e dans le gestionnaire de version, similaire au reste de la configuration du metadata layer, ne peut pas marcher pour le ",(0,i.jsx)(n.em,{children:"schema registry"})," parce qu'il faut pouvoir le mettre \xe0 jour automatiquement."]}),"\n",(0,i.jsxs)(n.li,{children:["Comme DB, on peut utiliser les m\xeames ",(0,i.jsx)(t.U,{children:"Cosmos DB"}),", ",(0,i.jsx)(t.U,{children:"Datastore"})," et ",(0,i.jsx)(t.U,{children:"DynamoDB"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["La structure des objets de sch\xe9ma sera :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"ID"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Version"})," : l'",(0,i.jsx)(n.em,{children:"ID"})," et la ",(0,i.jsx)(n.em,{children:"Version"})," forment ensemble une cl\xe9 unique. L'",(0,i.jsx)(n.em,{children:"ID"})," en elle-m\xeame n'est donc pas unique pour \xe9viter d'avoir \xe0 mettre \xe0 jour en permanence les configurations des sources et destinations."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Schema"})," : le champ qui stocke le sch\xe9ma ",(0,i.jsx)(t.U,{children:"Avro"})," au format texte."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Created At"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Updated At"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Concernant la strat\xe9gie de ",(0,i.jsx)(n.strong,{children:"gestion de version des sch\xe9mas"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il existe deux types de compatibilit\xe9 entre les sch\xe9mas :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Backward-compatible"})," : la derni\xe8re version du sch\xe9ma doit permettre de lire l'ensemble des donn\xe9es existantes, y compris produites par un ancien sch\xe9ma.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Avro"})," impose des r\xe8gles pr\xe9cises pour garder la ",(0,i.jsx)(n.em,{children:"backward-compatibility"}),". Par exemple ajouter une colonne le permet, dans ce cas la lecture d'une donn\xe9e ancienne par un sch\xe9ma r\xe9cent donnera lieu \xe0 l'usage de la valeur par d\xe9faut pour la colonne manquante."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Forward-compatible"})," : une version plus ancienne du sch\xe9ma doit permettre de lire les donn\xe9es produites par une version plus r\xe9cente.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Si on reprend l'exemple de l'ajout de colonne, ",(0,i.jsx)(t.U,{children:"Avro"})," permet une forward-compatibility : l'ancien sch\xe9ma ignorera la nouvelle colonne au moment de la lecture de la nouvelle donn\xe9e."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le renommage de colonne est l'\xe9quivalent d'une cr\xe9ation de colonne, et d'une suppression de colonne. Donc si on a des valeurs par d\xe9faut dans le sch\xe9ma, elle sera \xe0 la fois ",(0,i.jsx)(n.em,{children:"backward-compatible"})," et ",(0,i.jsx)(n.em,{children:"forward-compatible"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Avro"})," supporte aussi automatiquement certains changements de types, par exemple un entier 32 bits vers 64 bits. On peut aussi soi-m\xeame impl\xe9menter d'autres r\xe8gles de conversion, mais les auteurs le d\xe9conseillent pour garder la complexit\xe9 des pipelines faible."]}),"\n",(0,i.jsxs)(n.li,{children:["Les r\xe8gles d'\xe9volution de sch\xe9ma d'",(0,i.jsx)(t.U,{children:"Avro"})," sont disponibles ",(0,i.jsx)(n.a,{href:"https://avro.apache.org/docs/1.7.7/spec.html#Schema+Resolution",children:"dans leur doc"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"common data transformation pipelines"})," ne vont en g\xe9n\xe9ral pas avoir besoin de la pr\xe9sence de colonnes sp\xe9cifiques, et donc vont \xeatre r\xe9silientes aux changements de sch\xe9mas.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"business processing pipelines"})," en revanche vont y \xeatre beaucoup plus sensibles."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les auteurs conseillent d'",(0,i.jsx)(n.strong,{children:"utiliser les anciens sch\xe9mas"})," dans les pipelines, et de passer aux nouveaux quand les changements de code ont \xe9t\xe9 faits. Ca veut dire s'efforcer \xe0 faire des changements de sch\xe9mas ",(0,i.jsx)(n.em,{children:"forward-compatibles"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Quelle que soit la strat\xe9gie, m\xeame si la pipeline ne casse pas gr\xe2ce aux r\xe8gles de ",(0,i.jsx)(n.em,{children:"backward / forward compatibility"}),", il est possible qu'on se retrouve avec des ",(0,i.jsx)(n.strong,{children:"erreurs logiques"})," dans nos transformations.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Par exemple, une colonne indiquant le nombre de ventes est renomm\xe9e, et peut continuer \xe0 \xeatre lue de mani\xe8re forward compatible avec la valeur par d\xe9faut ",(0,i.jsx)(n.code,{children:"NULL"}),". Mais le dashboard se mettra \xe0 montrer une absence de ventes."]}),"\n",(0,i.jsx)(n.li,{children:"Il n'y a pas de solution simple \xe0 ce probl\xe8me. Il faut avoir un syst\xe8me de monitoring et d'alerting efficaces, et pr\xe9venir les clients en amont que leurs dashboards risquent d'avoir des incoh\xe9rences le temps de mettre \xe0 jour le code."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Alors que les fichiers peuvent avoir chacun leur version de sch\xe9ma associ\xe9e, la donn\xe9e qui se trouve dans une table du ",(0,i.jsx)(n.strong,{children:"data warehouse"})," ne peut pas avoir plusieurs sch\xe9mas en m\xeame temps.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On ne peut pas simplement utiliser le ",(0,i.jsx)(n.em,{children:"schema registry"})," pour mettre \xe0 jour la table du ",(0,i.jsx)(n.em,{children:"data warehouse"}),", il va falloir le faire avec ",(0,i.jsx)(n.strong,{children:"du code dans le module de schema-management"}),", qui fait partie des ",(0,i.jsx)(n.em,{children:"common data transformations"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\xc7a veut dire que les changements de sch\xe9ma se feront quand m\xeame de mani\xe8re automatique, avec des r\xe8gles pr\xe9-\xe9tablies o\xf9 on va g\xe9n\xe9rer le bon SQL pour restructurer la table, en fonction de chaque changement de sch\xe9ma ",(0,i.jsx)(t.U,{children:"Avro"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On ne peut pas non plus appliquer les m\xeames r\xe8gles qu'avec les transformations de sch\xe9mas entre fichiers : dans le cas de suppression d'une colonne (ou de renommage, qui implique une suppression de fait), on va ",(0,i.jsx)(n.strong,{children:"garder l'ancienne colonne"})," quand m\xeame pour garder la donn\xe9e historique.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Parfois, quand les donn\xe9es ne sont pas trop grosses, il pourra \xeatre pr\xe9f\xe9rable de supprimer la colonne et de la recr\xe9er avec les donn\xe9es historiques et les nouvelles donn\xe9es dedans."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["C\xf4t\xe9 ",(0,i.jsx)(n.strong,{children:"data warehouses des cloud vendors"})," :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Redshift"})," et ",(0,i.jsx)(t.U,{children:"Azure Synapse"})," ont une approche similaire :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ils sont ancr\xe9s dans le monde du relationnel, et n\xe9cessitent la d\xe9finition du sch\xe9ma avant de charger de la donn\xe9e."}),"\n",(0,i.jsxs)(n.li,{children:["Ils supportent ",(0,i.jsx)(n.code,{children:"ALTER TABLE"})," pour faire des changements sur les tables."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Redshift"})," supporte ",(0,i.jsx)(t.U,{children:"Avro"})," mais sans inf\xe9rence \xe0 partir du sch\xe9ma, alors que ",(0,i.jsx)(t.U,{children:"Synapse"})," supporte seulement ",(0,i.jsx)(t.U,{children:"CSV"}),", ",(0,i.jsx)(t.U,{children:"ORC"})," et ",(0,i.jsx)(t.U,{children:"Parquet"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Google BigQuery"})," a une approche moins relationnelle, et permet d'inf\xe9rer le sch\xe9ma \xe0 partir de la donn\xe9e qu'on lui donne.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il va aussi ajouter des colonnes au sch\xe9ma automatiquement en inf\xe9rant le type, si on lui pr\xe9sente de la donn\xe9e qui a des colonnes en plus. Il le supporte pour ",(0,i.jsx)(t.U,{children:"Avro"}),", ",(0,i.jsx)(t.U,{children:"JSON"})," et ",(0,i.jsx)(t.U,{children:"Parquet"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"En revanche, il ne permet pas de modifier les tables apr\xe8s coup, sauf en ajoutant ou supprimant des colonnes, ce qui peut prendre du temps et co\xfbter cher."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"9---data-access-and-security",children:"9 - Data access and security"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les donn\xe9es d'analytics sont utilis\xe9es par de plus en plus de personnes au sein des entreprises, et par des moyens vari\xe9s.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["1 - Il y a les ",(0,i.jsx)(n.strong,{children:"utilisateurs humains"})," qui utilisent en g\xe9n\xe9ral des outils BI ou veulent ex\xe9cuter des requ\xeates SQL, et parfois des data scientists qui veulent acc\xe9der \xe0 la ",(0,i.jsx)(n.em,{children:"raw data"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["2 - Et il y a les ",(0,i.jsx)(n.strong,{children:"applications"})," qui utilisent la donn\xe9e par exemple pour des applications ML de recommandation ou de pr\xe9diction. Le ",(0,i.jsx)(n.em,{children:"data warehouse"})," ne suffit pas pour ces cas d'usage."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Le ",(0,i.jsx)(n.strong,{children:"data warehouse"})," reste quand m\xeame l'outil le plus commun pour acc\xe9der \xe0 la donn\xe9e d'analytics, du fait de la compatibilit\xe9 avec les outils BI et du support du SQL.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"AWS Redshift"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il s'agit d'un ",(0,i.jsx)(n.em,{children:"data warehouse"})," ",(0,i.jsx)(n.strong,{children:"distribut\xe9"}),", c'est-\xe0-dire qu'il r\xe9partit la donn\xe9e sur plusieurs machines.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Un nœud ",(0,i.jsx)(n.em,{children:"leader"})," re\xe7oit les requ\xeates et r\xe9partit le travail \xe0 faire et les donn\xe9es sur les autres nœuds."]}),"\n",(0,i.jsxs)(n.li,{children:["Les autres nœuds eux-m\xeames sont subdivis\xe9s en ",(0,i.jsx)(n.em,{children:"slices"}),". Ces slices peuvent \xeatre d\xe9plac\xe9s de nœud en nœud, pour \xe9quilibrer la capacit\xe9 par du ",(0,i.jsx)(n.em,{children:"rebalancing"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Quand on cr\xe9e une table, on peut indiquer sa propri\xe9t\xe9 ",(0,i.jsx)(n.em,{children:"DISTSTYLE"})," pour choisir la mani\xe8re dont ses donn\xe9es seront distribu\xe9es sur les nœuds. C'est le r\xe9glage de performance le plus impactant.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"ALL"})," : une copie de la table est cr\xe9\xe9e sur chaque nœud. On ne peut le faire qu'avec les petites tables qui sont souvent l'objet de jointures."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"EVEN"})," : les rows de la table sont r\xe9partis de mani\xe8re \xe9quitable sur les nœuds."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"KEY"})," : permet d'indiquer une colonne dont les valeurs identiques donneront lieu \xe0 ce que la donn\xe9e soit stock\xe9e sur la m\xeame machine."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"AUTO"})," : vaut ",(0,i.jsx)(n.em,{children:"ALL"})," au d\xe9but, et passe \xe0 ",(0,i.jsx)(n.em,{children:"EVEN"})," quand la table grandit."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il est bas\xe9 sur ",(0,i.jsx)(t.U,{children:"PostgreSQL"})," et pr\xe9sente les caract\xe9ristiques des DB relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il ne supporte que les types “primitifs”, c'est-\xe0-dire pas les tableaux ou les objets imbriqu\xe9s. Il est donc peu adapt\xe9 \xe0 de la donn\xe9e JSON, avec laquelle les optimisations d'encodage ou de distribution dans les nœuds par cl\xe9 ne pourront pas servir."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["On peut optimiser la taille des donn\xe9es en choisissant le type d'encodage pour les colonnes : par exemple dans le cas o\xf9 une colonne peut avoir seulement quelques valeurs possibles, l'encodage ",(0,i.jsx)(n.em,{children:"byte-dictionary"})," permet de limiter la taille de ces donn\xe9es."]}),"\n",(0,i.jsxs)(n.li,{children:["Il poss\xe8de une fonctionnalit\xe9 appel\xe9e ",(0,i.jsx)(t.U,{children:"Spectrum"}),", qui permet de cr\xe9er une table dans ",(0,i.jsx)(t.U,{children:"Redshift"}),", dont les donn\xe9es sont sur ",(0,i.jsx)(t.U,{children:"S3"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ca permet d'\xe9viter d'utiliser des ressources CPU et de l'espace sur le ",(0,i.jsx)(n.em,{children:"data warehouse"}),", pour des donn\xe9es qu'on veut juste explorer par exemple."]}),"\n",(0,i.jsxs)(n.li,{children:["Les performances seront du coup moins bonnes que les donn\xe9es qui sont sur les nœuds ",(0,i.jsx)(t.U,{children:"Redshift"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les auteurs recommandent de cr\xe9er une DB d\xe9di\xe9e sur ",(0,i.jsx)(t.U,{children:"Redshift"})," pour regrouper ces tables qui pointent vers ailleurs."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Azure Synapse"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["C'est une DB distribu\xe9e comme ",(0,i.jsx)(t.U,{children:"Redshift"}),", avec un ",(0,i.jsx)(n.em,{children:"control node"})," principal qui re\xe7oit les requ\xeates, et qui fait appel aux autres nœuds.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il y a une ",(0,i.jsx)(n.strong,{children:"s\xe9paration storage / compute"}),". Les donn\xe9es sont s\xe9par\xe9es en 60 distributions, et sont associ\xe9es \xe0 des ",(0,i.jsx)(n.em,{children:"compute nodes"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Il n'est pas compl\xe8tement \xe9lastique, puisque pour redimensionner le cluster, il faut tout arr\xeater, et \xe7a peut prendre du temps."}),"\n",(0,i.jsxs)(n.li,{children:["Les tables peuvent \xeatre configur\xe9es pour la r\xe9partition de leurs donn\xe9es, de la m\xeame mani\xe8re que ",(0,i.jsx)(t.U,{children:"Redshift"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"REPLICATE"})," : l'\xe9quivalent de ",(0,i.jsx)(n.em,{children:"ALL"}),", c'est-\xe0-dire copier sur chaque nœud."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"ROUND ROBIN"})," : l'\xe9quivalent de ",(0,i.jsx)(n.em,{children:"EVEN"}),", c'est-\xe0-dire r\xe9partir entre les nœuds."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"HASH"})," : l'\xe9quivalent de ",(0,i.jsx)(n.em,{children:"KEY"}),", c'est-\xe0-dire sp\xe9cifier une colonne dont les valeurs permettront de r\xe9partir les donn\xe9es."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il pr\xe9sente des caract\xe9ristiques relationnelles.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il supporte seulement les types primitifs, et fournit des fonctions de parsing pour JSON, mais au prix de nombreuses optimisations perdues."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il a une fonctionnalit\xe9 similaire \xe0 ",(0,i.jsx)(t.U,{children:"Spectrum"}),", configurable par la notion de ",(0,i.jsx)(n.strong,{children:"pools"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"SQL pool"})," repr\xe9sente l'utilisation normale du ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"SQL on-demand pool"})," permet de faire des requ\xeates sur des donn\xe9es sur ",(0,i.jsx)(t.U,{children:"Azur Blob Storage"})," au format ",(0,i.jsx)(t.U,{children:"Parquet"}),", CSV ou JSON."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Spark pool"})," permet de faire des requ\xeates avec ",(0,i.jsx)(t.U,{children:"Spark"}),", sur des donn\xe9es qui sont dans ",(0,i.jsx)(t.U,{children:"Azur Blob Storage"}),". Ils permettent l'auto-scaling, mais n\xe9cessitent que 3 nœuds tournent en permanence."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Google BigQuery"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"BigQuery"})," est un ",(0,i.jsx)(n.strong,{children:"peu plus “manag\xe9”"})," que les deux autres, dans la mesure o\xf9 il n'y a pas de besoin de planifier la capacit\xe9 dont on aura besoin \xe0 l'avance.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La puissance de calcul est “provisionn\xe9e” ",(0,i.jsx)(n.strong,{children:"\xe0 chaque requ\xeate"}),", gr\xe2ce \xe0 des groupes de dizaines de milliers de nœuds qui tournent en permanence dans l'infra de Google."]}),"\n",(0,i.jsxs)(n.li,{children:["Comme il est plus manag\xe9, on peut aussi moins facilement contr\xf4ler la mani\xe8re dont les donn\xe9es d'une table sont r\xe9parties au sein des nœuds.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["On a quand m\xeame la notion de ",(0,i.jsx)(n.em,{children:"partitioning"})," qui permet de r\xe9partir les donn\xe9es selon les valeurs d'une colonne."]}),"\n",(0,i.jsxs)(n.li,{children:["Et de ",(0,i.jsx)(n.em,{children:"clustering"})," qui permet d'organiser physiquement les donn\xe9es de mani\xe8re \xe0 rendre les requ\xeates qu'on fait le plus souvent plus efficaces."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Le pricing se fait aussi sur la quantit\xe9 de donn\xe9es trait\xe9e, ce qui peut \xeatre avantageux quand on a de petits besoins, mais rend les co\xfbts difficilement pr\xe9dictibles."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les nœuds de calcul sont sur des machines diff\xe9rentes des nœuds de stockage : on n'a ",(0,i.jsxs)(n.strong,{children:["pas de ",(0,i.jsx)(n.em,{children:"data locality"})]}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"C'est moins rapide que si la donn\xe9e \xe9tait locale, mais \xe7a \xe9vite d'avoir \xe0 recopier la donn\xe9e \xe0 chaque rebalancing. La donn\xe9e est acc\xe9d\xe9e via le r\xe9seau local de Google qui est suffisamment performant pour que \xe7a passe."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"BigQuery"})," vient initialement plut\xf4t d'un syst\xe8me permettant de traiter des fichiers de log, et non pas un syst\xe8me relationnel comme les deux autres.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il a un ",(0,i.jsx)(n.strong,{children:"support natif des structures imbriqu\xe9es"}),", et peut traiter le JSON comme une structure et pas juste du texte, avec la possibilit\xe9 d'appliquer des traitements sur les attributs."]}),"\n",(0,i.jsx)(n.li,{children:"Il est du coup moins facilement compatible avec les outils BI, il faudra passer par une API REST."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les grandes organisations peuvent tirer parti de l'utilisation de ",(0,i.jsx)(n.strong,{children:"plusieurs cloud providers"}),", mais pour les petites le ",(0,i.jsx)(n.strong,{children:"co\xfbt op\xe9rationnel"})," n'en vaut pas la peine. Le choix du ",(0,i.jsx)(n.em,{children:"data warehouse"})," d\xe9pendra donc en g\xe9n\xe9ral du choix du cloud provider pour le reste de l'infra."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.strong,{children:"applications"})," utilisent de plus en plus la data dans des syst\xe8mes customer-facing, par exemple dans des syst\xe8mes de recommandation.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Donner \xe0 l'application un ",(0,i.jsx)(n.strong,{children:"acc\xe8s au data warehouse serait une mauvaise id\xe9e"})," pour plusieurs raisons :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"data warehouses"})," ne sont pas con\xe7ues pour offrir des ",(0,i.jsx)(n.strong,{children:"latences"})," se comptant en millisecondes, mais en secondes voire minutes sur de grandes quantit\xe9s de donn\xe9es."]}),"\n",(0,i.jsxs)(n.li,{children:["Ils ne sont pas con\xe7us pour supporter un trop ",(0,i.jsx)(n.strong,{children:"grand nombre de transactions"})," en m\xeame temps (par exemple des dizaines ou centaines de milliers) comme pourrait le n\xe9cessiter une application."]}),"\n",(0,i.jsxs)(n.li,{children:["Si l'application est compromise, l'ensemble du contenu du ",(0,i.jsx)(n.em,{children:"data warehouse"})," pourrait fuiter, alors que si l'application a seulement acc\xe8s \xe0 une DB qui a ce dont elle a besoin, on aura une meilleure ",(0,i.jsx)(n.strong,{children:"s\xe9curit\xe9"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - Cloud relational databases"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Chaque cloud provider a ses services de DBs manag\xe9es, qui tiennent sans probl\xe8mes jusqu'\xe0 1 TB. Au-del\xe0 de \xe7a, ou si on a besoin de situer les machines g\xe9ographiquement, il faut une DB distribu\xe9e."}),"\n",(0,i.jsxs)(n.li,{children:["AWS propose ",(0,i.jsx)(t.U,{children:"Relational Database Service"})," (RDS) pour ",(0,i.jsx)(t.U,{children:"PostgreSQL"}),", ",(0,i.jsx)(t.U,{children:"MySQL"}),", ",(0,i.jsx)(t.U,{children:"MariaDB"}),", ",(0,i.jsx)(t.U,{children:"Oracle"})," et ",(0,i.jsx)(t.U,{children:"SQL Server"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il propose ",(0,i.jsx)(t.U,{children:"Aurora"})," comme DB distribu\xe9e, compatible avec ",(0,i.jsx)(t.U,{children:"MySQL"})," et ",(0,i.jsx)(t.U,{children:"PostgreSQL"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["GCP propose ",(0,i.jsx)(t.U,{children:"Google Cloud SQL"}),", qui supporte ",(0,i.jsx)(t.U,{children:"MySQL"}),", ",(0,i.jsx)(t.U,{children:"PostgreSQL"})," et ",(0,i.jsx)(t.U,{children:"SQL Server"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il propose ",(0,i.jsx)(t.U,{children:"Google Cloud Spanner"})," pour la version distribu\xe9e."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Azure propose ",(0,i.jsx)(t.U,{children:"Azure SQL Database"}),", qui supporte ",(0,i.jsx)(t.U,{children:"MySQL"}),", ",(0,i.jsx)(t.U,{children:"PostgreSQL"})," et ",(0,i.jsx)(t.U,{children:"SQL Server"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il propose ",(0,i.jsx)(t.U,{children:"HyperScale"})," pour la version distribu\xe9e, disponible seulement pour ",(0,i.jsx)(t.U,{children:"SQL Server"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Cloud key / value data stores"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Les services key/value offrent une faible latence pour ins\xe9rer et retrouver des valeurs par leur cl\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ils sont souvent utilis\xe9s par les nouveaux projets pour pouvoir it\xe9rer vite sans avoir de migration \xe0 faire."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les cloud providers proposent soit une version ",(0,i.jsx)(n.em,{children:"pay per use"})," plus avantageuse en cas de faible utilisation, et une version ",(0,i.jsx)(n.em,{children:"pay per provisioned capacity"})," plus avantageuse en cas de grosse utilisation."]}),"\n",(0,i.jsxs)(n.li,{children:["AWS propose ",(0,i.jsx)(t.U,{children:"DynamoDB"}),", qui reste performant quel que soit le scale, et offre les deux types de facturation."]}),"\n",(0,i.jsxs)(n.li,{children:["GCP propose ",(0,i.jsx)(t.U,{children:"Datastore"})," qui est similaire \xe0 ",(0,i.jsx)(t.U,{children:"DynamoDB"})," et qui propose du ",(0,i.jsx)(n.em,{children:"pay per use"}),", et ",(0,i.jsx)(t.U,{children:"Cloud Bigtable"})," qui ne permet pas de mettre de contrainte de types sur les donn\xe9es, et supporte le ",(0,i.jsx)(n.em,{children:"price per provisioned capacity"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Cloud Bigtable"})," est d'ailleurs compatible avec ",(0,i.jsx)(t.U,{children:"HBase"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Azure propose ",(0,i.jsx)(t.U,{children:"CosmosDB"}),", qui a la particularit\xe9 de supporter les API clientes de ",(0,i.jsx)(t.U,{children:"MongoDB"}),", ",(0,i.jsx)(t.U,{children:"Cassandra"}),", SQL et de graph API, ce qui rend le portage depuis ces technos facile."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - Full-text search services"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Dans le cas o\xf9 la fonctionnalit\xe9 de notre application est de permettre une recherche dans la donn\xe9e, il existe ",(0,i.jsx)(t.U,{children:"Solr"})," et ",(0,i.jsx)(t.U,{children:"Elasticsearch"}),", tous deux bas\xe9s sur ",(0,i.jsx)(t.U,{children:"Lucene"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple, si on veut chercher quelque chose de similaire \xe0 ce qui est tap\xe9 par l'utilisateur."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["AWS propose ",(0,i.jsx)(t.U,{children:"CloudSearch"}),", Azure propose ",(0,i.jsx)(t.U,{children:"Azure Search"}),", et GCP ne propose rien de manag\xe9 au moment de l'\xe9criture du livre."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - In-memory cache"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les caches permettent des temps d'acc\xe8s inf\xe9rieurs \xe0 la milliseconde gr\xe2ce au stockage en RAM. Ils doivent \xeatre li\xe9s \xe0 une DB persistante pour pouvoir \xeatre reconstruits."}),"\n",(0,i.jsxs)(n.li,{children:["AWS propose ",(0,i.jsx)(t.U,{children:"ElasticCache"}),", qui supporte ",(0,i.jsx)(t.U,{children:"Memcached"})," et ",(0,i.jsx)(t.U,{children:"Redis"}),", GCP propose ",(0,i.jsx)(t.U,{children:"Memorystore"})," qui supporte ",(0,i.jsx)(t.U,{children:"Memcached"}),", et Azure propose ",(0,i.jsx)(t.U,{children:"Azure Cache"})," qui supporte ",(0,i.jsx)(t.U,{children:"Redis"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Les mod\xe8les de ",(0,i.jsx)(n.strong,{children:"machine learning"})," n\xe9cessitent l'acc\xe8s \xe0 une grande quantit\xe9 de donn\xe9es vari\xe9e, une grande puissance de calcul, et l'acc\xe8s \xe0 des outils sp\xe9cifiques. La ",(0,i.jsx)(n.em,{children:"cloud data platform"})," est parfaitement adapt\xe9e \xe0 \xe7a.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Dans les plateformes traditionnelles, les data scientists passent 80% de leur temps \xe0 r\xe9cup\xe9rer la donn\xe9e sur leur machine, et la nettoyer et la transformer pour qu'elle puisse \xeatre interpr\xe9t\xe9e par leurs outils.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ils vont ensuite faire des tests exploratoires pour comprendre ce qu'ils peuvent faire ce cette donn\xe9e."}),"\n",(0,i.jsx)(n.li,{children:"Puis ils s\xe9parent la donn\xe9e en deux : la donn\xe9e d'entra\xeenement et la donn\xe9e de validation."}),"\n",(0,i.jsxs)(n.li,{children:["Ils vont faire un cycle ",(0,i.jsx)(n.em,{children:"entra\xeenement / validation"})," o\xf9 ils vont plusieurs fois am\xe9liorer le mod\xe8le puis le tester contre la donn\xe9e de validation.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Cette validation permet d'\xe9viter l'",(0,i.jsx)(n.em,{children:"overfitting"})," o\xf9 le mod\xe8le ne serait bon que sur les donn\xe9es avec lesquelles il s'est entra\xeen\xe9."]}),"\n",(0,i.jsx)(n.li,{children:"Faire l'entra\xeenement sur leur machine locale leur prend beaucoup de temps."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Une fois que le mod\xe8le est fonctionnel, il faut le rendre production-ready pour le d\xe9ployer, en ajoutant du logging, de la gestion d'erreurs etc. ce qui est souvent difficile."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"cloud data platform"})," aide au d\xe9veloppement de mod\xe8les ML.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Une bonne partie de la mise en forme et de la validation des donn\xe9es est faite dans l'",(0,i.jsx)(n.em,{children:"ingestion layer"})," et dans le ",(0,i.jsx)(n.em,{children:"processing layer"})," avec les ",(0,i.jsx)(n.em,{children:"common data transformation steps"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Les data scientists peuvent copier la donn\xe9e comme ils veulent dans le ",(0,i.jsx)(n.em,{children:"storage"})," de la ",(0,i.jsx)(n.em,{children:"cloud data platform"}),", et faire de l'exploration ou du processing sans t\xe9l\xe9charger les donn\xe9es en local."]}),"\n",(0,i.jsx)(n.li,{children:"Ils peuvent collaborer sur un m\xeame jeu de donn\xe9es puisqu'il est dans le cloud, et peuvent avoir acc\xe8s \xe0 de la donn\xe9e de production en grande quantit\xe9."}),"\n",(0,i.jsxs)(n.li,{children:["Chacun des cloud vendors fournit un service de ML permettant de g\xe9rer un projet ML de bout en bout, et de mieux collaborer entre data scientists : ",(0,i.jsx)(t.U,{children:"SageMaker"})," chez AWS, ",(0,i.jsx)(t.U,{children:"AI Platform"})," chez GCP, et ",(0,i.jsx)(t.U,{children:"Azure ML"})," chez Azure."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"business intelligence"})," et le ",(0,i.jsx)(n.strong,{children:"reporting"})," sont en g\xe9n\xe9ral le premier usage de la donn\xe9e de type analytics.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ces outils n\xe9cessitent souvent que la donn\xe9e soit ",(0,i.jsx)(n.strong,{children:"relationnelle"}),", c'est-\xe0-dire que chaque donn\xe9e soit dans sa colonne avec la table “\xe0 plat” reli\xe9e \xe0 d'autres tables par des cl\xe9s \xe9trang\xe8res, plut\xf4t que d'avoir des donn\xe9es imbriqu\xe9es comme dans du JSON.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"BigQuery"})," commence \xe0 \xeatre support\xe9 par des outils comme ",(0,i.jsx)(t.U,{children:"Tableau"}),", mais tous ne le supportent pas correctement."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Bien que de nombreux outils BI supportent ",(0,i.jsx)(t.U,{children:"Spark SQL"}),", et pourraient se ",(0,i.jsxs)(n.strong,{children:["brancher directement sur le ",(0,i.jsx)(n.em,{children:"data lake"})]}),", ",(0,i.jsx)(n.strong,{children:"les auteurs le d\xe9conseillent"})," parce que \xe7a rendrait l'interface de ces outils peu interactive et lente. Se brancher sur le ",(0,i.jsx)(n.em,{children:"data warehouse"})," est bien plus adapt\xe9 pour cette raison."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(t.U,{children:"Excel"})," peut se brancher sur le ",(0,i.jsx)(n.em,{children:"data warehouse"})," gr\xe2ce \xe0 son API JDBC/ODBC, mais c'est un outil qui tourne sur une machine locale, donc il sera limit\xe9 sur la quantit\xe9 de donn\xe9es, et t\xe9l\xe9charger les donn\xe9es sur sa machine locale pose des probl\xe8mes de performance."]}),"\n",(0,i.jsxs)(n.li,{children:["On voit souvent des ",(0,i.jsx)(n.strong,{children:"outils externes"}),", par exemple chez d'autres cloud providers, acc\xe9der \xe0 la donn\xe9e de la ",(0,i.jsx)(n.em,{children:"cloud data platform"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut faire attention aux ",(0,i.jsx)(n.strong,{children:"co\xfbts de sortie des donn\xe9es"})," (",(0,i.jsx)(n.em,{children:"data egress costs"}),"), que chaque cloud provider applique."]}),"\n",(0,i.jsxs)(n.li,{children:["Chaque cloud provider a sa solution BI : ",(0,i.jsx)(t.U,{children:"Azure Power BI"})," qui est tr\xe8s connu, AWS ",(0,i.jsx)(t.U,{children:"QuickSight"}),", et ",(0,i.jsx)(t.U,{children:"DataStudio"})," et ",(0,i.jsx)(t.U,{children:"Looker BI"})," pour GCP."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"s\xe9curit\xe9"})," est essentielle pour une plateforme data.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il vaut mieux \xe9viter les acc\xe8s ad hoc d\xe8s qu'il y a un besoin, mais plut\xf4t utiliser les concepts de ",(0,i.jsx)(n.strong,{children:"Users"}),", ",(0,i.jsx)(n.strong,{children:"Groups"})," et ",(0,i.jsx)(n.strong,{children:"Roles"})," fournis par les cloud providers.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les groupes facilitent grandement la gestion des permissions, il vaut mieux les configurer \xe0 ce niveau l\xe0 dans la mesure du possible."}),"\n",(0,i.jsxs)(n.li,{children:["Une bonne pratique est de ne fournir que les permissions n\xe9cessaires \xe0 chaque type d'utilisateur (",(0,i.jsx)(n.em,{children:"principle of least privilege"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Il existe des outils cloud-native pour l'authentification, \xe0 la place des mots de passe, par exemple ",(0,i.jsx)(t.U,{children:"Azure Active Directory"}),". Les auteurs conseillent de les utiliser quand c'est possible."]}),"\n",(0,i.jsxs)(n.li,{children:["Certaines configurations permettent de rendre des services accessibles publiquement. Pour limiter le risque, on peut faire diverses choses comme des audits, ou l'utilisation du principe ",(0,i.jsx)(n.em,{children:"infrastructure-as-code"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Dans le cas o\xf9 on a des donn\xe9es sensibles, il ne faut pas h\xe9siter \xe0 chiffrer des colonnes particuli\xe8res."}),"\n",(0,i.jsx)(n.li,{children:"Une autre solution peut \xeatre de limiter l'acc\xe8s r\xe9seau \xe0 la donn\xe9e, dans le cas o\xf9 les utilisateurs seraient sur un r\xe9seau particulier."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"10---fueling-business-value-with-data-platforms",children:"10 - Fueling business value with data platforms"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.em,{children:"data platform"})," doit \xeatre organis\xe9e autour d'une ",(0,i.jsx)(n.strong,{children:"data strategy"}),", c'est-\xe0-dire \xeatre au service des objectifs business.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Parmi les grands objectifs business, on trouve :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Augmenter les revenus."}),"\n",(0,i.jsx)(n.li,{children:"Augmenter l'efficacit\xe9 op\xe9rationnelle."}),"\n",(0,i.jsx)(n.li,{children:"Am\xe9liorer l'exp\xe9rience utilisateur."}),"\n",(0,i.jsx)(n.li,{children:"Permettre l'innovation."}),"\n",(0,i.jsx)(n.li,{children:"Am\xe9liorer la conformit\xe9."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Exemples :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Dans le cas d'une entreprise de jeux vid\xe9o qui veut maximiser les achats ou la publicit\xe9 in-game, la strat\xe9gie peut \xeatre d'optimiser la plateforme data pour du real-time processing des \xe9v\xe9nements du jeu."}),"\n",(0,i.jsx)(n.li,{children:"Dans le cas d'une entreprise mini\xe8re qui veut r\xe9duire ses co\xfbts op\xe9rationnels, la strat\xe9gie peut \xeatre d'optimiser la plateforme pour ing\xe9rer la donn\xe9e des capteurs des engins miniers, et pr\xe9dire quand faire la maintenance."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.strong,{children:"maturit\xe9 data"})," d'une organisation passe par 4 \xe9tapes.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"1 - See"})," : le business veut voir des rapports et des dashboards pour mieux comprendre ce qui se passe par rapport \xe0 ce qui s'est pass\xe9 dans le pass\xe9.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Traditionnellement les rapports sont cr\xe9\xe9s par des personnes sp\xe9cialistes de ces outils, \xe0 la demande du business."}),"\n",(0,i.jsxs)(n.li,{children:["Dans les plateformes modernes, on applique le principe ",(0,i.jsx)(n.em,{children:"Bring Your Own Analytics (BYOA)"}),", o\xf9 ",(0,i.jsx)(n.strong,{children:"les personnes du business utilisent leurs propres outils"})," qu'ils branchent sur la ",(0,i.jsx)(n.em,{children:"data platform"}),", pour cr\xe9er leurs rapports."]}),"\n",(0,i.jsxs)(n.li,{children:["Ces outils sont branch\xe9s sur le ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2 - Predict"})," : une fois qu'on a ce qui s'est pass\xe9 et se passe, on veut pr\xe9dire ce qui va se passer, par exemple avec du ML.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Il faut que la plateforme puisse proposer une grande quantit\xe9 de donn\xe9es."}),"\n",(0,i.jsxs)(n.li,{children:["Les donn\xe9es brutes vont \xeatre plut\xf4t sur le ",(0,i.jsx)(n.em,{children:"data lake"}),", et les donn\xe9es raffin\xe9es sur le ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3 - Do"})," : on va donner le r\xe9sultat des deux premi\xe8res \xe9tapes \xe0 des syst\xe8mes pour d\xe9clencher des actions.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\xc7a peut \xeatre du ML avec par exemple des syst\xe8mes de recommandation, ou m\xeame simplement de la donn\xe9e qui est d\xe9plac\xe9e vers le syst\xe8me op\xe9rationnel pour servir les clients."}),"\n",(0,i.jsxs)(n.li,{children:["Le fait de d\xe9placer des donn\xe9es du monde analytics au monde op\xe9rationnel s'appelle l'",(0,i.jsx)(n.strong,{children:"orchestration"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"\xc7a implique que le syst\xe8me qui utilise cette donn\xe9e soit disponible et r\xe9ponde aux exigences d'un syst\xe8me de production."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"4 - Create"})," : la donn\xe9e initialement collect\xe9e comme analytics devient la source pour un nouveau produit.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Par exemple, une banque qui a collect\xe9 des donn\xe9es pour am\xe9liorer l'exp\xe9rience utilisateur en aidant les agents \xe0 anticiper les r\xe9actions des clients, s'est rendue compte qu'elle pouvait l'utiliser aussi pour am\xe9liorer l'app mobile."}),"\n",(0,i.jsx)(n.li,{children:"Autre exemple, une entreprise de s\xe9curit\xe9 s'est servie des dashboards construits pour visualiser les intrusions, pour montrer aux clients en quoi elle leur apportait de la valeur avec tous les risques qu'elle a \xe9vit\xe9s."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Certains challenges non techniques peuvent faire ",(0,i.jsx)(n.strong,{children:"\xe9chouer la cloud data platform"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"D\xe9livrer de valeur business rapidement"})," : le business a besoin d'it\xe9rations qui r\xe9solvent de vrais besoins au bout de quelques mois maximum.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Les auteurs conseillent de partir d'un use-case pas trop complexe li\xe9 \xe0 la data, et de l'impl\xe9menter en faisant avancer la plateforme. Et on passe comme \xe7a de use-cases en use-cases."}),"\n",(0,i.jsx)(n.li,{children:"L'alternative moins int\xe9ressante c'est d'ing\xe9rer toutes les sources possibles, pour finir par trouver des cas d'usage avec les sources qu'on supporte d\xe9j\xe0."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Faire adopter la plateforme par les utilisateurs"})," : les utilisateurs ont peut-\xeatre d\xe9j\xe0 leur mani\xe8re de travailler avec les analytics, en particulier la production de rapport traditionnelle plut\xf4t que la data self-service.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il y a les ",(0,i.jsx)(n.em,{children:"early adopters"})," qui supportent la nouvelle plateforme, les ",(0,i.jsx)(n.em,{children:"blockers"})," qui montrent leur scepticisme publiquement, les ",(0,i.jsx)(n.em,{children:"chickens"})," qui ont peur de tout ce qui est nouveau, et les ",(0,i.jsx)(n.em,{children:"avoiders"})," qui ne veulent pas toucher \xe0 ce qui est nouveau."]}),"\n",(0,i.jsxs)(n.li,{children:["Quelques conseils pour avoir une meilleure adoption :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["S'assurer que les premiers utilisateurs soient \xe0 la fois des ",(0,i.jsx)(n.em,{children:"early adopters"})," et des employ\xe9s influents."]}),"\n",(0,i.jsxs)(n.li,{children:["Apr\xe8s les ",(0,i.jsx)(n.em,{children:"early adopters"}),", on peut aider un ",(0,i.jsx)(n.em,{children:"blocker"})," pour tenter de le retourner. Si \xe7a marche c'est excellent pour le projet."]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"chickens"})," ont besoin de beaucoup de formation."]}),"\n",(0,i.jsxs)(n.li,{children:["Les ",(0,i.jsx)(n.em,{children:"avoiders"})," mettront plus de temps, mais c'est OK."]}),"\n",(0,i.jsxs)(n.li,{children:["Ce serait bien d'avoir un sponsor C-level qui soutient le projet, et cr\xe9e de la visibilit\xe9 pour lui.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Exemple : Disney avait lanc\xe9 un concours interne o\xf9 les utilisateurs data pouvaient montrer leurs r\xe9sultats avec la plateforme et \xeatre r\xe9compens\xe9s."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Obtenir et garder la confiance des utilisateurs"})," : il faut que la qualit\xe9 de la donn\xe9e soit suffisamment bonne pour que les utilisateurs aient confiance en elle.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Prendre en compte l'utilisateur de la donn\xe9e et ses besoins rendre dans le cadre de la ",(0,i.jsx)(n.strong,{children:"data governance"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Parmi les m\xe9triques de qualit\xe9 \xe0 surveiller, il peut y avoir le pourcentage de donn\xe9es correctes, les champs obligatoires remplis, la pr\xe9cision, la consistance, l'int\xe9grit\xe9 de la donn\xe9e etc."}),"\n",(0,i.jsxs)(n.li,{children:["Quand la qualit\xe9 qu'on s'est fix\xe9e n'est plus respect\xe9e, il faut :","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"1 - pr\xe9venir les consommateurs de la donn\xe9e."}),"\n",(0,i.jsx)(n.li,{children:"2 - Mettre l'\xe9quipe sur le coup pour r\xe9gler le probl\xe8me au plus vite."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"\xc9viter la formation de d'un silo autour de la data platform"})," : la responsabilit\xe9 de la donn\xe9e, des r\xe8gles de qualit\xe9 et la mesure de la qualit\xe9, les SLA etc. doivent \xeatre driv\xe9es par le business.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"La donn\xe9e part des sources potentiellement ext\xe9rieures, et irrigue l'organisation \xe0 travers divers syst\xe8mes. On ne peut pas consid\xe9rer que la responsabilit\xe9 de l'\xe9quipe technique s'arr\xeate au moment o\xf9 la donn\xe9e sort de la plateforme."}),"\n",(0,i.jsx)(n.li,{children:"Il faut constituer des \xe9quipes pluridisciplinaires capables de prendre en charge la responsabilit\xe9 du syst\xe8me de bout en bout : le fonctionnement de la plateforme et l'utilisation de la donn\xe9e."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Prendre en compte les co\xfbts"})," : pour que la plateforme soit un succ\xe8s, il faut adopter le point de vue de l'entreprise.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il faut s'int\xe9resser aux mani\xe8res d'optimiser les co\xfbts des services cloud (",(0,i.jsx)(n.em,{children:"FinOps"}),"), et comprendre les trade-offs qui y sont li\xe9s.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Et \xe0 l'inverse s'int\xe9resser \xe0 ce que la plateforme permet de rapporter d'un point de vue business."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Exemple : une grande entreprise de t\xe9l\xe9communications recueille des donn\xe9es IoT.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La bonne pratique est de faire le processing dans le ",(0,i.jsx)(n.em,{children:"data lake"})," avec ",(0,i.jsx)(t.U,{children:"Spark"}),", mais il se trouve que le business avait un deal avec GCP pour une utilisation illimit\xe9e de ",(0,i.jsx)(t.U,{children:"BigQuery"})," \xe0 prix fixe."]}),"\n",(0,i.jsxs)(n.li,{children:["Dans ce cas, la bonne chose \xe0 faire sera sans doute de faire des concessions sur le design, et faire le processing dans le ",(0,i.jsx)(n.em,{children:"data warehouse"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}n.default=(0,r.j)({MDXContent:function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,l.a)(),e.components);return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)},pageOpts:{filePath:"pages/books/designing-cloud-data-platforms.mdx",route:"/books/designing-cloud-data-platforms",title:"Designing Cloud Data Platforms",headings:d},pageNextRoute:"/books/designing-cloud-data-platforms"})},7854:function(e,n,s){"use strict";s.d(n,{f:function(){return r}});var i=s(5893);function r(e){let{children:n}=e;return(0,i.jsx)("em",{style:{color:"#a64d79",fontWeight:"bold",fontStyle:"italic"},children:n})}},8397:function(e,n,s){"use strict";s.d(n,{U:function(){return r}});var i=s(5893);function r(e){let{children:n}=e;return(0,i.jsx)("em",{style:{color:"#3d85c6",fontWeight:"bold",fontStyle:"normal"},children:n})}}},function(e){e.O(0,[673,888,774,179],function(){return e(e.s=9979)}),_N_E=e.O()}]);