{"/books/continuous-discovery-habits":{"title":"Continuous Discovery Habits: Discover Products that Create Customer Value and Business Value","data":{"":"","part-i---what-is-continuous-discovery#Part I - What is continuous discovery":"","1---the-what-and-why-of-continuous-discovery#1 - The what and why of continuous discovery":"Les équipes produit font deux activités distinctes : la discovery où il s’agit de décider de ce qui sera fait, et la delivery où il s’agit d’implémenter effectivement ces choses.\nLa plupart des entreprises investissent lourdement sur la delivery, mais peu sur la discovery pour bien choisir quoi construire.\n\n\nUn peu d’historique :\nDans les temps “anciens” la discovery se faisait exclusivement par les business leaders, qui décidaient de budgéter une fois par an. Les projets étaient à la fois hors délai, et servaient mal les besoins des utilisateurs.\nEn 2001, un groupe de développeurs a écrit le manifeste agile, qui poussait notamment l’idée de cycles courts avec un feedback rapide. Leur idée s’est répandue, mais les business leaders avaient du mal à lâcher la prise de décision sur ce qui doit être produit (discovery).\nPetit à petit, on en est arrivé à l’idée que la discovery doit être de la responsabilité des product managers plutôt que des business leaders, puis enfin de l’équipe produit plutôt que du product manager.\nOn a aussi raccourci le cycle de discovery jusqu’à co-créer la discovery avec le client sur une base continue plutôt que de valider les idées qu’on a au préalable. C’est ça qu’on appelle la continuous discovery.\nNDLR : on a ici une idée similaire à celle du DDD à propos de la collaboration permanente avec les domain experts, qui apportent leur expertise du domaine pendant que l’équipe produit apporte son expertise technique et d’analyse dans le cadre d’une co-création.\n\n\n\n\nQuand on parle de l’équipe produit, on parle ici de tous ceux qui sont en charge de créer le produit : ça inclut les product managers mais aussi les software engineers, les designers et tout autre rôle qui peut être rattaché à la construction de features pour un produit donné (product analyst, user researcher, customer success etc.).\nLe framework proposé dans ce livre est à destination d’un product trio composé au moins d’un product manager, d’un designer, et d’un software engineer.\nLe product trio peut être un quatuor ou même un quintuor pour être plus représentatif, mais il faut noter qu’il y a ici un trade-off : plus il y a de monde et plus on est inclusif sur les sensibilités permettant d’obtenir une discovery fine, mais en même temps plus on a va aussi prendre du temps pour prendre les décisions. 3 semble un nombre raisonnable la plupart du temps.\n\n\nIl y a 6 mindsets nécessaires pour adopter cette méthode :\n1 - Outcome-oriented : on s’intéresse à l’outcome (impact sur les utilisateurs et le business) plutôt qu’à l’output (nombre de features produites).\n2 - Customer-centric : les besoins de l’utilisateur sont un enjeu central, au même niveau que les enjeux business.\n3 - Collaborative : accepter la nature cross-fonctionnelle des équipes produit. Plutôt que le PM décide, le designer design, et le développeur code, on a chacun qui apporte son expertise au processus de discovery.\n4 - Visual : utiliser des supports visuels pour tirer parti de nos capacités spatiales de raisonnement.\n5 - Experimental : adopter une approche scientifique expérimentale en listant les hypothèses et en récoltant des preuves.\n6 - Continuous : plutôt que de voir la discovery comme quelque chose qu’on fait au début du projet, il faut la voir comme quelque chose qu’on fait tout au long du processus de développement, pour éviter de faire fausse route régulièrement.\n\n\nLes techniques modernes de discovery sont en fait déjà existantes dans de nombreuses entreprises. Ce qui est plus rare c’est l’aspect structuré et continu de ces techniques. Par conséquent la définition de la continuous discovery est la suivante :\nAt a minimum, weekly touchpoints with customers\nBy the team building the product\nWhere they conduct small research activities\nIn pursuit of a desired outcome","2---a-common-framework-for-continuous-discovery#2 - A common framework for continuous discovery":"Il y a en général une tension entre l’outcome business et la satisfaction de l’utilisateur, avec un risque de privilégier le 1er au détriment du 2ème, ce qui ne sera pas viable sur le long terme.\nExemple : en 2016, la banque Wells Fargo connaît un scandale. Les employés avaient fait accepter des services supplémentaires aux clients de manière frauduleuse à cause des objectifs uniquement business, sans couplage à la satisfaction utilisateur.\nPlutôt qu’avoir comme objectif d’augmenter à tout prix les revenus, si l’objectif était par exemple d’obtenir des clients qui veulent ouvrir plus de comptes, on aurait pu éviter le problème.\n\n\nAutre exemple : quand on navigue sur un site et qu’on est bombardé de pub désagréable.\n\n\nSouvent, les product trios prennent un outcome business, et commencent directement à générer des idées. Ils sautent l’étape importante du framing de l’opportunity space.\nOn va appeler dans la suite opportunity space l’ensemble des besoins utilisateur, leurs pain points, et leurs désirs.\nLes désirs sont importants aussi, on ne peut pas les réduire aux besoins. Exemple : manger une glace ou aller à Disneyland sont des désirs.\n\n\nIl y a en fait de nombreuses manières d’atteindre notre outcome, et celles-ci dépendent :\ndu framing de notre space opportunity (la manière dont on construit l’espace des problèmes et des désirs).\ndu choix de l’opportunity pour arriver à l’outcome.\n\n\nIl faut toujours examiner plusieurs opportinuties, et voir les conséquences de chacune avant de choisir.\n\n\nTeresa propose de structurer la discovery au travers d’un Opportunity Solution Tree (OST).\nIl s’agit d’une représentation graphique sous forme d’arbre avec l’outcome en haut, les opportunities possibles en dessous, les solutions possibles associées à chaque opportunity, et les assuption tests qui permettent de valider chaque solution. outcome ← opportunity ← solution ← assumption test\nCe modèle permet :\nDe résoudre la tension entre les besoins du customer et les besoins business.\nOn choisit l’outcome business et on liste ensuite toutes les opportunities qui permettent d’y parvenir, pour être sûr de satisfaire forcément les deux à la fois.\n\n\nD’aider à construire une connaissance partagée pour le product trio.\nNous avons tendance en général à sauter sur la première solution qui nous vient en tête, et à la défendre ensuite contre l’avis des autres. Ces crispations finissent dans une situation où PM décide parce qu’il a le dernier mot, au lieu d’une vraie collaboration.\nEn gardant sous les yeux une représentation graphique de toutes les possibilités, on s’évite d’en considérer une comme étant la “notre” à défendre à tout prix.\n\n\nD’aider le product trio à adopter un continuous mindset.\nVu qu’on a dans notre OST de plus petites opportunities qui permettent de réaliser une plus grosse opportunity, ça nous permet de délivrer de la vraie valeur à chaque sprint, plutôt qu’un bout de quelque chose de gros qui est censé délivrer de la valeur plus tard => On est vraiment agiles.\n\n\nDe prendre de meilleures décisions.\nVu qu’on a sous les yeux les possibilités, y compris celles qu’on a déjà explorées, on évite de tomber dans des biais qui vont fausser notre décision.\nPar exemple : à chaque fois qu’on a une nouvelle demande client, tomber dans le travers de se demander si on arrête tout pour l’implémenter ou non, au lieu de garder la vue d’ensemble.\n\n\nA ce propos, elle conseille un livre à lire juste après le sien : Decisive de Chip et Dan Heath.\n\n\nD’obtenir des cycles d’apprentissage rapides.\nOn dit souvent que les PM doivent définir le problème, et les développeurs apporter la solution. C’est une erreur. Pour être efficace, il faut que les mêmes personnes (product trio) soient en charge des deux.\nl’OST permet de visualiser les deux, et à chaque fois qu’une solution est implémentée et ne permet pas de satisfaire l’opportunity, on peut la garder en tête tout en sachant pourquoi ça n’avait pas marché, et pouvoir basculer sur une solution mieux pensée.\n\n\nDe construire une confiance dans le fait de savoir quoi faire ensuite.\nOn peut facilement se rendre compte qu’on n’a pas assez d’opportunities et faire des interviews client, ou pas assez de solutions et faire des ateliers pour générer des idées.\nLes meilleures équipes ne travaillent pas de haut en bas (définir un outcome clair, puis créer l’opportunity space, puis chercher les solutions, et enfin définir des assumption tests), mais sur tout l’arbre à la fois : chaque semaine ils affinent chaque élément de l’arbre, et travaillent sur plusieurs éléments en parallèle pour le faire évoluer.\n\n\nDe permettre un rapport aux stakeholders plus simple.\nMême avec de la bonne volonté, la direction a tendance à parfois revenir sur des demandes d’output, surtout en période de stress. Pour éviter ça il faut les garder au jus, avec la bonne quantité d'informations.\nIl ne faut ni leur donner trop d’informations, ni leur donner juste les conclusions du product trio. Pour qu’ils se sentent impliqués et puissent donner un feedback, il faut leur donner les principaux éléments qui ont été considérés et choisis ou rejetés. L’OST permet de servir de support visuel pour ça.","part-ii---continuous-discovery-habits#Part II - Continuous discovery habits":"","3---focusing-on-outcomes-over-outputs#3 - Focusing on outcomes over outputs":"L’idée d’utiliser les outcomes plutôt que les outputs existe depuis des décennies, récemment elle a été repopularisée avec les OKR chez Google.\nLes outcomes permettent de donner plus d’autonomie à l’équipe produit, en leur laissant trouver la bonne chose à faire pour régler un besoin business ou une problématique utilisateur, plutôt que leur donner une roadmap de features.\nIl existe 3 types d’outcomes :\nLes business outcomes mesurent l’avancement du business.\nExemple : on a une boutique de vente de nourriture personnalisée pour chien, on veut augmenter la rétention des clients mesurée sur 90 jours.\nÇa peut être soit des aspects financiers (rentrée d’argent, réduction de coûts), soit des initiatives stratégiques (parts de marché ou augmentation de ventes dans une catégorie spécifique, rétention d’utilisateurs etc.).\nCe sont en général des lagging indicators, c’est-à-dire qu’ils vont indiquer le résultat de ce qui s’est déjà produit depuis un moment.\nIls vont impliquer en général une coordination entre plusieurs fonctions business (produit, CSM, vente etc.).\n\n\nLes product outcomes mesurent à quel point le produit fait avancer le business.\nExemple : augmenter la valeur perçue de la nourriture pour chien, et augmenter le nombre de chiens qui aiment cette nourriture sont deux indicateurs qui permettront d’atteindre une meilleure rétention des clients à 90 jours.\nCe sont en général des leading indicators, qu’on peut utiliser pour itérer rapidement en fonction des résultats qu’on obtient semaine après semaine.\nIls sont sous la responsabilité de l’équipe produit, et représentent des objectifs sur lesquels l’équipe produit peut agir seule. C’est avec ces outcomes qu’elle avancera le mieux.\n\n\nLes traction metrics mesurent l’utilisation d’une feature particulière.\nExemple : augmenter le nombre de propriétaires de chiens qui utilisent la fonctionnalité calendrier.\nOn est bien ici sur de l’outcome, mais qui est limité à un output spécifique. Il y a donc un risque que l’équipe produit soit coincée si l’output choisi dont on mesure l’utilisation n’était pas le bon.\nEn général l’équipe produit avancera mieux avec un product outcome, mais il existe 2 cas où les traction metrics peuvent être utiles :\nQuand on a un product trio plutôt junior, limiter leur responsabilité en fixant un output particulier à optimiser peut être pertinent.\nSi on a un produit déjà mature (dont on a déjà fait pas mal de discovery pour défricher), et qu’on est sûr qu’une feature particulière est très utilisée, on peut vouloir l’optimiser et donc faire de la discovery avec une traction metric.\n\n\n\n\n\n\nLe choix des outcomes de l’équipe produit est le résultat d’une négociation entre le product trio et le product leader.\nLe product leader (par exemple CPO) amène la connaissance de ce qui est utile à ce moment là pour l’organisation, alors que le product trio amène la connaissance des clients et de la technique.\nLa négociation porte sur des outcomes et pas des outputs ou des solutions, et ne doit pas permettre au product leader de les réduire à des traction metrics. Il peut par contre demander à restreindre le champ, par exemple “augmenter le nombre de chiens qui aiment la nourriture dans telle ou telle région”.\nLes deux vont convenir d’une métrique à augmenter d’une certaine valeur, et en fonction de celle-ci, éventuellement aussi des ressources (software engineers) pour mener à bien cet outcome.\nLe fait que la participation des équipes à la définition de leurs outcomes apporte de meilleures performances est appuyé par des études.\n\n\nOn peut avoir des performance goals qui sont des objectifs mesurables et challenging (exemple : augmenter l’engagement de 10%), et des learning goals qui sont des objectifs non mesurés (exemple : découvrir les opportunités qui drivent l’engagement).\nLes études ont montré que les performance goals étaient plus efficaces quand la stratégie était déjà connue et qu’on avait identifié une métrique pertinente grâce à une précédente discovery, mais que les learning goals étaient plus efficaces dans le cas contraire.\n\n\nQuelques anti-patterns à éviter :\nAvoir trop d’outcomes en même temps : on s’éparpille en avançant un peu sur chacun des outcomes, mais on n’a pas de gros impact sur l’un d’entre eux. En général, le mieux est de chercher à satisfaire un outcome à la fois.\nFaire du ping pong entre plusieurs outcomes : on a tendance à basculer d’outcome en outcome à cause d’une culture du firefighting où un besoin client en chasse un autre. Rester sur le même outcome pendant plusieurs trimestres permet de profiter de ce qu’on a appris dans le 1er et d’être vraiment efficace sur les 2ème et 3ème trimestres.\nDéfinir des outcomes individuels plutôt que des outcomes pour le product trio : on a parfois des objectifs individuels pour obtenir de la rémunération, portant sur des outcomes business pour le PM, des outcomes UX pour le designer, et des outcomes techniques pour le tech. Ces objectifs empêchent le bon fonctionnement du product trio. Il vaut mieux des** outcomes d’équipe**.\nChoisir un output au lieu d’un outcome :\nVu qu’on est habitués à utiliser des outputs, on va avoir tendance à revenir à ça. Le bon critère à regarder pour voir si on est plutôt vers l’outcome ou l’output c’est** l’impact**.\nUn bon début c’est de s’assurer que notre outcome représente un nombre.\n\n\nSe focus sur un seul outcome au détriment de tous les autres : il faut en choisir un à la fois à faire avancer, mais il ne faut pas oublier de monitorer les autres outcomes pour s’assurer qu’ils ne se dégradent pas.\nPar exemple l’acquisition de clients, en vérifiant en même temps que leur satisfaction ne baisse pas.","4---visualizing-what-you-know#4 - Visualizing what you know":"Quand le product trio commence avec un outcome, il va d’abord créer un experience map pour représenter le workflow lié à cet outcome, et rassembler les connaissances actuelles du trio.\nIl faut **définir le scope **plus ou moins large pour notre experience map : un scope très large permettra d’explorer des marchés adjacents, et un scope plus étroit permettra de se cantonner à notre produit. En général on cherche quelque chose d'intermédiaire.\nExemple : On a une application de streaming, et notre outcome c’est d’augmenter le nombre de minutes regardées. Un scope large serait “Comment est-ce que les clients se divertissent en général ?”, et un scope étroit serait “Comment est-ce que les clients se divertissent avec notre service ?”. Quelque chose d’intermédiaire peut être “Comment est-ce que les clients se divertissent avec de la vidéo ?”.\n\n\nL’experience map doit représenter les étapes du point de vue du customer.\nPar exemple, si notre scope c’est “Comment est-ce que les clients se divertissent avec de la vidéo ?”, on peut commencer par imaginer quelqu’un qui nous partage le nom d’un contenu vidéo, puis on fait une action de recherche. On peut éventuellement être face à des problèmes pour trouver le contenu, et ainsi de suite.\n\n\nIl contient uniquement du contenu dessiné. Les mots étant moins précis, moins spécifiques que les images, on va dessiner des boîtes, des flèches, de petits pictogrammes etc. pour constituer nos étapes. Ne pas savoir dessiner n’est pas un problème.\nBien que ce soit contre-intuitif, il est plus efficace de construire l’experience map d’abord chacun de son côté pour ensuite mettre en commun. Si on travaille ensemble dès le début, la dynamique de groupe va faire que certains ne vont pas s’exprimer pleinement.\nPour la mise en commun :\nOn va d’abord chacun expliquer notre map aux autres. On pose des questions pour clarifier, mais le but n’est pas de “défendre” sa vision.\nEnsuite on construit un schéma commun qui va être la combinaison des maps initiaux. Il n’est pas question d’en choisir un des trois et d’avancer avec ça.\nOn récupère tous les noeuds de chacun des maps. Ce sont les événements qui représentent des étapes.\nOn crée un nouveau map dans lequel on met tous ces noeuds.\nOn fusionne les noeuds similaires.\nOn relie les noeuds par des flèches. Pas seulement le happy path mais aussi les chemins d’échec, qui vont créer des frustrations chez le client.\nOn ajoute du contexte sur ce que pensent, ressentent les clients, toujours sous forme visuelle.\n\n\n\n\nQuelques anti-patterns à éviter :\nS’embourber dans de longs débats sur des détails : quand c’est le cas, il vaut mieux dessiner ce que chacun veut dire pour faire ressortir clairement le désaccord (ou bien souvent l’accord parce qu’on était en quiproquo), et passer à la suite.\nUtiliser des mots à la place des dessins : les dessins font appel à une autre partie de notre cerveau et sont ici plus efficaces. Avec le temps, on se débrouille pour dessiner de moins en moins mal.\nAller à la suite en considérant que notre map représente la vérité : c’est juste un brouillon, on va le retoucher un grand nombre de fois à mesure qu’on parle aux clients.\nOublier d’affiner notre map commune à mesure que nos connaissances augmentent : même en assistant aux mêmes réunions avec les clients, si on ne refait pas le point sur la map régulièrement, chaque membre du trio retiendra des choses différentes, et aura en tête une experience map différente.","5---continuous-interviewing#5 - Continuous interviewing":"Bien souvent, les clients ne savent pas à l’avance les solutions qu’ils veulent.\nEn parlant aux clients, notre but est d’obtenir une meilleure compréhension de l’opportunity space, pas de leur demander quelle feature ils aimeraient (l’espace des solutions).\n\n\nIl est important que le client nous parle de quelque chose qui l’intéresse. Donc la première chose à obtenir de lui c’est quels sont les besoins, pain points, et désirs qui comptent le plus pour lui. On pourra ensuite choisir des sujets à aborder parmi ceux-là.\nOn est parfois déçu de ne pas pouvoir faire parler le client de ce qu’on veut, ou même parfois d’avoir des clients non coopératifs. Ce n’est pas très grave : on en aura au moins un autre la semaine prochaine.\n\n\nEn posant des questions directes aux gens, on peut bien souvent obtenir une réponse fausse, parce que le cerveau a tendance à rationaliser pour reconstruire des informations manquantes, ou transformer les choses pour les mettre en cohérence avec la vision globale de la personne, la perception qu’elle a de son identité etc.\nCeci est démontré par des décennies de recherches sur la manière d’interroger les personnes.\nExemple : Teresa pose à une personne la question de savoir quel critère elle utilise pour s’acheter des jeans. Elle répond “le fit” comme critère numéro 1. Puis Teresa lui demande de lui raconter la dernière fois qu’elle a acheté un jean : c’était sur Amazon. Donc les vrai critères c’est le prix, la marque, la disponibilité rapide. Le fit n’était pas le critère numéro 1 pour l’achat.\n\n\nEn fait il va falloir distinguer les research questions qui sont les questions directes qu’on poserait pour obtenir des informations sur l’opportunity space, et les interview questions qui sont celles qu’on va réellement poser.\nLes interview questions vont consister à demander au client de nous raconter des cas concrets de choses qu’il a pu faire sur le sujet qui nous intéresse.\nExemple : Au lieu de demander “Quel critère utilise-tu pour acheter des jeans”, on demande “Raconte-moi la dernière fois que tu as acheté des jeans”.\n\n\nOn peut aussi faire varier le scope en fonction de ce qu’on recherche, de la même manière qu’avec l’experience map.\nExemple : pour le service de streaming.\nRacontez-moi la dernière fois que vous avez utilisé notre service de streaming. => en apprendre plus sur notre produit pour l’améliorer.\nRacontez-moi la dernière fois que vous vous êtes diverti en regardant du streaming. => comprendre comment on se place par rapport aux concurrents.\nRacontez-moi la dernière fois que vous-vous êtes diverti. => découvrir avec quoi la catégorie de notre produit est en compétition (exemple théâtre etc.).\n\n\n\n\n\n\nIl faut aider le client à raconter son histoire :\nLes gens sont habitués à parler autant que leur interlocuteur, si on veut les faire parler plus que nous, il faut leur dire dès le début qu’on veut qu’ils racontent leur histoire avec le plus de détail possible.\nIl faut l’aider à avancer dans les étapes :\nNoter les personnages clé, les challenges rencontrés etc. pour pouvoir le relancer.\nUtiliser notre experience map pour suivre son histoire à travers nos propres nœuds, et éventuellement le relancer sur des étapes dont il n’aurait pas parlé.\n\n\nSi le client généralise une réponse (“Quel challenge avez-vous eu ?” “En général les challenges sont…”) alors il faut lui demander de revenir à l’exemple concret. On a vu que la généralisation menait à une vision faussée de la réalité.\n\n\nPour pouvoir se rappeler des interviews sur la durée, il faut résumer le résultat de chaque interview dans un interview snapshot, qui est une page résumant l’interview.\nSi possible il faut y mettre une photo de la personne qu’on a interviewé, ou une image qui va nous faire penser à elle.\nIl faut aussi y mettre une phrase qui nous a marqué, pour pouvoir nous en rappeler.\nNDLR : un peu comme le channel #thingsclientssaid.\n\n\nLa partie quick facts permet d’indiquer de quel type de client il s’agit, en fonction de la catégorisation qu’on souhaite faire de nos clients. Ca permettra ensuite de comparer ce qui a été remonté avec les clients de la même catégorie.\nEnsuite on a de la place pour noter les opportunities.\nSi le client donne des solutions (features) qu’il aimerait, il faut essayer d’en extraire l’opportunity. Par exemple en posant la question : Qu'est-ce que cette feature vous apporterait ?\nIl faut s’efforcer de formuler l’opportunity avec les mots du client, pour que ça représente son point de vue, et non le point de vue de notre entreprise.\n\n\nOn a aussi une place pour noter des insights qui ne sont pas des opportunities (ni des besoins, ni des pain points, ni des désirs).\nEt enfin on a une place pour dessiner la story racontée par le client, avec des boîtes et des traits, sur le même modèle que notre experience map.\n\n\nIl est très important de** faire des interviews de clients chaque semaine**. Commencer ou arrêter une habitude est beaucoup plus difficile que d’en maintenir une.\nLe plus difficile pour ça c’est d’automatiser le process de recrutement des personnes qu’on va interviewer. On a parfois des urgences business, des absents dans l’équipe etc. et dans ces moments on a besoin d’avoir son interview de la semaine déjà bookée sans rien faire.\nParmi les manières de recruter :\nLe plus simple est d’avoir un petit bandeau qui s’affiche dans notre application, et qui dit “Est-ce que vous auriez 20mn à nous accorder pour améliorer notre service, en échange de 20€ ?”.\nÇa marche bien si on a beaucoup de trafic, dans ce cas on peut demander le numéro de la personne avec un formulaire.\nSi on a moins de trafic, à la place du numéro on peut proposer un service de scheduling en ligne pour que le client puisse directement réserver un créneau avec nous.\nSi on n’a pas de trafic du tout, on peut toujours utiliser de la publicité qui redirige vers une landing page, où il y aura le formulaire proposant de nous aider.\n\n\nUne autre solution est de demander aux** équipes customer facing** (CSM, sales etc.).\nD’abord on leur demande de rejoindre un de leurs calls 5 mn à la fin, pour prendre une story. Le but est de mettre un pied dans la porte pour qu’ils disent oui facilement.\nUne fois qu’on est plus à l’aise, on peut demander aux équipes CSM de programmer des interviews pour nous. On leur donne des “triggers” qu’on met à jour régulièrement.\nPar exemple : si un client pose une question sur la feature X, programme lui un interview avec nous.\n\n\n\n\nEnfin, si on a une audience trop petite ou trop difficile à aller chercher, on peut aussi constituer un customer advisory board, c’est-à-dire convaincre un groupe de clients de participer à des meetings réguliers avec nous.\nL’avantage c’est que ça permet de suivre l’évolution de leurs problématiques dans le temps. Le désavantage potentiel c’est qu’on risque d’avoir un échantillon pas forcément représentatif.\n\n\n\n\n\n\nIl faut que les interviews soient faites par l’ensemble du product trio.\nSi l’un des membres se retrouve à être “la voix du customer”, il acquerra trop de pouvoir dans le groupe, et déséquilibrera le processus de décision dans la discovery, avec un “Oui mais c’est ça que le client veut”.\nLe product trio est composé de personnes diverses justement pour prendre les meilleures décisions possibles, chacun amenant sa sensibilité.\n\n\nQuelques anti-patterns à éviter :\nCompter sur une seule personne pour le recrutement ou le fait de faire les interviews : si elle n’est pas là, l’habitude d’interviewer s’arrête. Il faut plutôt que chaque membre du trio sache recruter et interviewer.\nPoser des questions “Who, What, Why, How, When” : ça fait de longues interviews et donne des données non fiables. Il faut plutôt préparer des research questions, et des interviews questions correspondantes qu’il faudra poser.\nFaire des interviews seulement quand on en a besoin : l’habitude en elle-même a beaucoup de valeur. Et en plus on aura des réponses rapides.\nPartager ce qu’on apprend avec le reste de l’équipe en envoyant des pages de notes ou des enregistrements des interviews : il vaut mieux utiliser les interview snapshots. On ne peut pas demander au reste de l’équipe de consacrer autant de temps que nous aux interviews.\nS’arrêter pour synthétiser les 6 à 12 dernières interviews : si on est dans un mode continu, on synthétise aussi en continu avec les interview snapshots, pas par batch.","6---mapping-the-opportunity-space#6 - Mapping the opportunity space":"Avoir un backlog d’opportunities est un bon début, mais la priorisation est compliquée parce que le sizing de chaque opportunity est différent, et qu’elles s’overlap parfois. => L’OST répond à ces problématiques.\nIl faut essayer de trouver plus de sub-opportunities à mesure qu’on fait des interviews, pour deux raisons :\nÇa permet de répondre à des opportunities qui paraissent trop grosses.\nÇa permet de délivrer de la valeur en continu, en suivant la philosophie agile.\n\n\nDans l’OST, à propos de la relation entre les noeuds parents / enfants :\nLe noeud enfant permet d’avancer sur la problématique du parent.\nDeux nœuds enfants du même niveau doivent pouvoir être résolus chacun de leur côté sans impacter l’autre.\nExemple : Dans le cadre du streaming, “Je ne sais pas comment chercher un show particulier” permet de faire avancer la problématique parente “Je ne trouve rien à regarder”, sans faire avancer l’autre enfant du même niveau “J’ai terminé les épisodes de mon show favori”.\n\n\nIl y a deux techniques pour que les opportunities du même niveau soient indépendants, à chaque fois il faut les associer avec des moments distincts dans le temps :\nOn peut reprendre notre experience map, et associer chaque nœud à un nœud d’opportunity top level dans l’OST.\nSi notre experience map n’est pas encore bien formée, on peut parcourir nos interview snapshots, et essayer de trouver des nœuds qui reviennent dans le dessin de la story. Une fois qu’on a ces nœuds on peut les associer avec les noeuds top level de notre OST.\n\n\nUne fois qu’on a nos nœuds top level de l’OST, on peut y ajouter nos opportunities à partir de nos interview snapshots.\nAvant d’ajouter chaque opportunity, on revérifie bien qu’elle permet d’avancer sur notre outcome, et que c’est vraiment une opportunity.\nOn les place d’abord simplement en dessous de l’opportunity top level correspondante.\n\n\nOn va ensuite, branche top level par branche top level, ajouter de la structure à notre OST :\nOn va trouver des opportunities qui se ressemblent, et on va leur chercher une opportunity parente.\nSi l’opportunity parente n’existe pas, c’est OK de la créer, vu que les enfants eux sont bien issus de vrais interviews.\nSi deux opportunities sont vraiment les mêmes formulées différemment, on peut les regrouper en une.\nOn va ensuite chercher à regrouper nos mini-arbres ensemble avec des parents communs, jusqu’à arriver à l’opportunity top level.\n\n\nLe framing de l’opportunity space est une étape importante.\nSi la construction de l’OST nous prend 30mn, c’est qu’on l’a fait un peu vite fait. Mais il ne faut pas non plus y passer des heures et des heures.\nC’est un processus itératif, on va de toute façon remodeler l’OST de nombreuses fois.\n\n\nQuelques anti-patterns à éviter :\nCréer les opportunities depuis la perspective de l’entreprise : on peut se demander à chaque nœud de l’OST : est-ce que j’ai entendu un client dire ça ? Ou si on a dû le créer : est-ce qu'un client dirait ça ?\nExemple : un client ne dira jamais “J’aurais bien aimé avoir plus d’abonnements au service de streaming”, mais il pourra dire “J’aurais aimé avoir plus de contenu intéressant”.\n\n\nAvoir des sous-arbres verticaux : si on est dans ce cas, c’est\nsoit qu’on a deux opportunities qui sont en fait à peu près les mêmes, et qu’on peut reformuler en une.\nsoit il nous manque des opportunities de même niveau parce que l’opportunity enfant n’est pas suffisante pour résoudre le parent. Dans ce cas, il faudra trouver d’autres opportunities pour ne pas rester sur un sous-arbre vertical.\n\n\nDes opportunities ont plusieurs parents : si on a bien nos opportunities top level qui sont vraiment des moments différents dans le temps ça ne doit pas arriver. Il est possible que l’opportunity problématique soit formulée de manière trop générale.\n**Des opportunities non spécifiques **: des opportunities comme “J’aimerais que ce soit facile à utiliser” ne sont pas assez spécifiques, on peut les rendre plus spécifiques, par exemple “J’aimerais que voir un show soit plus facile”.\nDes opportunities qui sont des solutions déguisées : Il faut qu’il y ait plusieurs solutions possibles à une opportunity, sinon c’est déjà une solution.\nPar exemple : “Je veux passer rapidement les pubs” est en fait une solution. L’opportunity pourrait être “Je n’aime pas voir les pubs”, et auquel on pourra répondre par les passer, mais aussi par “Faire des pubs plus attrayantes” ou encore “Avoir un abonnement sans pub”.\n\n\nCapturer des sentiments comme des opportunities : si on a un sentiment qui est donné par le client, c’est qu’il y a une opportunity pas loin. Mais il faut alors** chercher la cause** de ce sentiment pour avoir l’opportunity.\nExemple : “Je me sens frustré” => Pourquoi => “Je déteste retaper mon mot de passe à chaque achat”.","7---prioritizing-opportunities-not-solutions#7 - Prioritizing opportunities, not solutions":"Il faut chercher à répondre à une seule opportunity à la fois.\nC’est en cohérence avec la philosophie Kanban qui consiste à limiter le nombre de tâches en cours.\nDes études ont montré que limiter le nombre de tâches en cours permet d’avoir une meilleure qualité, de délivrer de manière plus consistante, et d’avoir moins de plaintes côté client.\n\n\nGrâce à l’OST on va pouvoir choisir notre prochaine opportunity sans avoir à tout prioriser et re-prioriser.\nOn part du haut, et on compare les opportunities top level, pour en choisir une seule.\nOn descend dans le sous-arbre de celle qu’on a choisi, et on fait pareil : on choisit l’enfant de même niveau le plus prioritaire.\nOn fait ça jusqu’à atteindre le bas de l’arbre : on a alors notre opportunity.\n\n\nLes critères pour prioriser sont :\nLe sizing de l’opportunity : l’impact que l’opportunity a sur nos clients.\nPas besoin que ce soit précis : on a juste besoin de pouvoir comparer les nœuds enfants de même niveau entre eux.\nOn peut utiliser comme données : des analytics, le funnel des sales, les tickets de support, des sondages, nos interview snapshots etc.\nIl faut aussi distinguer combien de clients sont touchés, et à quelle fréquence ils sont touchés. Parfois c’est peu de clients très touchés, ou beaucoup de clients peu touchés.\n\n\nLes facteurs liés au marché :\nIl s’agit de maîtriser notre positionnement par rapport aux entreprises concurrentes, et éventuellement des segments de marché extérieurs qui pourraient venir grignoter notre marché actuel (par exemple la vidéo par câble qui grignote notre service de streaming).\nEn fonction de notre positionnement actuel, on pourra prioriser soit des enjeux importants pour les sales, soit des enjeux stratégiques pour nous maintenir sur notre marché et en conquérir d’autres.\n\n\nLes facteurs liés à l’entreprise : On doit prendre en compte la stratégie, la vision et les valeurs de l’entreprise, pour aller dans le même sens.\nLes facteurs liés au client : Il s’agit de l’impact sur la satisfaction client : on va choisir l’opportunity qui en procurera le plus.\n\n\nIl ne faut pas attribuer de note précise à chaque opportunité et pour chaque critère. Les critères permettent d’avoir un débat au sein du product trio, et le choix doit être grossier et subjectif.\nIl est important que les décisions de discovery en général soient considérées par le product trio comme potentiellement **modifiables au bout de quelques jours **(la discovery est continuous).\nAvec cet état d’esprit on n’a pas besoin de passer trop de temps à choisir l’opportunity. Si elle se révèle mauvaise, on la changera très vite.\n\n\nQuelques anti-patterns à éviter :\nRemettre la décision à plus tard, quand il y aura plus de données : on en apprendra plus en examinant les conséquences d’une opportunity qu’on choisit maintenant, qu’en attendant d’avoir plus de données pour un meilleur choix.\nLe mieux est de limiter le temps de décision : on se donne une heure ou deux (ou au pire un jour ou deux) et à la fin du délai on décide de l’opportunity.\n\n\nUtiliser principalement un des 4 critères pour le choix de l’opportunity : les 4 critères permettent d’avoir un point de vue différent sur l’importance de chaque opportunity. Il faut les utiliser tous.\nAvoir une conclusion pré-établie, et tenter de la justifier : faire ça est une perte de temps. Il faut faire l’exercice avec une ouverture d’esprit pour s’ouvrir à d’autres perspectives.","8---supercharged-ideation#8 - Supercharged ideation":"Les études montrent que quand on recherche la créativité, **les meilleures idées sont parmi les dernières qu’on trouve. **Plus on en cherche, et plus on tombe sur des choses originales et pertinentes.\nIl faut donc résister à notre propension à choisir la première idée ou solution qui nous passe par la tête.\n\n\nToutes les opportunities ne nécessitent pas d’y passer du temps pour trouver des solutions originales. On en a besoin surtout pour celles qui sont stratégiques, où on veut se distinguer de la concurrence.\nLes études montrent que faire un brainstorming est moins efficace que de chercher des idées chacun de son côté : on trouve plus d’idées différentes, des idées plus originales et plus variées.\nLes raisons sont entre autres le fait d’être poussé à plus chercher quand on est seul, alors qu’on peut se reposer sur les autres quand on est en groupe. Il y a aussi le fait que les premières idées du groupe formatent les suivantes, avec une gêne à proposer des idées à priori pas assez bonnes.\nCeci dit on est moins souvent coincés en groupe, donc on a l’impression d’être plus performants.\nUne solution encore plus efficace est d’alterner les sessions seul et en groupe : on trouve efficacement des idées chacun de son côté, puis on est débloqués grâce à l’apport du résultat des autres, et on peut à nouveau avancer chacun de son côté à partir de là.\n\n\nOn est tous performants pour générer des idées. Ça peut prendre un peu de temps au début si on est rouillé mais ça revient toujours.\nPlutôt que de prendre une grosse heure pour trouver des idées, il vaut mieux répartir ça dans la journée et dans différents endroits (entre deux meetings, en marchant après manger etc.).\nOn peut aussi profiter du fait que notre cerveau travaille alors qu’on arrête d’y penser, et le lendemain on tombe sur une bonne idée qui était en gestation.\nOn peut s’inspirer de la concurrence, et aussi d’entreprises dans des domaines qui n’ont rien à voir (bien des problématiques sont communes à différents domaines).\nOn peut aussi considérer le point de vue d‘utilisateurs particuliers : les utilisateurs qui arrivent pour la première fois, les utilisateurs handicapés, jeunes, vieux, vivant loin etc.\n\n\nConcrètement pour générer des solutions à nos opportunities :\n1 - On revoit notre opportunity cible et son contexte, on vérifie qu’elle est bien sizée etc.\n2 - On génère des idées seul, avec les techniques ci-dessus.\n3 - On partage nos idées avec l’équipe. Ça peut être en live ou en asynchrone (slack ou autre).\n4 - On répète les étapes 2 et 3, jusqu’à obtenir 15 à 20 idées.\n5 - On élimine les idées qui ne répondent pas à l’opportunity qu’on vise.\n6 - On va dot-voter avec l’équipe : chacun a 3 points à mettre sur les idées de son choix (avec possibilité de mettre sur les mêmes), et on élimine celles qui en ont le moins, jusqu’à arriver à seulement 3 idées.\nLes études montrent que pour ce qui est du choix des idées, on est plus efficaces en groupe.\nOn peut être amenés à faire plusieurs tours de dot-vote pour éliminer les idées.\nPas besoin de consensus sur chaque idée, par contre il faut que chacune des idées choisies aient au moins une personne qui soit enthousiasmée par celle-ci. Si ce n’est pas le cas, il faut revoter.\n\n\n\n\nQuelques anti-patterns à éviter :\nNe pas inclure une diversité de perspectives : si la plupart des exercices du livre sont faits pour le product trio, il est préférable de faire la génération d’idées avec l’équipe produit entière. On peut même inviter d'autres stakeholders importants. Plus on aura du monde, plus on aura des idées diverses.\nGénérer trop de variations de la même idée : quand on est bloqués, on a tendance à reprendre les mêmes idées un peu différentes pour avoir l’impression d’en avoir plein. Il faut se forcer à en trouver d’autres, y compris en allant chercher ce que font d’autres produits.\nLimiter la recherche d’idées à une seule session : les études montrent que le fait sur un temps plus long, par petites sessions, est plus efficace.\nChoisir des idées qui ne permettent pas de répondre à l’opportunity choisie : avant de dot-voter, il faut bien éliminer les idées sans rapport, même si elles ont l’air intéressantes en elles-mêmes. Sinon on se disperse.","9---identifying-hidden-assumptions#9 - Identifying hidden assumptions":"On a en général** tendance à s’accrocher à nos idées**.\nParmi les biais ici il y a notamment :\nLe biais de confirmation qui fait qu’on va accorder de l’importance à ce qui confirme notre idée, et peu d’importance à ce qui la réfute.\nLe biais des coûts irrécupérables qui fait que plus on investit dans une idée, plus on pense devoir continuer.\n\n\nPour éviter ce phénomène, on a choisi de traiter 3 idées en même temps, et on va itérer rapidement pour en traiter le maximum sans rester longuement sur une idée particulière.\n\n\nPour pouvoir tester 3 idées en même temps, on ne peut pas les implémenter à chaque fois toutes. Il faut à la place tester les assumptions sous-jacentes.\nLa plus grande difficulté c’est de trouver ces assumptions.\n\n\nIl y a 5 types d’assumptions qui nous intéressent :\nDesirability assumptions : le fait de savoir si nos utilisateurs ont envie de faire ce qu’on imagine qu’ils ont envie de faire à travers notre idée.\nViability assumptions : le fait de savoir si l’idée va vraiment apporter au business suffisamment de valeur (et pas seulement aux clients).\nFeasibility assumptions : le fait de savoir si c’est faisable d’un point de vue technique, mais aussi de potentiels problèmes légaux, culturels etc.\nUsability assumptions : le fait de savoir si les clients vont pouvoir l’utiliser, le comprendre etc.\nEthical assumptions : le fait de savoir si on pourrait causer des problèmes éthiques ou dangereux.\nCa concerne ce qu’on va faire des données qu’on collecte :\nEst-ce qu’on va partager ces données avec des tiers ?\nEst-ce que nos clients comprennent ce qu’on fait de leurs données.\nS’ils le comprenaient est-ce qu’ils seraient d’accord ?\n\n\nÇa peut aussi être d’autres types de problèmes :\nEst-ce que notre produit peut devenir addictif et nuire à l’utilisateur ?\nEst-ce que certains utilisateurs seront exclus de notre feature ?\nEst-ce qu’on contribue aux inégalités sociales ?\nEst-ce qu’on expose l’identité de personnes à qui ça pourrait causer du tort ?\nComment les trolls d’internet pourraient-ils détourner ça ?\n\n\nCa pourrait aussi être des dommages pour notre business :\nEst-ce que la solution va aider ou nuire à notre marque ?\nEst-ce qu’on comblera les attentes des clients ou est-ce qu’on les laissera déçus ?\n\n\nUne question qui marche bien pour trouver les problèmes : “Si le New York Times (ou un autre grand média) publiait un article en première page, détaillant notre solution, l’ensemble de nos échanges internes, les conséquences sur l’écosystème etc. Est-ce que ce serait une bonne chose ou non ?”\n\n\n\n\nChaque membre du trio peut mettre des assumptions différentes derrière une idée. Pour s’aligner sur les assumptions, on va utiliser le story mapping : cartographier les étapes faites par l’utilisateur pour obtenir de la valeur de notre produit.\n1 - On va partir du principe que la solution existe déjà, et voir l’apport de valeur sur l’utilisateur. Il ne s’agit pas de mesurer la difficulté d’implémentation.\n2 - On va ensuite identifier les acteurs clés : ça peut être plusieurs users dans un réseau social, des acheteurs/vendeurs, un user et un chatbot etc.\n3 - On liste les étapes nécessaires pour chaque acteur, pour que la fonctionnalité qui nous intéresse apporte de la valeur.\n4 - On dispose les étapes en** séquence horizontale **sur un graphique.\nIl faut respecter l’ordre causal des étapes entre acteurs.\nSi on a des étapes optionnelles on les représente aussi.\nOn représente les successful paths, et s’il y en a plusieurs, on les représente tous.\n\n\n5 - On explicite les suppositions résultant de nos étapes. On va passer sur chacune des étapes et se poser la question de chaque type d’assumption (desirability, feasibility etc.). On va en avoir facilement des dizaines.\nExemple : nous explorons 3 solutions pour notre service de streaming. Parmi elles, on commence par “Intégrer des chaînes locales dans notre service (ABC, NBC etc.)”.\nOn part du principe que le service existe déjà, et on liste les acteurs : le client, la plateforme et la chaîne locale partenaire.\nOn liste les étapes :\nLe client arrive pour regarder du sport en direct.\nNotre plateforme montre les choix de contenu possible.\nLe client choisit.\nLa chaîne locale envoie le contenu.\nLe client le regarde.\n\n\nOn va ensuite expliciter les assumptions qui en résultent.\nPar exemple pour l’étape 1 “Le client arrive pour regarder du sport en direct”, on a les assumptions suivantes :\nDesirability : notre client veut regarder du sport.\nDesirability : Notre client veut regarder du sport sur notre plateforme.\nUsability : Notre client sait qu’il peut regarder du sport sur notre plateforme.\nUsability : Notre client pense à notre plateforme quand il est temps de regarder du sport.\nFeasibility : Notre plateforme est disponible quand le client veut regarder du sport.\n\n\nOn peut aussi tirer des assumptions de viability ou ethical à partir de la story map. Par exemple :\nViability : Intégrer un flux de chaîne partenaire locale ne coûtera pas trop cher.\nEthical : Les clients seront d’accord pour que nous partagions les données de vues avec les chaînes locales.\n\n\n\n\n\n\n\n\nIl ne faut pas trop s’inquiéter d’avoir des dizaines d’assumptions. Si on a bien fait notre travail, la plupart seront justes, et l’important c’est surtout de trouver celles qui sont risquées.\nUne autre technique pour identifier les assumptions risqués c’est de faire une session** pre-mortem**.\nIl s’agit de se placer dans quelques mois, d’imaginer que le produit a été un échec cuisant, et se demander pourquoi.\nPour que ça marche il est important d’imaginer que le produit a été un échec, pas qu’il pourrait l’être.\n\n\nUne autre manière de les trouver encore c’est d’utiliser l’opportunity solution tree, en remontant depuis les solutions vers les opportunities, puis vers l’outcome.\nIl s’agit d’utiliser une phrase du genre “La solution permet d’adresser l’opportunity parce que…”, ou “L’opportunity permet de driver l’outcome parce que…”.\nPar exemple :\nAjouter des chaînes locales permettra à nos clients de regarder du sport en direct parce que…\nLes sports que nos clients veulent sont sur les chaînes locales.\nLa plupart des sports populaires sont sur les chaînes locales.\nNos clients ont des chances de vouloir regarder des sports populaires.\n\n\nAdresser l’opportunity “Regarder des sports en direct” permettra de faire avancer l’outcome produit “Augmenter les minutes regardées par semaine” parce que…\nLes gens regarderont des sports en plus de ce qu’ils regardent déjà.\nMême s’ils regardent moins les autres choses, les show sportifs sont plus longs et le temps augmentera.\nSi chaque session de visionnage est plus longue, le temps global regardé sera plus long.\n\n\nAdresser l’outcome produit “Augmenter les minutes regardées par semaine” permet d’adresser l’outcome business “Augmenter le nombre de renouvellement d’abonnement” parce que…\nLes gens qui regardent longtemps sont plus enclins à renouveler leur abonnement.\n\n\n\n\n\n\nLa combinaison de ces méthodes permet de trouver les assumptions de chaque catégorie. Mais à force, si on devient suffisamment fort, on ne sera plus obligé de les utiliser toutes.\nSouvent, les équipes ont des points faibles, qu’ils peuvent combler par une ou deux techniques. Par exemple remonter l’OST pour les viability assumptions, utiliser les questions à se poser pour les ethical assumptions etc.\n\n\nPour prioriser nos assumptions à tester, on va utiliser l’assumption mapping.\nIl s’agit de trouver les assumptions les plus risquées, celles qui impliquent un “acte de foi” (leap of faith).\nOn va placer les assumptions sur un graphique avec en abscisse le niveau de preuves qu’on a sur le fait que notre assumption est vraie, et en ordonnée l’importance de l’assumption pour la réussite de notre solution.\nToutes les assumptions sont certes importantes, mais certaines sont plus problématiques à contourner si jamais elles se révèlent fausses.\nIl n’y a pas besoin d’être précis, tout ce qui compte c’est de placer les assumptions dans le graphique relativement aux autres assumptions.\n\n\nIl s’agira ensuite de récolter les assumptions “leap of faith” en haut à droite de notre graphique, de le faire pour les 3 idées et de tester chacune de celles-ci.\n\n\nQuelques anti-patterns à éviter :\nNe pas générer assez d’assumptions : Teresa en génère en général 20/30 par idée. On n’aura pas à toutes les tester, mais si n’en génère pas beaucoup on ne trouvera pas non plus les plus risquées.\nFormuler les assumptions de manière négative : on est parfois tenté de formuler une assumption comme “Les clients ne retiendront pas leur mot de passe”, mais cette formulation rendra le test plus difficile. Il vaut mieux formuler ce qui est nécessaire pour que la solution fonctionne (que les clients retiennent leur mot de passe par exemple).\n**Ne pas être assez spécifique **: une assumption comme “Les clients auront le temps” n’est pas assez précise, il vaut mieux quelque chose comme “Les clients prendront le temps de parcourir toutes les options de la page de démarrage”.\nFavoriser certaines catégories au détriment d’autres : si on a par exemple des difficultés de faisabilité, on aura tendance à oublier de tester que la solution est au moins désirable. On oublie aussi souvent les problématiques éthiques etc. Il faut utiliser les catégories pour trouver les angles morts.","10---testing-assumptions-not-ideas#10 - Testing assumptions, not ideas":"Une fois qu’on a notre “leap of faith” d’assumptions pour les 3 idées, il vaut mieux éviter de se précipiter pour les tester.\nParfois les tests ne sont pas pensés pour les assumptions visées, mais pour tester l’idée en entier.\nParfois on teste sur la mauvaise audience, ou on s’éparpille vers des données intéressantes mais sans rapport avec le problème.\n\n\nIl faut tester les trois idées en même temps. Si on les traite une par une, on risque de céder à nos biais (confirmation et coûts irrécupérables).\nPour rendre nos assumptions les plus risqués moins risqués, on va** collecter des données sur ce que les clients font vraiment dans un cas spécifique**, pas sur ce qu’ils disent qu’ils feraient en général.\nIl va s’agir de simuler une mise en situation, et de comparer le comportement de l’utilisateur avec celui que notre assumption aurait sous-tendu.\nExemple : si on reprend notre exemple de plateforme de streaming, et qu’on veut tester l’assumption “Nos clients veulent regarder du sport”. On va pouvoir simuler la situation en présentant un mockup de la page d’accueil de notre plateforme, et leur demander : “Qu’est-ce que vous aimeriez regarder maintenant ?” en leur proposant plusieurs options.\n\n\nOn se retrouve souvent avec plusieurs idées qui partagent la même assumption : du coup la traiter permet de traiter plusieurs idées.\nIl est important de définir à l’avance la condition de succès. Si on ne la définit pas à l’avance, on n’aura pas de résultat actionable, et surtout on va céder à nos biais.\nExemple : Pour le test de notre assumption sur les utilisateurs qui veulent regarder du sport, on décide à l’avance que le succès serait d’avoir 4 utilisateurs sur 10 qui choisissent de regarder du sport.\nPour ce qui est du chiffre en question, on va le négocier au sein du product trio. A noter qu’il ne s’agit pas de prouver la chose, mais juste de réduire le risque.\n\n\nIl est préférable de commencer par de petits tests pas chers pour avoir un potentiel signal d’échec rapide, plutôt qu’investir beaucoup sur un gros test sans avoir déjà eu un retour.\n1 - On choisit l’assumption la plus risquée pour faire un test et la rendre moins risquée ou l’éliminer.\n2 - On choisit à nouveau l’assumption la plus risquée. Si la précédente est toujours la plus risquée, alors on fait un test plus important sur elle, sinon on prend une autre plus risquée avec un petit test.\n3 - On fait des tests de plus en plus gros sur nos assumptions risquées qui ont survécu jusqu’à ce qu’implémenter l’idée dans notre application soit moins cher que le test.\nAvec de petits échantillons on risque d’avoir des faux positifs ou des faux négatifs, mais ce n’est pas très grave.\nPour alléger ce risque, on peut tenter de diversifier les personnes interviewées, en particulier sur des critères en rapport avec l’assumption testée.\nSi on tombe sur un faux négatif (l’assumption est invalidée alors qu’elle était bonne), on pourra toujours se rattraper sur l’assumption suivante. Au pire on se retrouvera avec un petit redesign ou une idée abandonnée. Mais des idées il y en a des milliers.\nC’est pareil en cas de faux positif (l’assumption est validée alors qu’elle était fausse), on s’en rendra compte au test de assumption suivante, ou au test plus important de cette même assumption qui sera probablement en contradiction avec notre faux positif.\n\n\nComparé à ce que fait la science : on utilise ici une méthode proche de la méthode scientifique, mais notre but est d’aller bien plus vite. On réduit simplement le risque plutôt que de rechercher la Vérité.\n\n\nSelon Marty Cagan, les meilleures équipes font 15 à 20 itérations de discovery par semaine (qu'est-ce qu’elle veut dire par “itération de discovery” ?).\nPour être efficaces il y a deux outils pour tester rapidement les assumptions :\n1 - Le user-testing non modéré (unmoderated user-testing) : on a un outil qui nous permet de créer un prototype, et d’ajouter des tâches à faire ou des questions. On envoie ça à des utilisateurs qui pourront le remplir quand ils auront le temps. Et on n’aura plus qu’à visionner les vidéos de ce qu’ils ont fait.\nCes outils (elle ne donne pas d’exemples) sont des game-changers : ils permettent de faire en un jour ou deux ce qu’on mettait plusieurs semaines à faire en terme de test d’assumptions.\n\n\n2 - Le sondage à question unique (one question survey) : on crée un formulaire avec une seule question, et on l’envoie à nos clients.\nÇa peut être utile par exemple pour trianguler un test d’assumption qu’on aurait déjà fait avec le user-testing non modéré, en posant la question d’une autre manière.\nÇa peut être pour tester les préférences des utilisateurs : “Veuillez sélectionner vos sports préférés parmi la liste”.\n\n\nLes mêmes règles d’assumption testing s’appliquent à ces outils : on simule des instances spécifiques pour mettre la personne en situation ou aller chercher un comportement passé précis. Et on ne demande pas ce qu’elle fait en général ou ce qu’elle fera dans le futur.\n\n\nOn a aussi parfois les données qu’on cherche déjà dans nos bases de données.\nPar exemple : le nombre de fois où les sports ont été recherchés par les utilisateurs.\nAttention là aussi à définir à l’avance le nombre qui constituerait un critère de succès du test.\n\n\nPour aller plus loin sur les types de tests à mettre en place elle conseille deux livres :\nUX for Lean Startups de Laura Klein.\nTesting business ideas de David Bland.\n\n\nQuelques anti-patterns à éviter :\nDes simulations trop longues : le but est d’aller vite pour pouvoir itérer dans la bonne direction. Nos tests devraient être complétés en un jour ou deux, ou une semaine max.\nUtiliser des pourcentages au lieu des nombres bruts pour les critères de succès : définir les nombres (total et de succès) permet de garder en tête la fiabilité au cours des différents tests plus ou moins importants.\nNe pas définir suffisamment de critères d’évaluation : il faut au moins le nombre total de personnes à interroger, et le nombre total de succès. Mais parfois si notre test est plus complexe, il peut y avoir plusieurs valeurs à mesurer au total et de succès.\nPar exemple, si on envoie des emails, on peut mesurer ceux qui ouvrent, ceux qui cliquent sur notre lien etc.\n\n\nTester avec les mauvais utilisateurs : bien s’assurer que les personnes interviewées ont bien les besoins, pain points ou désirs de l’opportunity visée.\nConcevoir les tests avec un scénario plus difficile que le plus basique : il faut que notre scénario soit celui qui a le plus de chances de marcher, et on le challengera par la suite avec plus de monde et des cas plus complexes. Ca permet d’accorder une vraie valeur à un cas d’échec, plutôt que de se dire que ça aurait pu marcher avec d’autres personnes ou fait autrement.","11---measuring-impact#11 - Measuring impact":"Il ne faut pas essayer de tout mesurer dès le début.\nOn peut passer des semaines à perdre du temps à essayer de tout planifier, choisir le nom des events qu’on track etc. pour se rendre compte que ce n’était pas ce qu’on croyait.\n\n\nIl faut commencer par mesurer ce dont on a besoin pour valider nos assumption tests, et pas au-delà.\nExemple : Teresa était dans une université, sur un système permettant d’aider les étudiants à trouver un job.\nSon équipe avait remarqué qu’on posait les mauvaises questions aux étudiants sur l’application, et qu’à cause de ça ils ne s’engageaient pas sur la plateforme.\nIls ont créé un prototype d’une version alternative, et l’ont utilisé pour tester des assumptions :\nLes étudiants feront plus de recherches si on leur pose des questions plus simples.\nLes étudiants verront plus de jobs recommandés.\nLes étudiants postuleront à plus de jobs recommandés.\n\n\nPour tester ces assumptions, les critères d’évaluation étaient :\n250 visiteurs sur 500 utiliseront l’interface.\n63 étudiants sur 500 verront un job recommandé.\n7 étudiants sur 500 postuleront à un job recommandé.\n\n\nEt pour mesurer ça, ils ont collecté :\n# de personnes qui ont visité la page de recherche\n# de personnes qui ont fait une recherche\n# de personnes qui ont vu au moins un job\n# de personnes qui ont postulé à au moins un job\n\n\nIls n’ont pas commencé par mesurer tous les clics mais bien seulement les actions pour valider leurs tests.\n\n\n\n\nOn peut mesurer le nombre total d’actions, ou le nombre de personnes faisant l’action au moins une fois.\nPour choisir l’un ou l’autre, on peut se demander si une même personne effectue l’action plusieurs fois, est-ce que ça apporte plus de valeur vis-à-vis de ce qu’on recherche.\n\n\nIl faut aussi mesurer l’impact sur l’outcome, même si c’est difficile, et ne pas se contenter de leading indicators.\nSi on reprend l’exemple de l’université, les mesures faites représentaient des leading indicators (nombre d’étudiants postulant à un job via la plateforme), mais ne mesuraient pas l’outcome business qui était : “Les étudiants trouvent un job via la plateforme”.\nL’outcome business lui-même était hors de leur contrôle du fait que l’obtention du job se passait hors de la plateforme.\nIls l’ont mesuré quand même au travers d’un questionnaire envoyé aux étudiants quelques temps après leur process. Ils ont itéré dessus pour augmenter le nombre de réponses.\n\n\n\n\nQuelques anti-patterns à éviter :\nRester coincé en essayant de tout mesurer : le problème le plus fréquent est de penser qu’on peut connaître à l’avance ses besoins produit à mesurer.\nIl vaut mieux mesurer seulement les assumption tests du moment, puis leur lien avec le product outcome, et avec le temps le lien avec le business outcome.\n\n\nSe concentrer à fond sur les assumption tests et en oublier de remonter dans l’OST : il ne faut pas oublier de remonter de l’outcome produit vers l’outcome business pour bien vérifier qu’on apporte de la valeur d’une manière qui sera durable.","12---managing-the-cycles#12 - Managing the cycles":"Ce chapitre présente de vrais exemples pour illustrer le fait que le processus de discovery n’est pas linéaire. Il faut prendre en compte les résultats pour soit continuer, soit revenir à une autre étape pour changer quelque chose.\n1 - Simply Business est une compagnie d'assurance.\nL’équipe produit reçoit régulièrement un pain point de la part de leurs clients : les retards de paiements. Elle confirme aussi ça avec une étude de marché.\nElle imagine 3 solutions pour aider les clients : des articles pour conseiller les petites entreprises, des rabais pour les payeurs rapides de leurs clients, et une solution technique de collection des paiements.\nEn une semaine ils ont fait 3 assumption tests pour chaque solution, et les ont proposé sous forme d’unmoderated tests. Les premiers tests cherchaient à savoir si les utilisateurs avaient bien compris les différentes offres.\nLes résultats tombent très vite : les utilisateurs n’étaient en fait pas intéressés, parce qu’ils pensaient qu’être sortis de la boucle serait dommageable pour leur business. Ils ont le problème, mais ne veulent pas de l’aide de Simply Business sur ça.\nL’équipe produit a donc choisi de déprioriser cette opportunity, et d’en choisir une autre dans leur OST, puis d’utiliser leurs interviews suivantes pour rebondir rapidement sur une autre sujet.\nConclusion : ils ont gagné du temps en évitant de créer une feature sur un mauvais sujet.\n\n\n2 - CarMax reconditionne et revend des voitures.\nL’équipe produit repère une opportunity : “Je veux être confiant sur l’état de la voiture”.\nElle teste d’abord que des réparations cosmétiques sont vraiment importantes pour le client avec un assumption test présentant une voiture avec un défaut cosmétique moins cher, et une voiture sans le défaut mais plus chère. Le test est concluant.\nIl y a 2 types de solutions possibles :\nCelles qui vont être spécifiques à chaque voiture, où il faudra indiquer ce qui a été réparé par CarMax. Celles-ci sont difficiles à mettre en œuvre et impliqueront plusieurs équipes.\nEt puis il y a la solution de type “quick win” où il s’agira de communiquer sur la qualité générale de l’entreprise, et son attention portée au détail.\n\n\nIls ont décidé de tenter la solution quick win, et ont mené des assumption tests avec des communications sur certaines photos bien placées.\nMalheureusement leurs seuils n’ont pas été atteints, parce que les clients voulaient absolument des éléments spécifiques à la voiture qui les intéresse.\nIls ont donc choisi de remettre l’opportunity à plus tard parce qu’elle prendrait du temps. En attendant, ils ont lancé l’idée auprès des autres équipes, et sont repartis sur autre chose.\nEt justement ils ont fini par la mettre en place plus tard.\nConclusion : il faut parfois explorer des solutions, et les repousser à plus tard où elles seront plus pertinentes.\n\n\n3 - FCSAmerica prête de l’argent à des agriculteurs.\nL’équipe produit s’est intéressée à la possibilité de digitaliser certaines actions, mais ils avait aussi noté que les clients aimaient la relation de confiance avec de vraies personnes de l’entreprise.\nIls ont investigué et on trouvé que les clients cherchaient déjà en ligne pour savoir combien ils pouvaient emprunter.\nIls ont donc choisi de digitaliser ce service. Et pour ajouter la touche humaine, ils y ont ajouté un chat interactif.\nMalgré toutes les tentatives pour le rendre attractif, les clients fermaient systématiquement le chat. La conclusion a été que les clients ne voulaient pas de contact humain à ce moment-là, mais seulement plus tard.\nConclusion : on peut parfois se baser sur ce que les clients font déjà, pour les pousser plus loin et les faire aller dans une direction qui sert les besoins business.\n\n\n4 - Snagajob permet aux chômeurs de trouver un emploi.\nPour améliorer la satisfaction des clients, l’équipe produit s’est attaquée à un pain point côté employeurs : “Les candidats ne répondent pas aux appels”.\nIls ont commencé par aller les voir, et les conseiller gratuitement pour suivre ce qu’ils faisaient pendant un mois.\nLe premier problème constaté était que les candidats avaient changé, et utilisaient beaucoup le mobile avec les textos, et très peu les appels vocaux.\nLa solution ne pouvant pas être d’utiliser des textos côté employeurs parce qu’ils ne voulaient pas, ils ont créé un service accessible via mobile pour les candidats.\nPuis ils ont découvert un problème de lenteur pour réserver un rendez-vous avec beaucoup de va-et-vient. Ils ont alors développé une solution pour améliorer ça.\nIls ont alors découvert que les candidats ne se présentaient parfois pas etc. A chaque fois ils ont amélioré la solution en allant d’opportunity en opportunity.\nConclusion : adresser des opportunities et sub-opportunities permet de petit à petit résoudre un outcome.\n\n\nQuelques anti-patterns à éviter :\nTrop s’engager sur une opportunity : les histoires de Simply Business et CarMax montrent qu’il faut savoir accepter qu’une opportunity peut être intéressante mais pas pour notre contexte, ou pas pour tout de suite.\nÉviter les opportunities difficiles : les quick wins sont bien sûr intéressants, mais une fois qu’on les a réalisés il ne faut pas rejeter les opportunities difficiles à traiter qui peuvent apporter beaucoup. Dans l’histoire de CarMax, ils ont certes laissé l’opportunity pour plus tard, mais ils ont quand même lancé l’idée pour qu’elle fasse son chemin, et ont fini par la traiter.\nTirer des conclusions à partir d’éléments superficiels : dans le cas de FCSAmerica ils auraient pu abandonner l’opportunity de digitalisation de par l’information que les gens préféraient le contact humain pour la confiance. Mais ils ont trouvé un moyen de concilier le besoin business avec les besoins des clients en cherchant un peu.\nAbandonner avant que les petits changements aient pu porter leurs fruits : dans l’histoire de Snagajob ils ont dû régler plusieurs petites opportunities avant d’entrevoir l’amélioration itérativement.","13---show-your-work#13 - Show your work":"Même en faisant une excellente discovery, si on n’a pas le reste de l’entreprise et en en particulier les décideurs avec soi, nos idées ne seront pas mises en place. Il faut donc les convaincre.\nQuand on présente notre travail, il ne faut pas aller directement à la conclusion et donc aux solutions, mais plutôt insister sur les opportunities.\nTout le monde a des solutions pré-établies en tête, et les leaders aussi. Si on amène la discussion sur le terrain des solutions, on va entrer en confrontation avec celles du leader. Or c’est lui qui a le pouvoir et qui aura donc le dernier mot, même s’il n’a pas fait le travail de discovery.\nIl y a même un dicton chez les PM : “The HiPPO always wins” (HiPPO = Highest Paid Person’s Opinions).\n\n\nIl faut présenter le cheminement qu’on a suivi en s’appuyant sur l’OST, et en prenant le temps de le faire.\nL’intérêt de faire ça c’est qu’on va rendre les personnes comme actrices du processus de discovery, et donc plus enclines à accepter les choix qui en découlent.\nD’abord présenter l’outcome, puis donner des éléments de contexte au travers des opportunities.\nIl faut bien mentionner les choix importants qu’on a pu faire au cours de la discovery et pourquoi. On peut leur présenter certains interview snapshots pour appuyer ce qu'on dit.\nIl ne faut pas oublier de prendre des feedbacks de la part de nos interlocuteurs à chaque étape, pour les intégrer à notre travail.\nUne fois le contexte posé on peut présenter les solutions et les assumption tests, en prenant toujours les feedbacks.\n\n\nQuelques anti-patterns à éviter :\nDire les conclusions au lieu de montrer le cheminement.\nSurcharger les stakeholders avec trop de détails : ne pas aller directement aux conclusions ne veut pas dire détailler l’analyse de chaque interview snapshot et le résultat de chaque discussion en interne.\nEn fonction de qui il s’agit, la personne peut vouloir plus ou moins de détails : notre manager voudra sans doute des détails chaque semaine, alors que le CEO voudra quelque chose de très concis\nAttention cependant : concis ne veut pas dire parler des conclusions plutôt que des opportunities. On peut être concis en mentionnant le cheminement de la discovery avec seulement les éléments les plus déterminants qui nous ont mené à nos conclusions.\n\n\nArgumenter avec les stakeholders sur pourquoi leurs idées ne peuvent pas fonctionner : même si l’idée est mauvaise au regard des éléments de discovery, il faut donner des éléments au stakeholder pour qu’il parvienne à cette conclusion par lui-même, plutôt que lui dire et risquer de le braquer.\nSi son idée est bonne mais ne fit pas avec l’outcome, il faut le guider dans l’OST et lui dire qu’on peut la considérer, mais plutôt pour un autre outcome.\n\n\nEssayer de gagner le “combat idéologique” au lieu de nous concentrer sur les décisions qui dépendent de nous : quelle que soit la qualité de notre discovery, si une personne plus haut placée que nous veut prendre une décision qui va contre nos conclusions, nous ne pourrons pas gagner contre elle.\nSi on se retrouve à dire “C’est la manière dont c’est censé être fait”, alors il faut prendre une inspiration et aller faire un tour.\nLa seule chose qu’on puisse faire c’est donner les clés de compréhension pour que la personne arrive à nos conclusions par elle-même. Si elle a des conclusions différentes malgré tout, on ne gagnera de toute façon pas contre elle. Teresa conseille de choisir ses combats, et de choisir plutôt ceux qu’on peut faire avancer.","part-iii---developing-your-continuous-discovery-habits#Part III - Developing your continuous discovery habits":"","14---start-small-and-iterate#14 - Start small, and iterate":"Durant toute sa carrière, Teresa a pu être confrontée à des environnements qui ne pratiquaient pas la discovery moderne, où il s’agissait de créer des produits puis de les présenter aux clients.\nSa méthode qui a toujours marché a été de faire les choses bien de son côté, sans se préoccuper du fonctionnement global de l’entreprise qui ne dépendait pas d’elle.\nLa chose la plus importante qu’elle a cherché à faire à chaque fois c’est chercher un contact avec les clients, et le garder tout au long de l’évolution du produit.\n\n\nPour mettre en place la méthode de ce livre dans notre entreprise, elle conseille de :\n1 - Se constituer un product trio.\nLes activités décrites dans ce livre sont destinées à être faites en groupe. Si on est PM, il faut trouver un software engineer et un designer qui acceptent de participer aux activités de discovery. Si pas de designer dans l’entreprise, on peut prendre une personne qui a une sensibilité sur le sujet.\nOn peut commencer petit en les incluant dans certaines activités et décisions, et itérer en allant de plus en plus loin.\n\n\n2 - Commencer à parler aux clients.\nUne fois qu’on a notre trio, on est prêt pour établir notre keystone habit : le contact hebdomadaire avec les clients. Cette habitude sera la pierre angulaire des autres habitudes de la continuous discovery.\nIl est souvent difficile d’établir ce contact avec les clients. Soit parce que les sales ou autres veulent en garder l’exclusivité, soit parce que les clients sont difficilement joignables etc. Mais il y a toujours des moyens d’y arriver.\n\n\n3 - Travailler à l’envers.\nDans le cas où on se trouve dans une entreprise orientée delivery uniquement, et où les features à faire sont élaborées par la direction sans discovery, on peut partir des features qu’on nous donne et remonter la chaîne : vers l’opportunity, puis vers l’outcome produit et l’outcome business.\nSi on parle régulièrement avec les clients, on pourra confirmer les opportunities sous-jacentes, et trouver des assumption tests pour les features. On pourra alors identifier les tests qui peuvent être problématiques.\nOn peut demander aux stakeholders ce qu’ils espèrent comme impact de la feature, et mesurer ça. Si l’impact n’est pas atteint (ce qui, sans discovery, va forcément arriver), on peut revenir vers eux et leur dire qu’on peut faire mieux, en leur proposant de générer des idées avec nous à partir de notre OST.\nAttention par contre à ne pas jouer les je-sais-tout ou, je-vous-l’avais-dit. Il faut qu’ils se sentent acteurs de cette nouvelle manière de faire, et pas en position défensive.\n\n\n4 - Utiliser les rétrospectives pour s’améliorer.\nOn peut par exemple utiliser les rétrospectives de Scrum si on applique cette méthode pour y ajouter une partie sur la discovery.\nOn peut se poser la question “Qu’est-ce qu’on a appris qui nous a surpris dans ce sprint ?”, suivi de “Qu’est-ce qu’on aurait pu faire pour le savoir plus tôt ?”.\nSi on n’a pas eu l’impact espéré, est-ce qu’on a négligé de tester une assumption ? Est-ce qu’on l’avait mal catalogué hors de notre leap of faith ? Est-ce qu’on a eu des problèmes sur la faisabilité et pourquoi ?\n\n\n\n\nQuelques anti-patterns à éviter :\nSe dire que ça ne fonctionnera jamais chez nous : Teresa a vu cette méthode implémentée dans toutes sortes d’entreprises, petites comme grandes. Il vaut mieux se concentrer sur ce qui est en notre pouvoir pour faire marcher la discovery.\nÊtre le champion de “la bonne manière” de faire : vouloir appliquer la méthode strictement et d’un coup peut aussi être dommageable. Il faut y aller itérativement et l’adapter à son contexte.\nAttendre d’avoir la permission au lieu de commencer à faire ce qui est dans nos possibilités : il ne faut pas hésiter à parler aux clients si on en a la possibilité. Ne pas hésiter aussi à parler à des personnes similaires à nos clients dans notre entourage. Tout est bon à prendre pour commencer.","15---whats-next#15 - What’s next":"Teresa propose plusieurs ressources pour aller plus loin :\nSouscrire à la newsletter mensuelle “Product Talk” : chaque mois il y a un article, soit sur une team qui marche bien, soit sur un sujet particulier de discovery.\nRejoindre la communauté “Continuous Discovery Habits” pour interagir avec d’autres personnes intéressées par le sujet.\nS’inscrire à une Master Class en petit groupe avec Teresa.\nS’inscrire à un cours d’approfondissement.\nEmbaucher un coach de chez Product Talk (teresa@producttalk.org)."}},"/books/designing-data-intensive-applications":{"title":"Designing Data-Intensive Applications","data":{"":"","1---reliable-scalable-and-maintainable-applications#1 - Reliable, scalable and maintainable applications":"Data-intensive désigne le fait que les données soient le bottleneck, par opposition à compute-intensive qui fait référence au CPU.\nLes frontières entre les différentes catégories (base de données, cache, système de queuing etc.) deviennent parfois floues. Par ex : Redis est un cache utilisé comme système de queuing, ou encore Kafka qui est un système de queuing avec une garantie de persistance comme une BDD.\nIl y a 3 enjeux principaux auxquels on répond quand on conçoit un système de données :\nLa fiabilité (reliability) consiste à fonctionner correctement malgré les fautes matérielles, logicielles, ou humaines.\nLes disques durs sont connus pour faire des fautes tous les 10 à 50 ans, ce qui veut dire que sur un parc de 10 000 disques, il y en a un qui saute tous les jours. On peut prévenir ce genre de problème par de la redondance (RAID par ex).\nLes fautes logicielles sont beaucoup plus insidieuses, et peuvent causer des dégâts en chaîne. Pour les prévenir on peut mettre en place du monitoring, prévoir des restarts de processus en cas de crash etc. Mais ça reste bien maigre en soi.\nLes fautes humaines sont inévitables, il faut concevoir les systèmes de manière à décourager les actions problématiques, faire beaucoup de tests automatisés, rendre facile le fallback etc.\n\n\nLa scalabilité consiste à accompagner le système dans sa montée en charge en termes de données, de trafic ou de complexité.\nParler de scalabilité tout court n’a pas vraiment de sens, il faut préciser sur quel aspect on scale.\nIl faut d’abord décrire le load sur lequel on veut scaler. Par ex (page 11) : pour twitter le load clé c’est le nombre de followers par personne :\nla 1ère solution consiste à recréer la timeline de tweets de chaque utilisateur depuis la base de données\nla 2ème à constituer des timelines à jour dans un cache, et de mettre à jour les timelines des followers à chaque tweet. Du coup avec la solution 2 tout dépend du nombre de followers.\nTwitter a fini par adopter une solution hybride : la 2ème solution par défaut, et la 1ère pour les comptes avec énormément de followers. Par défaut la timeline est dans le cache, mais si une célébrité est suivie, une requête sera faite pour récupérer les tweets.\n\n\nEnsuite il faut décrire la métrique de performance. Il s’agit d’augmenter le load qu’on a décrit pour voir jusqu’où on tient.\nSi notre métrique concerne un service en ligne, on va en général prendre le temps de réponse.\n(Le temps de réponse et la latence sont différents : la latence concerne le temps pendant lequel la requête est latente, c'est-à-dire qu’elle attend d’être traitée. Le temps de réponse est plus long que ça.)\nIl faut reproduire la requête un grand nombre de fois, et prendre la médiane pour avoir une idée du temps que ça prend. Dans la même idée on peut prendre les percentiles pour voir par ex. si on arrive à rester sous un certain seuil pour 99.9% de nos requêtes (appelé p999).\n\n\n\n\nPour répondre aux problématiques de scalabilité :\nUne réponse à un certain load ne marchera pas pour un load beaucoup plus important : il faut repenser régulièrement son architecture si on scale vraiment.\nIl y a le scale vertical (machine plus puissante) et le scale horizontal (plus de machines, qu’on appelle aussi shared-nothing architecture).\nEn réalité, on utilise souvent un mix des deux : des machines puissantes pour certaines tâches, et du scaling horizontal pour d’autres.\n\n\nLa création de machines supplémentaires peut être manuelle ou “élastique”. La version élastique permet d’adapter aux grandes variations mais est plus complexe aussi.\nHabituellement, avoir une application stateful qui est sur plusieurs machines est difficile à gérer, donc on essaye de garder la BDD sur une seule machine jusqu’à ce que ce ne soit plus possible. Avec l’évolution des outils, ceci sera sans doute amené à changer.\nIl n’y a pas de magic scaling sauce : chaque application de grande échelle a ses propres contraintes, ses propres bottlenecks, et donc sa propre architecture.\nQuand on crée un produit, il vaut au début passer surtout du temps à développer les fonctionnalités qu’à penser son hypothétique scaling.\n\n\n\n\n\n\nLa maintenabilité consiste à pouvoir à la fois perpétuer le système et le faire évoluer en un temps de travail raisonnable.\nPour qu’un système soit maintenable dans le temps, il faut travailler sur ces aspects :\noperability : la facilité pour les ops de faire tourner le système.\nIl faut faciliter la vie au maximum pour les ops. Ex : fournir un bon monitoring, permettre d’éteindre une machine individuellement sans affecter le reste, avoir de bonnes valeurs par défaut et un comportement auto-réparateur, tout en permettant aux ops de prendre la main.\n\n\nsimplicity : que le système soit le moins complexe possible pour le comprendre rapidement et pouvoir travailler dessus.\nOn peut par exemple réduire la complexité accidentelle, c’est-à-dire la complexité non nécessaire liée seulement à l’implémentation mauvaise.\nSinon globalement une bonne chose à faire c’est d’introduire des abstractions pour appréhender le système plus facilement. Par ex. les langages haut niveau sont des abstractions de ce qui se passe dans la machine.\n\n\nevolvability : la facilité à changer ou ajouter des fonctionnalités au système.\nIl s’agit ici de l’agilité mais appliquée à tout un système, et pas à de petites fonctionnalités.","2---data-models-and-query-languages#2 - Data Models and Query Languages":"Le modèle de données relationnel a dominé le stockage depuis les années 70, en apportant de l’abstraction autour de la manière dont les données étaient structurées, contrairement aux autres alternatives.\nToutes les tentatives de détrôner SQL ont échoué, la hype est retombée.\n\n\nNoSQL arrive dans les années 2010 et regroupe tout un ensemble de technologies qui permettent de pallier aux problématiques de scalabilité, et d’offrir une plus grande flexibilité que les BDD relationnelles\nParmi elles, il y a notamment les BDD basées sur le modèle de document.\nIl est probable que les BDD relationnelles et NoSQL soient utilisées conjointement dans le futur.\n\n\nIl y a un décalage entre la POO et le format de BDD relationnel, qui oblige à une forme de conversion. Pour certaines données on pourrait utiliser une structure en document comme JSON par exemple au lieu du relationnel. Par ex pour le cas des infos d’un CV, on pourrait la ville d’un job autant de fois qu’elle apparaît.\nOn répète alors éventuellement plusieurs fois certaines informations dans les entrées, ou alors on les met dans une table à part mais on fait les jointures à la main depuis le code applicatif.\nEn réalité, ce problème est apparu dès les années 70. Le modèle hiérarchique (proche du modèle sous forme de document qui a fait résurgence récemment donc) faisait face à 2 autres modèles : le modèle relationnel et le modèle en réseau (network model) qui a fini par être abandonné.\nLe modèle en réseau consistait à avoir un modèle hiérarchique mais avec la possibilité pour chaque donnée d’avoir plusieurs parents. Mais ça rendait le code applicatif difficile à maintenir.\n\n\n\n\nLa normalisation consiste justement dans les BDD relationnelles à trouver ce genre de répétition, et à les factoriser en une nouvelle table. Le but est d’éviter la duplication, et donc de renforcer la consistance des données. Ça permet aussi de les modifier facilement en un seul endroit.\n\n\nComparaison aujourd’hui du modèle relationnel et du modèle de document :\nSimplicité du code applicatif :\nLe modèle de document mène à un code applicatif plus simple dans le cas où il y a peu de relations many to many ou many to one (pour les one to many c’est ok puisqu’on répète de toute façon la donnée dans la table du modèle de document).\nDans le cas contraire il faudrait faire les jointures à la main donc le modèle relationnel serait meilleur (code applicatif plus simple et jointures par la BDD plus efficaces).\nDans le cas où il y a une forte interconnexion entre les données (de nombreuses relations many to many), c’est alors le modèle en graphe qui serait le plus pertinent.\n\n\nFlexibilité du schéma de données :\nC’est un peu comme la différence entre le typage statique et dynamique des langages de programmation : le modèle relationnel force à déclarer un type de données et à s’y conformer ou faire une migration. Le modèle de document permet de changer de type de données en cours de route et donc la gestion des données est entièrement confiée à l’application, qui gagne en liberté et du coup en responsabilité.\nLe modèle de document est vraiment meilleur quand les données sont de type hétérogène, ou encore si elles sont déterminées par un système extérieur sur lequel la BDD n’a pas le contrôle.\n\n\nLocalité des données :\nVu que dans le modèle de document les données sont copiées dans chaque entrée, elles sont locales à celles-ci. On peut donc les avoir avec juste une requête, et on utilise moins le disque dur qu’avec le modèle relationnel. En revanche on va chercher le document entier, donc si on a souvent besoin d’un tout petit morceau ça n’en vaut peut être pas le coup.\nCertaines BDD relationnelles permettent aussi de localiser des tables vis-à-vis d’autres (ex : Spanner database de Google, Oracle, ou encore Bigtable data model (utilisé dans Cassandra et Hbase).\n\n\n\n\nLes différentes implémentations de BDD ont tendance à converger : la plupart des BDD relationnelles supportent les opérations dans du contenu XML ou JSON, et RethinkDB et MongoDB permettent de faire une forme de jointure automatique, même si moins efficace.\nLe modèle relationnel offre un langage déclaratif, alors que le modèle hiérarchique n’offre qu’un langage impératif. L’avantage du déclaratif c’est que ça abstrait des détails qui peuvent être laissés à la discrétion de l’outil de BDD qu’on utilise pour faire des optimisations.\nMapReduce, qui est un modèle popularisé par Google et disponible dans MongoDB, CouchDB et Hadoop est entre le déclaratif et l’impératif. Il abstrait certaines opérations mais permet aussi d’ajouter du code en plein milieu d’une requête qui aurait été atomique en SQL.\n\n\nDans les bases de données de graphes, les données sont représentées sous forme d’entités reliés par des traits.\nEx : Facebook utilise un graphe géant où sont présentes des entités variées (personne, lieu, commentaire), reliés entre eux avec des types de liens différents.\nModèle property graph (implémenté par Neo4j, Titan, InfiniteGraph) :\nIl y a deux tables : les entités (vertices) et les traits (edges) avec chacun leurs propriétés, et pour les edges la liste des couples d’entités reliés par son biais.\nOn peut facilement créer de nouveaux types de liens, sans avoir besoin de vraiment modifier la structure de la BDD.\nLe langage Cypher est un langage déclaratif inventé pour Neo4j.\nL’avantage c’est que le langage de graphe permet de trouver des données en parcourant un nombre indéterminé de chemins, et donc de faire un nombre non connu à l’avance de jointures. C’est possible en SQL mais avec une syntaxe beaucoup plus longue.\n\n\nModèle triple-store (implémenté par Datomic, AllegroGraph) :\nIl s’agit de la même chose que le property graph, mais présenté différemment : on a un groupe de 3 données qui sont (sujet, prédicat, objet).\nTurtle et SPARQL sont des langages qui permettent d’utiliser le triple-store.","3---storage-and-retrieval#3 - Storage and retrieval":"Un des moyens d’organiser les données dans une BDD est d’utiliser un système de log : l’ajout de données est fait en ajoutant le contenu à la fin d’un fichier (ce qui est très rapide), et la lecture est faite en parcourant l’ensemble des données (ce qui est très lent O(n)).\nPour accélérer la lecture, on peut créer des index sur les champs dont on estime qu’ils vont souvent servir à faire des recherches. Ça accélère la lecture, mais ça ralentit l’écriture puisqu’il faudra mettre à jour l’index à chaque fois.\nOn peut utiliser des Hash index tels que implémentés dans Bitcast, le moteur de stockage de Riak. Il s’agit d’avoir une structure associant une clé à un offset en mémoire vive. A chaque recherche on n’a qu’à trouver la clé et on peut directement lire la donnée sur disque.\nPour des raisons pratiques (consistance des données, performance grâce aux opérations séquentielles et non pas random), les fichiers de BDD ne sont jamais modifiés. On écrit les nouvelles données séquentiellement (donc pas de concurrence pour l’écriture) toujours à la fin du fichier, et on fait du ménage dans le fichier dans un nouveau fichier de BDD régulièrement. Pareil pour supprimer une donnée : on insère une commande dans le fichier et ce sera supprimé à la prochaine copie / optimisation du fichier de BDD.\nLes limitations c’est qu’il faut que les clés tiennent en mémoire vive sinon c’est foutu, et que les recherches de “ranges” de clés ne sont pas efficaces, ça revient à chercher les clés une par une.\n\n\n\n\n\n\nOn peut aussi stocker les données sous forme triée dès le début. On a alors les Sorted String Table (SSTable). Ca consiste à avoir une structure d’arbre triée en mémoire où vont les nouvelles données (qu’on va appeler la memtable). Et tous les quelques Mo on écrit ça sur DD. Puis régulièrement on va faire des opérations en tâche de fond pour grouper les arbres triés en un seul. Lors d’une recherche, on va d’abord chercher dans le bloc le plus récent, puis de moins en moins récent, jusqu’à arriver au gros bloc, sachant que tous les blocs sont déjà triés.\nUn des avantages c’est qu’on n’a plus à faire entrer toutes les clés en RAM. On peut avoir en mémoire un nombre de clés beaucoup plus épars qui indique les offsets.\nAutre avantage aussi qui répond au problème du hash index : on peut faire des recherches de “range” d’index, vu que tout est déjà trié.\nEt aussi, comme tout est déjà trié et qu’on a les offsets des données groupe par groupe, on peut compresser des groupes de données ensemble.\nCe mécanisme est aussi appelé Log Structure Merge Tree (LSM Tree) en référence à un papier décrivant le mécanisme.\nDe nombreux moteurs de BDD utilisent ce principe :\nLevelDB (peut être utilisé dans Riak) et RocksDB\nCassandra et HBase, inspirés du papier Bigtable et Google.\nLucene (moteur d’Elasticsearch et de Solr) utilise un mécanisme similaire pour indexer le texte.\n\n\nEn terme d’optimisations :\nLa recherche peut être lente : on cherche dans la structure en mémoire, puis dans le premier bloc en BDD et ainsi de suite tant qu’on ne trouve pas, jusqu’à avoir cherché dans le bloc déjà compacté. Pour remédier à ça on peut approximer la recherche avec des structures efficaces appelées Bloom Filters.\nIl y a 2 types de stratégies de compaction :\nsize-tiered : les nouveaux et petits blocs sont régulièrement fusionnés avec les anciens et plus gros blocs.\nHBase utilise cette technique, alors que Cassandra supporte les deux.\n\n\nleveled : les blocs sont plus petits et la compaction se fait de manière plus incrémentale, utilisant moins d’espace disque.\nLevelDB tient son nom du fait qu’il utilise cette technique. On a aussi RocksDB ici.\n\n\n\n\n\n\n\n\nLa structure de BDD la plus utilisée et depuis longtemps est le B-Tree. La plupart des BDD relationnelles l’utilisent, mais aussi une bonne partie des BDD NoSQL.\nIl s’agit d’avoir des pages de taille fixe (en général 4 ko), organisés en couches (rarement plus de 4 couches). Chaque page contient des clés et des références vers des zones physiques de disque pour aller chercher les clés entre deux clés indiquées (sorte de dichotomie donc). On descend de couche en couche jusqu’à arriver à une page qui contient des données et pas de références vers d’autres pages.\nComme avec les LSM-Tree, pour que les B-Tree survivent à un crash sans perte de données, on va écrire toutes les opérations dans un fichier de log avant de modifier la BDD elle-même. Ensuite on peut détruire ce fichier de log.\nIl peut y avoir des problèmes de concurrence avec les B-Tree, on va alors utiliser des locks locaux pour bloquer correctement une partie de la BDD pour le thread qui écrit dedans. Ce problème n’existe pas avec les LSM-Tree puisque les opérations de restructuration sont faites en arrière plan.\n\n\nComparaison B-Tree / LSM-Tree :\nChacun a des avantages et inconvénients, le mieux selon Kleppmann c’est de tester empiriquement dans notre cas particulier lequel a la meilleure performance.\nA priori, la plupart du temps l’écriture serait plus rapide sur les LSM-Tree (à priori parce qu’il y aurait souvent une write amplification moins importante), alors que la lecture serait plus rapide sur les B-Tree (parce que les LSM-Tree ont besoin de lire plusieurs groupes de données triées jusqu’à ce que la compaction soit faite en arrière-plan).\nLes LSM-Tree sont meilleurs en particulier sur les disques durs mécaniques étant donné qu’ils organisent leurs données séquentiellement et ne font pas d’accès random.\nLes LSM-Tree stockent leurs données sur moins d’espace grâce à la compression, mais en même temps au moment où les données arrivent, ils les stockent dans un autre fichier que la BDD principale. Jusqu’à ce que les opérations d’arrière-plan soient exécutées, il y a des copies plus ou moins récentes des données qui coexistent sur le disque.\nLes B-Tree offrent plus de prédictibilité. Même si une opération d’écriture peut prendre plus de temps, on reste constant et évite des pics dans les hauts percentiles, qui peuvent arriver avec les LSM-Tree dans le cas où le disque serait par exemple surchargé et que les opérations d’arrière plan prendraient du retard.\n\n\nEn plus des index primaires il est possible de faire des index secondaires, qui vont indexer en fonction d’une autre colonne dont on estime qu’elle sera utile pour la recherche de données. La différence avec l’index primaire c’est qu’on n’a pas besoin d’avoir une unicité sur les données de la colonne indexée.\nCet index peut soit contenir une référence vers l’endroit où est stockée la donnée (qu’on appelle heap file), soit une version dupliquée de la donnée elle-même (on parle de clustered index). Il y a des avantages et inconvénients évidents à le faire et ne pas le faire (rapidité de recherche vs temps d’écriture et consistance).\n\n\nOn peut aussi faire des index multi-colonnes. Ça permet de chercher par plusieurs champs en même temps.\nLe plus connu est l’index concaténé, qui consiste à accoler plusieurs champs ensemble dans l'index, par ex “NomPrénom”, qui permet de chercher par “Nom”, ou par “NomPrénom”, mais pas par “Prénom”.\nIl y a aussi les index multi-dimensionnels, qui permettent de pouvoir chercher avec plusieurs colonnes indépendamment, utile par exemple pour la recherche de coordonnées géospatiales longitude / latitude.\nC’est implémenté par ex par PostGIS dans PostgreSQL, qui utilise des R-trees en interne.\n\n\n\n\nLucene permet de faire des recherches de termes avec des distances (une distance de 1 signifie qu’avec une lettre différente dans le mot, il sera retenu) grâce à sa structure de clés en mémoire particulière.\nLa RAM étant de moins en moins chère, on peut imaginer des BDD entièrement en RAM.\nPour pallier le risque de perte de données, on peut écrire sur disque en parallèle, avoir de la RAM avec batterie, ou encore faire des réplications en mémoire.\nPlusieurs moteurs de BDD fonctionnent comme ça :\nVoltDB, MemSQL et Oracle TimesTen, ainsi que RAMCloud qui est open source.\nRedis et Couchbase offrent une durabilité faible en écrivant sur disque de manière asynchrone.\n\n\nContrairement à ce qu’on pourrait penser, le gain de performance d’utiliser des BDD in-memory ne vient pas forcément de l’écriture/lecture sur DD en elle-même, puisque l’OS met de toute façon les données récemment manipulées en cache dans la RAM. En fait, le gain vient surtout du temps de conversion des données dans un format qu’on peut écrire sur DD.\n\n\nCe qu’on appelle transaction n’a pas forcément besoin d’être ACID (atomic, consistant, isolé, durable). Il s’agit simplement d’un terme désignant des lectures/écritures avec faible latence, par opposition à batch, qui lui désigne les jobs faits périodiquement dans le temps.\nLa transaction classique est appelée OLTP (OnLine Transaction Processing). Il existe un autre type de transaction : OLAP (OnLine Analytic Processing) qui consiste à agir sur peu de colonnes mais un très grand nombre d’entrées, pour faire des analyses de données (par exemple des comptages, statistiques etc.).\nDepuis les années 80 les grandes entreprises stockent une copie de leur BDD dans un Data Warehouse : une base de données structurée de manière à optimiser les requêtes d’analyse, et ne risquant pas d’affecter la prod.\nSQL permet d’être performant sur l’OLTP comme sur l’OLAP, globalement c’est ça qu’on va utiliser sur les data warehouses. Par contre les BDD sont structurées bien différemment pour optimiser l’analyse.\nUn certain nombre d’acteurs proposent des data warehouses avec des licences commerciales onéreuses : Teradata, Vertica, SAP HANA, ParAccel (ainsi que Amazon RedShift qui est une version hostée de ParAccel)\nD’autres acteurs open source de type SQL-on-Hadoop concurrencent les premiers : Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, Apache Drill.\n\n\nDe nombreux data warehouses sont organisés selon un star schema. On a la fact table au centre avec en général des dizaines voir centaines de champs, et représentant les événements étudiés. Et autour on a les dimension tables, répondant aux questions who, what, where, when, how, why et liées à la fact table par des foreign keys, ils représentent en quelques sortes les metadata.\nUne variation du star model s’appelle snowflakes, il s’agit d’une version plus normalisée, où on va davantage découper les dimention tables en sous-tables.\n\n\nLa plupart du temps, les data warehouses utilisent un column-oriented storage plutôt qu’un row-oriented. Il s’agit de stocker les données des colonnes physiquement côte à côte, parce que les requêtes vont avoir besoin de lire en général quelques colonnes entières.\nLa plupart du temps les bases en colonne sont relationnelles, mais il y a par ex Parquet, basé sur Google Dremel, qui est orienté colonne mais non-relationnel.\nCassandra et HBase ont un concept de column families, mais ça consiste seulement à stocker toutes les colonnes d’une entrée ensemble, elles sont en réalité essentiellement row oriented.\nDans le cas où les valeurs dans les colonnes se répètent (en particulier s’il y a beaucoup plus de valeurs que de valeurs possibles), on peut faire une compression sur les colonnes. Par exemple une compression de type bitmap encoding\nUn des avantages des column oriented storages c’est que ça se prête bien à un traitement optimal entre la RAM et le cache du CPU, avec de petits cycles de traitement de données compressées provenant de la même colonne.\nOn peut profiter du mécanisme des LSM-Trees avec les données en mémoire et le reste de la BDD sur disque, pour trier les entrées d’une façon particulière. Par exemple, on peut choisir la colonne qui est souvent la plus recherchée, et trier les entrées de manière à avoir toutes les entrées avec la même valeur dans cette colonne côte à côte. Et ainsi de suite pour les colonnes secondaires. Ça permet une meilleure recherche mais aussi une meilleure compression pour ces colonnes-là.\nOn peut également choisir de trier différemment chaque copie de la BDD qu’on possède, pour choisir celle qui nous arrange le plus au moment de faire une requête. C-Store et Vertica font ça.\n\n\nPour optimiser les requêtes dans les column oriented databases, à la place d’un inde on peut mettre en place une materialized view, qui consiste à ajouter une valeur ou une colonne de valeurs contenant des calculs (MIN, MAX, SUM etc.).\nUn cas particulier s’appelle le data cube, il s’agit de prendre deux colonnes comme composant les deux dimensions d’une valeur qu’on cherche à analyser, et d’ajouter une colonne représentant un agrégat (par ex la somme des valeurs sur une des dimensions).\nCette pratique permet d’accélérer les requêtes parce que certaines choses sont pré-calculées, mais ça offre aussi moins de flexibilité. Donc en général on s’en sert comme boost de performance tout en laissant aux data analyst la possibilité de faire les requêtes qu’ils veulent.","4---encoding-and-evolution#4 - Encoding and evolution":"Quand on change les fonctionnalités, y compris la BDD, il est utile de pouvoir faire une rolling upgrade (ou staged rollout). Il s’agit de déployer le code sur certains nœuds, s’assurer que tout va bien, puis déployer progressivement sur les autres.\nCela implique que le code et la BDD doivent être backward-compatibles (supporter les fonctionnalités du code précédent) mais aussi forward-compatibles (que le code précédent ignore les nouvelles fonctionnalités).\n\n\nLes données ont besoin d’au moins 2 représentations : une en mémoire avec des pointeurs vers des zones mémoire, et une quand on veut transmettre la donnée sur disque ou à travers le réseau. Il faut alors que tout soit contenu dans le bloc de données. La conversion de la mémoire vers la version transportable s’appelle encoding (ou serialization ou marshalling).\nIl existe des formats liés à des langages, comme pickle pour python, mais ces formats ne sont ni performants, ni ne gèrent bien la rétro (et forward) compatibility. Et le format de données est trop centré sur un langage particulier.\nOn a les formats plus standards comme XML, JSON et CSV.\nXML et CSV ne distinguent pas les nombres des strings, alors qu’en JSON on distingue les nombres mais pas les flottants des entiers par exemple.\nXML et JSON ont la possibilité d’avoir des schémas associés mais ceux-ci ne font pas consensus.\nGlobalement ces formats sont suffisants pour une communication entre organisations, tant que celles-ci s’accordent sur des conventions. En réalité, la plus grande difficulté est que des organisations différentes s’accordent sur quoique ce soit.\n\n\nOn a enfin les formats binaires non spécifiques à un langage et conçus pour la performance.\nIl existe des versions binaires (ne faisant pas consensus) pour JSON et XML. Par exemple pour JSON il y a MessagePack, BSON, BJSON, UBJSON, BISON et Smile. Le souci de ces formats c’est qu’ils sont assez peu compacts parce qu’ils embarquent le nom des champs répété à chaque entrée de donnée.\nApache Thrift, développé par Facebook, et Protocol Buffers (ou protobuf), développé par Google sont deux formats binaires apparus en open source en 2007/2008. Leur particularité est qu’ils ont besoin d’un schéma, et qu’ils ne répètent pas le nom des champs pour gagner de la place. Ils ont tous les deux des adaptations dans la plupart des langages pour sérialiser / désérialiser des données dans ce format à partir des structures du langage.\nA propos des formats :\nConcernant Thrift : Il a deux formats différents :\nBinaryProtocol qui remplace le nom des champs par des chiffres faisant référence aux champs du schéma.\nCompactProtocol qui possède des optimisations supplémentaires pour gagner de la place en encodant le numéro du champ et le type de champ sur un seul byte, et en utilisant par ex des entiers de longueur variable.\n\n\nConcernant Protobuf : il est globalement assez similaire au CompactProtocol de Thrift, avec des petites différences dans la manière d’encoder les bytes.\n\n\nA propos de l’évolution des schémas (Protobuf et Thrift) :\nAjout de champ : comme les champs sont représentés par un numéro reporté dans le schéma, on peut facilement ajouter un champ. Ce sera backward-compatible puisque le nouveau code pourra toujours lire les données qui n’ont pas les nouveaux champs, et ce sera forward-compatible parce que l’ancien code pourra juste ignorer les champs ayant un numéro qu’il ne connaît pas.\nOn ne peut juste pas ajouter une donnée obligatoire après une donnée optionnelle, parce que le nouveau code ne pourrait plus lire les données anciennes qui n’auraient pas ce champ obligatoire.\n\n\nSuppression de champ : c’est possible à condition qu’ils soient optionnels (les obligatoires ne pourront jamais être enlevés).\nModification de type de données : c’est parfois possible, mais il y a parfois le désavantage que notre donnée peut être tronquée par le code ancien qui ne lit pas toute la longueur prise par la donnée.\nProtobuf ne possède pas de tableaux mais demande à ajouter un même champ plusieurs fois si on le veut dans un tableau. Cela permet de pouvoir transformer un champ unique en tableau du même type et inversement.\nThrift ne fournit pas cette flexibilité de transformation puisqu’il a un type pour le tableau, mais il supporte les tableaux imbriqués.\n\n\n\n\n\n\n\n\nApache Avro a été développé en 2009 pour Hadoop.\nIl est similaire à Thrift et Protobuf, et a deux schémas : un (Avro VDL) lisible par les humains, et un autre plus pratique pour les machines.\nPour aller chercher un format encore plus compact, Avro ne mentionne pas de numéros pour les champs, il les met simplement les uns à la suite des autres dans le bon ordre, avec juste leur type et le contenu.\nLe support de l’évolution du schéma dans Avro se fait en considérant que la machine qui a écrit la donnée a son schéma, et la machine qui lit a le sien. A partir de ces 2 schémas, et à condition qu’ils soient compatibles, Avro calcule exactement la conversion nécessaire pour que les données lues soient correctement interprétées.\nTous les champs avec une valeur par défaut dans le schéma peuvent être ajoutés, supprimés ou changés d’ordre d’apparition.\nOn peut modifier les types de champs, avec les mêmes problématiques que pour Protobuf et Thrift.\n\n\nL’information du schéma de celui qui a écrit la donnée étant centrale dans l’encodage / décodage, elle doit être fournie avec la donnée.\nPour un grand fichier contenant plein de données, on met le schéma au début du fichier et il concerne toutes les données.\nPour une base de données qui a potentiellement des données avec des schémas différents, on peut ajouter un nombre faisant référence au schéma à chaque entrée de donnée, et avoir une table avec tous les schémas. C’est ce que fait Espresso (la base de données de document de Linkedin, qui utilise Avro)\nPour une connexion par réseau, les deux entités connectées peuvent se communiquer le schéma au début de la connexion et le garder tout au long de celle-ci. C’est ce qui se fait pour le Avro RPC protocol.\n\n\nUn de principaux avantages d’Avro sur Thrift et Protobuf est que comme le numéro des champs n’est pas dans les données, on peut facilement générer des données organisées dans n’importe quel ordre, ou avec des champs en plus etc. les mettre au format Avro avec un schéma associé, et ils pourront être lus sans problèmes. Avro a été conçu pour gérer des données générées dynamiquement, ce qui n’est pas le cas de Thrift et Protobuf.\n\n\n\n\n\n\nIl est intéressant de noter que les données associées à des schémas sont pratiques à bien des égards, que ce soit pour la flexibilité, le faible espace occupé, la documentation vivante que ça fournit. Et ces données se couplent bien avec les bases de données “schemaless” (les non relationnelles principalement donc) qui permettent de gérer les schémas au niveau de l’application.\nQuand on passe les données d’un processus à un autre il faut s’assurer que la donnée est bien comprise malgré les versions des programmes tournant sur ces processus. Il y a 3 manières de passer les données encodées d’un processus à un autre :\n1- Dataflow through databases\nDans le cas des bases de données, le processus qui écrit encode la donnée, et le processus qui lit la décode. Ces processus peuvent embarquer des versions différentes du code, et donc il faut une backward compatibility pour pouvoir lire les données avec l’ancien schéma, et éventuellement une forward compatibility dans le cas où un noeud avec le nouveau code aurait écrit des données, et que ces données doivent être lues avec un noeud dont le code est ancien.\nIl y a aussi une autre chose à laquelle il faut penser dans le code applicatif : pour le cas de la forward compatibility, si un vieux code traite des données possédant de nouveaux champs, il pourra ignorer ceux-ci, mais il faut absolument qu’il pense à les garder s’il veut mettre à jour ces données, sinon elles pourraient être supprimées sans le vouloir.\nContrairement au code qui finit par être chargé à la version la plus récente sur tous les nœuds, la base de données a en général diverses versions des données, certaines de plusieurs années. Dans la mesure du possible on ne remplit pas le contenu des nouveaux champs dans les anciennes entrées, mais on met juste null dedans. Le format Avro fournit une bonne manière de travailler avec des données nouvelles et anciennes de manière transparente.\n\n\n2- Dataflow through services\nLa communication par réseau se fait souvent avec une architecture client / serveur. Par exemple, le navigateur est client et le serveur fournit une API sur laquelle le navigateur va faire des demandes.\nOn a le même principe côté serveur avec l’architecture orientée services (SOA) (contrairement au nom SOAP n’est pas spécifiquement lié à SOA) ou plus récemment avec quelques changements ce qu’on appelle l’architecture microservices. Il s’agit d’avoir des entités indépendantes qui communiquent entre-elles via messages, et qui peuvent être mises à jour de manière indépendante, tout en communiquant avec le même format de données qui assure leur compatibilité.\nQuand des services utilisent le protocole HTTP, on appelle ça des web services. On peut les trouver entre les utilisateurs et les organisations, entre deux services d’une même organisation, ou entre deux organisations avec par exemple les systèmes de carte de crédit ou le protocole OAuth pour l’authentification.\nIl y a 2 approches populaires pour les web services : REST et SOAP (et GraphQL qui a été open sourcé en 2015 et ne figure donc pas dans le livre ?).\nREST : principes de design généraux utilisant à fond les fonctionnalités du protocole HTTP et collant à son fonctionnement (par exemple pour le contrôle du cache, pour le fait d’identifier les ressources avec des URLs).\nOpenAPI (Swagger) permet de documenter convenablement les API REST.\n\n\nSOAP : protocole basé sur XML appelé Web Service Description Language (WSDL). SOAP se base beaucoup sur la génération de code et les outils. Les messages sont en eux-mêmes difficiles à lire par un être humain. Malgré les efforts ostentatoires, l’interopérabilité n’est pas très bonne entre les diverses implémentations de SOAP. SOAP est surtout utilisé dans les grandes entreprises parce qu’il est plus ancien.\n\n\nEn plus des web services, il y a un autre groupe de protocoles de communication à travers le réseau appelé Remote Procedure Call (RPC). Il s’agit en RPC d’appeler des méthodes sur des objets, et d’attendre une réponse de ces appels.\nIl y a d’anciens protocoles spécifiques à un langage ou super complexes comme EJB (Java), DCOM (Microsoft), COBRA (trop complexe et non backward compatible). Mais il y a aussi de nouveau protocoles comme gRPC utilisant Protobuf, Finagle qui utilise Thrift, Rest.li qui utilise JSON sur HTTP, et Avro et Thrift qui ont leur propres implémentations de RPC.\nLes protocoles RPC essayent de faire passer les appels réseau pour des appels à des méthodes, mais ces choses sont de nature complètement différente : un appel réseau est imprédictible, il peut prendre un temps variable, il peut finir en timeout, il peut réussir tout en n’envoyant pas de réponse, il ne peut pas stocker de valeurs en mémoire liés par des pointeurs etc. REST quant à lui assume que les appels réseau sont de nature bien différente en les présentant comme tels.\nCeci est à tempérer un peu avec les implémentations récentes de RPC qui sont plus explicites sur la nature différente en fournissant par exemple des promesses pour encapsuler les appels asynchrones.\n\n\nBien que les protocoles RPC avec un encodage binaire permettent une plus grande performance que du JSON par dessus REST, REST bénéficie d’un débuggage plus facile avec la possibilité de tester à travers les navigateurs, il est supporté partout, et il est compatible avec un large panel d’outils construits autour (serveurs, caches, proxies etc.). Pour toutes ces raisons, le RPC est utilisé seulement au sein d’une même organisation, typiquement dans un même datacenter.\n\n\n\n\n\n\n3- Message-passing dataflow\nIl existe une manière de transmettre des données entre 2 processus qui se situe entre les appels RPC et les messages passés par une base de données : il s’agit de la transmission de messages asynchrone. On ne passe pas par le réseau mais par un message broker (ou message queue).\nLes avantages de cette approche comparé à RPC sont :\nLe broker peut faire buffer le temps que le(s) consommateur soit disponible.\nLe message peut être redélivré en cas d’échec ou de crash.\nCelui qui envoie et qui reçoit ne se connaissent pas, il y a un découplage à ce niveau.\nIl peut y avoir plusieurs consommateurs d’une même queue.\n\n\nUn des inconvénients potentiels c’est que le receveur n’est pas censé répondre. L'envoyeur envoie puis oublie.\nLes message brokers sont historiquement des logiciels propriétaires comme TIBCO, IBM WebSphere, et webMethods. Plus récemment on a des brokers open source comme RabbitMQ, ActiveMQ, HornetQ, NATS et Apache Kafka.\nLes message brokers n’imposent en général pas de format de données, donc on peut très bien utiliser les formats Avro / Thrive / Protobuf, et profiter de leur flexibilité pour pouvoir déployer indépendamment les producteurs et consommateurs.\n\n\n\n\n\n\nL’actor model consiste à se débarrasser de la problématique de concurrence avec la gestion de threads et de ressources partagées en créant des actors indépendants, ayant chacun leurs états encapsulés, et communiquant avec les autres actors via messages asynchrones.\nDans la version distribuée, ces interlocuteurs peuvent alors être sur le même nœud ou sur un nœud différent, auquel cas le message sera sérialisé pour être transmis via le réseau de manière transparente.\nIl y a une plus grande transparence vis-à-vis du fait que les messages peuvent être perdus qu’avec RPC.\nUn framework actor distribué inclut un broker pour transmettre les messages. Il y en a 3 qui sont populaires :\nAkka qui utilise la sérialisation de Java, mais peut être utilisé avec par exemple Protobuf pour permettre une meilleure backward/forward compatibilité.\nOrleans supporte les rolling upgrades avec son propre système de versionning.\nErlang OTP supporte les rolling upgrades mais il faut les planifier avec attention.","5---replication#5 - Replication":"Il y a 3 raisons pour vouloir faire de la réplication :\nGarder une copie des données proche des utilisateurs et donc avoir une faible latence.\nPermettre au système de fonctionner même si certains composants sont foutus.\nPour scaler le nombre de machines et donc le nombre de requêtes qu’on peut traiter.\n\n\nTout l’enjeu de la réplication réside dans le fait de propager les changements dans tous les réplicats. Il y a 3 algorithmes pour ce faire : single-leader, multi-leader, et leaderless.\nLa réplication des BDD distribuées a été étudiée depuis les années 70 et n’a pas changé depuis parce que la nature des réseaux n’a pas changé. En revanche, l'utilisation industrielle de ces techniques est quant à elle récente.\nLa réplication la plus évidente est la leader-based replication. Pour écrire une donnée il faut le faire auprès du nœud leader, qui va mettre à jour sa copie de la BDD et envoyer un log (ou stream) de mise à jour à tous les nœuds suiveurs qui vont l’appliquer.\nCe type de réplication est intégré au sein :\ndes BDD relationnelles suivantes : PostgreSQL, MySQL, Oracle Data Guard, SQL Server’s AlwaysOn Availability Groups.\ndes BDD non relationnelles suivantes : MongoDB, RethinkDB, Espresso.\nEt même au sein de message brokers distribués comme : Kafka et RabbitMQ.\n\n\nLa réplication peut être synchrone ou asynchrone. Si elle est synchrone, alors le nœud leader doit attendre que tous les suiveurs aient répondu “ok” de leur côté pour répondre à son tour que la transaction s’est bien passée. Si elle est asynchrone, il répond tout de suite même s' il y a eu un problème du côté des followers.\nDans la pratique on choisit rarement le mode synchrone parce que n’importe lequel des nœuds pourrait mettre plusieurs minutes à répondre à cause de problèmes réseau.\nOn choisit parfois une réplication semi-synchrone, qui consiste à avoir un seul nœud suiveur synchrone, et le reste asynchrones. De cette manière on est assurés d’avoir les données à jour sur au moins 2 nœuds. Et si le nœud suiveur synchrone ne répond plus, on promeut un autre nœud suiveur comme synchrone pour le remplacer.\nLa réplication asynchrone est également souvent choisie, surtout si les suiveurs sont nombreux ou distribués géographiquement.\n\n\nL’ajout d’un noeud suiveur supplémentaire se fait en faisant un snapshot de la base de données du noeud leader, en copiant ça sur le nouveau noeud, puis en demandant au leader tous les logs de mise à jour (toutes les transactions) qui ont eu lieu depuis le snapshot. Le nouveau nœud peut alors rattraper son retard et devenir un nœud suiveur normal.\nEn cas d’échec :\nd’un nœud suiveur : le nœud sait à quel log il s’est arrêté, donc dès qu’il va mieux, il peut demander au nœud leader l’ensemble des transactions qu’il a ratées, et se remettre à jour. Ça s'appelle le catch-up recovery.\nd’un nœud leader : c’est plus compliqué à gérer. Il faut un timeout pour déterminer qu’un nœud leader est en échec, et passé ce timeout on entame un processus de failover c’est-à-dire de remplacement du leader.\nLa procédure peut être automatique ou manuelle. Un timeout trop long peut mener à une interruption du service trop longue, et un timeout trop court dans un contexte de surcharge peut mener à gérer encore moins bien la charge. Pour cette raison, certaines organisations préfèrent la méthode manuelle.\nLe choix du nouveau leader est un problème de consensus, discuté plus tard dans le livre. A priori le nœud le plus à jour serait le meilleur choix.\nIl est possible que l’ancien leader revienne et pense qu’il est toujours le leader en acceptant les opérations en écriture. C’est une situation dangereuse qu’il faut prévoir correctement.\nDans le cas de réplication asynchrone, certaines transactions peuvent ne pas avoir été passées aux suiveurs. Si l’ancien leader revient en tant que suiveur ensuite, que faire de ces transactions ? En général on les supprime, mais c’est pas trop trop.\nEt ça peut être même problématique si les transactions sont en lien avec d’autres outils. Par exemple chez Github, un suiveur asynchrone MySQL était devenu leader avec des transactions manquantes. Il se trouve que la clé primaire était aussi utilisée dans un cache Redis qui lui avait les nouvelles transactions. Comme les nouvelles entrées ont été assignées à des valeurs de la clé primaire qui avaient existé auparavant, des utilisateurs ont pu avoir accès à des clés privées d’autres utilisateurs, issus du cache.\n\n\n\n\n\n\nFonctionnement de la réplication au niveau des messages (logs) :\nStatement-based replication : il s’agit de faire suivre toutes les instructions de base de données aux suiveurs. Par exemple un INSERT, un UPDATE, un DELETE etc.\nDans le cas d’instructions non déterministes comme RAND(), on va se retrouver avec des valeurs différentes dans les suiveurs.\nPour les champs auto-incrémentés, il faut absolument que l’ordre des requêtes soit exactement le même, ce qui limite les transactions concurrentes.\nIl est possible de travailler à rendre déterministe toutes les instructions qui pourraient poser problème, en envoyant une valeur plutôt qu’une instruction dans ces cas-là, mais il y a plein d’edge cases à traiter.\nEn général cette approche n’est pas très utilisée pour cette raison-là. MySQL l’utilisait jusqu’à une certaine version, mais utilise l’approche row-based replication depuis. VoltDB utilise en revanche cette approche.\n\n\n\n\nWrite-ahead log (WAL) shipping : il s’agit d’envoyer aux suiveurs le log des messages bas niveau (tel qu’il est utilisé par les BDD LSM-Tree, ou tel qu’il est utilisé par les B-Tree le temps que l’opération se fasse, et pour pouvoir la refaire à partir de ce log en cas d’échec).\nL’avantage c’est que c’est déterministe, mais l’inconvénient c’est que les messages sont couplés à une implémentation bas niveau de la BDD. Ce qui veut dire qu’on ne peut pas faire tourner une version différente entre le leader et les followers. Et donc exit les zero-downtime rolling updates : il faut une période de downtime.\nCe mécanisme est utilisé par PostgreSQL et Oracle.\n\n\nLogical (row-based) log replication : il s’agit de faire un peu la même chose qu’avec le WAL, mais on utilise un format de log indépendant de la BDD, avec les fonctionnalités minimales pour pouvoir mettre à jour correctement la BDD.\nOn est donc découplé du format bas niveau utilisé par la BDD, et on peut faire du zero-downtime.\nÇa permet aussi d’envoyer les logs à une autre base de données de type data warehouse en temps réel.\nMySQL binlog peut être configuré pour utiliser ce mécanisme.\n\n\nTrigger-based replication : dans le cas où on recherche plus de flexibilité, plutôt que d’utiliser les mécanismes built-in des BDD, on peut bouger la logique de réplication au niveau applicatif.\nOracle GoldenGate permet de mettre à disposition les logs de la BDD pour le code applicatif et implémenter ce mécanisme.\nUn autre moyen de l’implémenter est d’utiliser les triggers et stored procedures qui existent dans la plupart des BDD relationnelles. On peut grâce à ça exécuter du code applicatif à chaque transaction. Le résultat est placé dans une table à part et lu par un processus à part.\nDatabus for Oracle et Bucardo for Postgres font ça par exemple.\n\n\nCe mécanisme arrive avec son lot de bugs, et est moins performant. Mais il offre de la flexibilité.\n\n\n\n\nDans le cas où on choisit la leader-based replication avec des followers asynchrones parce qu’on cherche à scaler en lecture en ayant plein de followers, les followers vont se retrouver régulièrement en retard. En général c’est une fraction de seconde, mais dans la montée en charge ou avec des problèmes de réseau ça peut devenir des minutes. Ce retard s’appelle le replication lag. Et on parle d’eventual consistency pour désigner ce problème de consistance momentané des données.\nParmi les problèmes survenant il y a :\nread-after-write consistency : le fait, pour un utilisateur, de pouvoir lire ses propres writes juste après : s’il écrit un message et recharge la page, et qu’il ne voit pas son message, il pourrait se mettre à paniquer.\nOn peut lire toute donnée qui a potentiellement été modifiée par l'utilisateur depuis le leader, et les autres depuis les suiveurs. Par exemple, le profil d’un utilisateur ne peut être modifié que par lui, donc on le lit depuis le leader.\nDans le cas où la plupart des données sont potentiellement modifiables par l’utilisateur, on perdrait l’intérêt du scaling à tout lire depuis le leader. On peut alors par exemple ne lire depuis le leader que les données qui ont été modifiées dans les dernières minutes, ou encore monitorer le replication lag pour ne pas lire depuis les suiveurs qui sont trop en retard.\nLe client peut retenir le timestamp (temps ou donnée d’ordre logique) de la dernière écriture, et l’envoyer avec la requête. Le serveur peut alors n’utiliser que les suiveurs qui sont à jour jusqu'à ce timestamp, ou attendre qu’ils le soient avant de répondre.\nDifficulté supplémentaire dans le cas de dispersion géographique : toute requête envoyée au leader ne sera pas forcément proche de l’utilisateur.\nAutre problématique : si on veut que l’utilisateur puisse voir ses écritures depuis tous ses outils (navigateur, mobile etc.).\nÇa rend inopérant la technique de se souvenir de la dernière modification côté client puisqu’il y a alors plusieurs clients.\nOn a aussi des problèmes supplémentaires dans le cas où il y a plusieurs datacenters. Les deux appareils pourraient être dirigés vers des datacenters différents.\n\n\n\n\nmonotonic reads : un utilisateur pourrait obtenir des données récentes depuis un replica à jour, puis recharger la page et obtenir des données anciennes depuis un replica moins à jour. Ça donne l’impression d’aller dans le passé.\nPour éviter ce phénomène on peut servir un même utilisateur toujours avec le même nœud suiveur tant que celui-ci est vivant.\n\n\nConsistent prefix reads : il s’agit ici de respecter l’ordre causal des choses. Il faut que les données écrites en BDD le soient toujours dans le bon ordre. C’est un problème qui survient quand on partitionne la BDD (on en parlera au chapitre suivant).\nOn peut alors essayer de mettre les informations liées entre-elles dans la même partition.\nOn peut aussi utiliser des algorithmes qui empêchent les données d’être dans un ordre non causal.\n\n\nConcernant le replication lag et l’eventual consistency qui en résulte, une des solutions pour y répondre s’appelle les transactions. Leur but est d’abstraire tout l’aspect distribué du code applicatif, et de s’occuper de répondre aux problèmes décrits ici (read-after etc.). Certaines personnes disent d’abandonner les transactions qui seraient trop coûteuses, mais on nuancera ça par la suite.\n\n\n\n\nOn a ensuite la multi-leader replication. On a plusieurs leaders qui mettent à jour des suiveurs, et qui se mettent aussi à jour entre eux.\nEn général la complexité supplémentaire induite par le fait d’avoir plusieurs leaders n’en vaut pas la peine si on n’a qu’un seul datacenter.\nLes avantages du multi-leader dans un environnement multi datacenter :\nOn peut par exemple avoir un leader par datacenter, ce qui permet d’éviter de traverser la terre pour faire des requêtes d’écriture, donnant une perception de performance aux utilisateurs.\nUn datacenter entier et son leader peut avoir un problème, puis rattraper son retard sur les autres datacenters dès que c’est bon.\nLes erreurs réseau hors du datacenter impactent moins ce qui se passe dans le datacenter, étant donné qu’on n’est pas obligé d’aller chercher un nœud leader d’un autre datacenter pour écrire.\n\n\nCertaines BDD supportent le multi-leader nativement, mais en général il faut un outil externe. C’est le cas pour Tungsten Replicator pour MySQL, BDR pour PostgreSQL et GoldenGate pour Oracle.\nIl y a également CouchDB qui a été conçu pour permettre de résoudre facilement les situations de multi-leader.\n\n\nCes deux exemples illustrent le même principe que la réplication multi-leader :\nUne application de calendrier pour mobile, desktop etc. pourrait fonctionner en maintenant une copie de la BDD dans chaque client, les laissant ajouter des événements même en étant hors ligne, et synchroniser les BDD quand les clients sont à nouveau en ligne. On a bien plusieurs leaders qui peuvent écrire, une possibilité d’eventual consistency le temps que le réseau revienne, et un travail de résolution des conflits à faire.\nLes applications d’édition collaborative de texte comme Etherpad ou Google Docs fonctionnent comme si plusieurs leaders pouvaient faire des changements sur leur propre version locale, et propager ces changements de manière asynchrone. Ca aurait pu être du single-leader si chaque personne prenait un lock avant de faire un changement (un peu comme dans dropbox), mais là chaque changement est ajouté à un niveau vraiment atomique au document.\n\n\nLe problème principal de la multi-leader replication c’est les conflits entre leaders ayant eu chacun une transaction en écriture sur une même donnée.\nOn pourrait demander aux leaders d’attendre que l’autre leader ait fini sa transaction avant d’en accepter une, mais alors on reviendrait à la position de single-leader, on perdrait l’avantage d’avoir plusieurs leaders acceptant des connexions en même temps.\nDans la mesure du possible, vu que la résolution de conflit est complexe et en général mal gérée, il vaut mieux éviter les conflits. On peut par exemple rediriger les requêtes d’un même utilisateur toujours vers le même datacenter.\nParfois un datacenter est hors d’usage, ou un utilisateur peut se déplacer et se rapprocher d’un autre datacenter, et on va vouloir le rediriger vers un autre leader. Il faut alors faire converger le conflit vers un état consistent :\nOn peut donner un identifiant à chaque transaction, basé sur un timestamp, un nombre aléatoire etc. et se dire que le plus grand nombre gagnera lors du conflit pour faire valider sa transaction, annulant l’autre. Dans le cas du timestamp c’est du last write wins (LWW). C’est très populaire mais on perd des transactions.\nOn peut donner un identifiant à chaque leader, et se dire que celui qui a le plus grand gagne toujours la résolution de conflit. Mais c’est pareil qu’avec l’identifiant de transaction : on va perdre des données.\nOn peut choisir une stratégie de fusion des données des 2 requêtes, par exemple ordonner le texte alphabétiquement et le concaténer.\nBucardo par exemple permet d’écrire un bout de code en perl pour choisir quoi faire des requêtes en conflit dès que le conflit apparaît au moment de l’écriture.\n\n\nEnregistrer le conflit avec les 2 données quelque part, et laisser le code applicatif gérer ça par exemple en demandant à l’utilisateur quoi faire pour ce conflit.\nCouchDB fonctionne de cette manière-là : il stocke les 2 données, puis à la lecture les envoie toutes les deux à l’application..\n\n\n\n\nUn conflit peut être de manière évidente la modification d’un même champ, mais ça peut aussi être plus subtile et difficile à détecter. Par exemple une vérification au niveau applicatif qu’une chambre d'hôtel ne peut être réservée et donc mentionnée que par une seule réservation. Si le code applicatif a validé la requête, mais qu’on en l’a faite une dans chaque leader, on va réserver deux fois.\n\n\nQuand on a 2 leaders, ils vont forcément s’envoyer chacun des updates. Mais si on en a plus, alors on peut avoir diverses topologies de propagation des mises à jour entre leaders :\nLa star topology consiste à avoir un leader au centre qui va mettre à jour tous les autres, et prendre des mises à jours d’eux.\nLa circular topology consiste à ce que chaque leader mette à jour son voisin, et finissant en boucle. Une information au niveau de la requête permet alors de savoir si elle a déjà été traitée par le nœud courant pour arrêter la boucle.\nLa plus générale est le all-to-all topology, où tous les leaders mettent à jour tous les autres.\nUn des avantages du all-to-all est que si un nœud ne fonctionne plus, c’est transparent. Pour le circular et star il pourrait bloquer l’information et il faut alors reconfigurer la topologie.\nL’inconvénient des all-to-all est que certaines connexions peuvent être plus rapides que d’autres, et alors si on se basait sur des timestamp pour l’ordre des transactions par exemple, cet ordre pourrait ne pas être bon.\nPour pouvoir faire quand même respecter la causalité dans ce cas, on peut utiliser la technique des version vectors.\n\n\nGlobalement la résolution de conflits est plutôt mal gérée dans les solutions existantes : par exemple PostgreSQL BDR ne fournit pas de garantie causale des écritures, et Tungsten Replicator pour MySQL ne détecte même pas les conflits.\n\n\n\n\n\n\nOn a enfin la leaderless replication, qui consiste à ce que tous les nœuds puissent accepter les requêtes en lecture et écriture.\nCette idée était tombée dans l’oubli depuis longtemps et a été remise au goût du jour quand Amazon l’a implémentée dans sa base de données Dynamo system. Elle est depuis utilisée dans les BDD open source Riak, Cassandra et Voldemort. Elles sont connues pour être les “BDD Dynamo-style”.\nEn cas de problème dans un des nœuds, celui-ci va rater des requêtes. Et quand il reviendra, ses données seront anciennes et le client risque d’obtenir des données pas à jour en lisant depuis ce nœud.\nPour résoudre le problème chez le client, le client peut envoyer la requête à tous les nœuds, et à la réception utiliser la version la plus à jour parmi ceux reçus, grâce à des numéros de version dans ces messages.\nPour s’assurer que le noeud se remet à jour il y a 2 moyens implémentés dans les systèmes Dynamo-style :\nRead repair : Quand le client lit une valeur en parallèle depuis tous les réplicas, s’il constate une différence chez l’un d’entre eux qui aurait une version de transaction plus ancienne, il le met à jour avec une requête d’écriture. Ceci permet de mettre à jour les valeurs souvent lues.\nAnti-entropy process : Pour les valeurs peu lues, on peut avoir des tâches de fond qui tournent, et dont le but est de repérer les différences entre nœuds, et mettre à jour ceux qui sont en retard.\n\n\n\n\nPour savoir si une requête a réussi, on peut utiliser le quorum consistency.\nSoit :\nn le nombre de nœuds à qui on envoie les reads et writes.\nw le nombre de nœuds qui doivent confirmer un write pour le considérer comme réussi.\nr le nombre de nœuds qui doivent confirmer un read pour qu’il soit considéré comme réussi.\n\n\nAlors pour respecter le principe du quorum il faut que w + r > n.\nTypiquement on choisit n impair, et w = r = (n + 1) / 2 (arrondi au supérieur).\nSi on a beaucoup de lectures et peu d’écritures, on pourra mettre r = 1, comme ça dès qu’un seul nœud valide la lecture alors la transaction est validée. Les lectures sont alors plus rapides mais un seul nœud qui est down empêche alors l’écriture en BDD pour respecter la formule w + r > n).\n\n\nAvantages et inconvénients du quorum :\nL’intérêt de ce quorum c’est que les reads et writes se chevauchent, et donc qu’il y ait forcément au moins un nœud qui soit complètement à jour, pour être sûr que la donnée lue qui sera gardée sera complètement à jour.\nOn peut très bien choisir de ne pas respecter la formule du quorum et avoir moins de reads et writes nécessaires pour la validation. On aura alors une plus faible latence, une plus grande availability, mais une moins bonne consistance (on aura régulièrement des lectures renvoyant des données pas tout à fait à jour).\n\n\nMême avec le quorum, on peut se retrouver avec des données pas à jour dans certains cas :\nSi on utilise le sloppy quorum, on peut se retrouver avec les writes sur d’autres nœuds que les reads, et donc le chevauchement n’est plus garanti.\nIl s’agit d’une option activable sur les BDD qui permet, dans le cas où une large partie de noeuds est momentanément non disponible, de choisir de prendre quand même les writes sur d’autres noeuds partitionnés qui ne font pas habituellement partie de n pour ces valeurs-là. Et quand les nœuds sont de retour, on leur donne ces valeurs (hinted handoff). Le problème c’est que pendant le temps où ils n’étaient pas là, ils avaient peut être certaines valeurs plus à jours qu’eux seuls avaient, et les reads ont pu être servis avec des valeurs pas à jour.\n\n\nDans le cas de writes concurrents, on est en présence d’un conflit qu’il faut résoudre comme discuté précédemment. Si on choisit de résoudre en annulant une des requêtes, alors on perd des données.\nSi un read se fait en concurrence avec un read, le write pourrait être effectif chez certains replicas, et on ne sait pas ce que retournera alors le read.\nSi un write a réussi sur certains réplicas mais pas tous, et que la transaction est en voie d’annulation, les réplicas où ça a réussi peuvent renvoyer cette valeur qui sera fausse.\nSi le nœud à jour échoue, le nombre de nœuds en écriture tombe en dessous de w, et on peut n’avoir aucun nœud qui a la version à jour par rapport aux écritures déjà validées au moment de répondre.\nDes problèmes de timing dont on parlera plus tard peuvent aussi survenir.\n\n\nOn voit bien que les Dynamo-style databases ne garantissent qu’une eventual consistency, même en respectant le quorum. Pour avoir des garanties plus fortes comme le “read your writes”, “monotonic reads” etc. il faudra faire appel aux transactions et au consensus.\n\n\nMalgré l’eventual consistency de la leaderless replication, il peut être important de quantifier à quel point les données sont peu à jour dans les divers nœuds. Il faudrait mettre en place du monitoring mais c’est beaucoup moins simple que pour le leader-based où on peut facilement observer le replication lag du leader vers les followers. Là on peut avoir des valeurs peu lues très anciennes.\nLa leaderless replication est tout à fait aussi adaptée au multi-datacenter :\nCassandra et Voldemort traitent les nœuds dans les divers datacenters comme des nœuds normaux, avec le n global et un n configurable pour chaque datacenter. En général les clients n'attendent que le quorum du datacenter le plus proche pour maximiser le temps de réponse.\nRiak ne fait du leaderless classique qu’au sein des datacenters, la synchronisation cross-datacenter se fait de manière asynchrone, un peu à la manière du multi-leader replication.\n\n\n\n\nA propos de la gestion des écritures concurrentes :\nIl faut noter que malheureusement ça ne se fera pas automatiquement par les implémentations des BDD qui sont relativement mauvaises. En tant que développeur, il faut connaître ces problèmes et implémenter des solutions nous-mêmes.\nVoici quelques éléments de réflexion :\nLast write wins (LWW) : on en avait parlé, il s’agit d’éliminer une des deux transactions concurrentes en déterminant par une méthode arbitraire laquelle est la dernière dans le cas où il n’y a pas de relation de causalité. C’est arbitraire parce que la causalité entre des événements qui ne se connaissent pas n’a pas de sens. C’est ça qu’on appelle des transactions concurrentes.\nC’est problématique parce que même après avoir dit au client que la transaction s’est bien passée, elle peut être annulée en arrière-plan de manière silencieuse.\nLWW est la seule méthode de résolution de conflit supportée par Cassandra, et une feature optionnelle dans Riak. Dans Cassandra il est recommandé d’utiliser un UUID comme clé pour éviter autant que possible des écritures concurrentes.\n\n\nCe qu’il nous faut donc c’est pouvoir distinguer deux événements concurrents de deux événements causaux. Dans le cas où c'est causal on pourra faire respecter l’ordre. C’est seulement dans le cas de la concurrence qu’on est condamné à perdre des données, fusionner les données ou avertir l’utilisateur.\nPour fusionner les données, on peut utiliser des structures spéciales qui le permettent facilement comme les structures CRDT supportés par Riak.\nEn interne il s’agit de fusionner les éléments dans une liste en cas d’ajout, et de poser des tombstones dans le cas d’une suppression plutôt que de supprimer directement. Cela permet de mieux gérer la suppression au niveau de plusieurs nœuds qui en prennent connaissance au fur et à mesure, et ont besoin d’effectuer l’opération eux-aussi.\n\n\nPour distinguer les événements causaux des concurrents, on peut utiliser les version vectors. Chaque replica a sa version qu’il incrémente à chaque traitement, et l’ensemble de ces versions sont appelées version vector. Ces valeurs sont utilisées par chaque réplica pour déterminer s’il y a de la causalité ou si on garde les deux versions concurrentes.\nLes version vectors sont disponibles dans Riak 2.0, et sont appelés causal context. Le version vector est envoyé aux clients quand les valeurs sont lues, et renvoyé par les clients quand une valeur est écrite.","6---partitioning#6 - Partitioning":"Les partitions sont appelées :\nshard dans MongoDB, Elasticsearch et SolrCloud\nregion dans HBase\ntablet dans Bigtable\nvnode dans Cassandra et Riak\nvBucket dans Couchbase\n\n\nUn cluster shared-nothing signifie qu’il s’agit de plusieurs machines distinctes, par opposition au scaling vertical où c’est la même machine qui partage le processeur, la RAM etc. là on ne partage rien à part à travers le réseau.\nLes partitions existent depuis les années 80, et ont été redécouvertes par les BDD NoSQL et les Data warehouses Hadoop-based.\nVis-à-vis de la réplication, la notion de partition vient s’y superposer. On peut par exemple avoir des nœuds (ordinateurs) avec plusieurs partitions, et chacun d’entre eux peut être soit leader soit follower pour telle ou telle copie de telle ou telle partition.\nLa raison principale de vouloir des partitions est la scalabilité. Avec la réplication on pouvait scaler pour lectures, mais le partitionnement permet de scaler aussi en écriture.\nCependant, pour que le scaling fonctionne bien, il faut que la charge soit équitablement répartie entre les nœuds. Pour ce faire, on peut par exemple répartir aléatoirement les données dans les partitions (mais ça nécessiterait de demander à tous les nœuds en parallèle à chaque recherche).\nQuand la charge est mal répartie on appelle ça des partitions skewed (biaisées). Et quand un seul nœud se retrouve à tout gérer on l’appelle le hot spot.\n\n\nParmi les types de partitions on a :\nLa partition par key range. On va attribuer un range de clés à chaque nœud, et y stocker ces données-là. Si on connaît ce range à l’avance, on pourra même directement demander au nœud concerné pour notre recherche.\nOn pourra par exemple avoir le 1er nœud qui a les clés commençant par A et B, et le dernier les clés commençant par W, X, Y et Z.\nBigtable et son équivalent open source HBase, ainsi que RethinkDB et MongoDB jusqu’à la version 2.4 utilisent cette technique.\nDans chaque partition, on peut garder les entrées triées de la même manière que les LSM-Tree.\nOn a un risque de hot spot, par exemple dans le cas où on recherche par la clé qui serait le timestamp, et que les partitions sont groupées par journée. La partition du jour courant risque de devenir un hot spot. Dans ce cas on peut préfixer la clé par un nom ou autre chose, pour constituer un index concaténé par exemple.\n\n\nLa partition par hash of key. On a la même manière de stocker par clé qu’avant, sauf qu’on va hasher la clé avec une fonction de hash simple (mais qui ne donne pas de duplicata). Et on va assigner des ranges de hashs aux partitions. Ceci fait que les données seront aléatoirement réparties.\nMongoDB, Cassandra et Voldemort utilisent ce mécanisme.\nLe désavantage de hasher la clé c’est qu’on ne peut plus faire facilement de recherche par range.\nDans MongoDB, si on a activé les clés hashées, il faut envoyer les queries de range à toutes les partitions.\nRiak, Couchbase et Voldemort ne supportent pas du tout les queries de range.\nCassandra utilise un compromis entre les deux stratégies (hash et clé normale) : on a une clé composée avec une première partie hashée déterminant la partition, et ensuite une 2ème partie permettant de faire une recherche, y compris de range, dans la SSTable. On doit donc d’abord fixer la partition et ensuite on peut chercher ce qu’on veut efficacement.\n\n\nHasher la clé peut parfois ne pas suffire à éliminer les hot spot : dans le cas spécifique où on a une donnée qui est accédée / écrite de manière massive (par exemple une célébrité qui est fortement suivie qui s’exprime), il faut diviser cette entrée-là en plusieurs entrées sur plusieurs machines. On peut par exemple préfixer le hash d’un nombre et le répartir sur 100 machines différentes. Mais alors les lectures devront à chaque fois faire appel à toutes ces partitions et reconstruire la bonne donnée.\nPour le moment les BDD ne gèrent pas automatiquement ce genre de fonctionnalité, donc il faut le faire à la main.\n\n\nIl existe un concept appelé consistent hashing, mais il est surtout utilisé pour les caches, et n’est pas efficace avec les BDD. Certaines docs de BDD l'invoquent par erreur, mais pour éviter la confusion il vaut mieux qu’on parle de hash partitioning.\n\n\n\n\nLes indexes secondaires sont extrêmement pratiques pour faire des recherches dans la BDD, mais elles introduisent une complexité supplémentaire.\nPar rapport à leur support :\nHBase et Voldemort ont évité de les supporter pour éviter la complexité de l’implémentation.\nRiak a commencé leur support.\nPour Elasticsearch et Solr, ils sont leur raison d’être.\n\n\nOn a 2 manières de les implémenter avec le partitionnement :\nDocument-based partitioning : on va créer un index local à la partition. Toutes les entrées de la partition seront indexées pour la colonne choisie, mais l’index n’aura aucune idée de ce qui est indexé sur une autre partition.\nLe problème c’est que quand on veut faire une recherche par index secondaire, on va alors devoir faire une requête auprès de toutes les partitions, puisque les partitions sont séparées par clé primaire, pas par l’index secondaire. On appelle ça le scatter / gather. Ceci fait que la requête va coûter cher, et devoir attendre que tous les nœuds répondent (donc on est soumis au problème des hauts percentiles qui nous ralentissent potentiellement beaucoup).\nCette approche est utilisée quand même dans MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud et VoltDB.\n\n\nTerm-based partitioning : on crée un index global. Mais bien entendu il est hors de question de le mettre sur un seul nœud, au risque que ça devienne un bottleneck. On va le partitionner de même qu’on a partitionné l’index primaire : les premières clés de l’index secondaires seront dans la partition 1, celles juste après dans la partition 2 etc.\nCette technique rend la recherche rapide : puisqu’on sait quel nœud contient l’index qu’on veut, on lui envoie la requête directement. Par contre l’écriture est plus lente puisqu’elle va impliquer des modifications dans plusieurs partitions (celle de la donnée et de l’index primaire, et celle de l’index secondaire pour le mettre à jour).\nEn pratique, la mise à jour de l’index secondaire avec le term-partitioning se fait de manière asynchrone, et tant pis si une recherche avec l’index secondaire immédiatement après une écriture ne fonctionne pas.\nParmi les implémentations :\nAmazon DynamoDB met à jour son index term-partitioned de manière asynchrone.\nRiak et Oracle data warehouse permettent de choisir la technique de partitionnement de l’index secondaire.\n\n\n\n\n\n\n\n\nRégulièrement, pour augmenter les capacités ou remplacer une machine malade, on doit rediriger les requêtes et déplacer les données d’une machine à l’autre. On appelle ça le rebalancing entre partitions. Il y a plusieurs stratégies pour l’implémenter :\nUne stratégie à ne pas faire : hash mod N. Si on décidait de faire le modulo du hash de nos transactions pour les répartir dans les noeuds (par exemple le hash % 12 si on a 12 noeuds), alors à chaque fois que le nombre de noeuds changerait, on devrait faire du rebalancing, ce qui est beaucoup trop coûteux.\nFixed number of partitions. On va choisir un grand nombre de partitions, plus grand que le nombre de nœuds qu’on imagine qu’on va avoir, et on va attribuer plusieurs partitions par nœud (par exemple 100 par nœud). De cette manière, dès qu’on ajoute ou supprime un nœud, on peut déplacer quelques partitions ici et là pour équilibrer le tout.\nIl faut bien choisir le bon nombre de partitions, s’il y en a trop ça crée un manque de performance du fait de chercher dans trop de partitions, s’il n’y en a pas assez on va déplacer de trop gros blocs au moment du rebalancing. Ça peut être difficile à trouver si notre charge varie beaucoup.\nCette approche est utilisée par Riak, Elasticsearch, Couchbase et Voldemort.\n\n\nDynamic partitioning. Pour les BDD utilisant le partitionnement de type key range (et pas hash range), avoir un nombre de partitions fixe peut être problématique par rapport au skewing, et choisir à la main combien en mettre par nœud est fastidieux. On va donc vouloir un système qui répartir dynamiquement les partitions, par rapport à la quantité de données présente dans chaque partition.\nQuand une partition est jugée dynamiquement trop grosse, elle est coupée en 2 et une moitié est éventuellement déplacée sur un autre nœud.\nHBase et RethinkDB par exemple utilisent le partitionnement dynamique (puisqu’ils utilisent aussi le key range partitioning).\nPour le key range c’est obligatoire, mais le dynamic partitioning peut aussi être utilisé avec le hash range partitioning. MongoDB par exemple donne le choix de key range ou hash range, et dans les deux cas fait le rebalancing de manière dynamique.\n\n\nPartitioning proportionally to nodes. Le nombre fixe de partitions et le nombre dynamique de partitions est basé sur la taille des partitions. On peut choisir plutôt de se baser sur le nombre de partitions par nœud indépendamment de leur taille. On fixe un nombre de partitions par nœud et on répartit les données dedans. Si on ajoute un nœud, les partitions existantes maigrissent pour transférer une partie de leur données dans les partitions du nouveau nœud.\nCassandra et Ketama utilisent cette méthode.\n\n\nOn a un peu évoqué l’aspect manuel / automatique, mais plus concrètement :\nCouchbase, Voldemort et Riak créent des suggestions de rebalancing automatiquement, mais demandent la validation d’un administrateur humain pour opérer le rebalancing.\nLe rebalancing complètement automatique peut être tentant, mais il faut bien voir que c’est une opération longue et coûteuse, et que faire un mauvais rebalancing dans certaines conditions peut créer une cascade d’échec, le système croyant à tort que certains noeuds surchargés sont morts ou ce genre de chose. Globalement avoir un humain dans la boucle du rebalancing est une bonne idée.\n\n\n\n\nA propos de la question du routing de la requête, comment le client va savoir à quel nœud envoyer sa requête ?\nIl existe plusieurs solutions open source. Globalement 3 possibilités se dégagent :\nLe client envoie à un nœud en mode round robin (chacun son tour), et ce nœud qui connaît le bon nœud va faire lui-même la demande, va réceptionner la réponse, et la retransférer au client.\nLe client envoie la requête à un routing tier qui connaît le partitionnement actuel, et va pouvoir envoyer la requête au bon nœud.\nLe client connaît déjà le bon nœud, et va directement lui envoyer la requête.\n\n\nDans tous les cas, il y a le problème de savoir comment l’entité qui connaît le partitionnement actuel reste à jour malgré les rebalancing ? C’est un problème difficile.\nIl y a des protocoles pour atteindre un consensus dans les systèmes distribués, mais ils sont compliqués. On en parlera au chapitre 9.\nDe nombreux systèmes utilisent un service dédié au mapping entre partition / nœud et adresse ip.\nZooKeeper est l’un d'entre eux : tous les nœuds s’enregistrent auprès de ZooKeeper et lui notifient les rebalancings. C’est lui qui fait autorité en matière de routing. Et il notifie les entités qui en ont besoin (par exemple le routing tier) de l’état du réseau de nœuds / partitions.\nEspresso de Linkedin utilise Helix, qui lui-même utilise ZooKeeper.\nHBase, SolrCloud et Kafka utilisent aussi ZooKeeper.\nMongoDB utilise son outil maison et mongos daemons comme routing tier.\nCassandra et Riak utilisent un gossip protocol pour que les nodes s’échangent leurs changements de topologie. La requête peut alors arriver sur n’importe quel nœud qui la redirigera correctement vers le bon. Ça met plus de complexité sur les nœuds, mais ça évite la dépendance à un outil externe comme ZooKeeper.\nCouchbase ne fait pas de rebalancing automatique. Et il est couplé en général avec moxi, qui est un routing tier écoutant les changements venant des nœuds.\n\n\n\n\nEnfin concernant l’accès au routing tier par le client, son adresse ip en changeant que rarement, une configuration de nom via DNS est suffisante pour y accéder.","7---transactions#7 - Transactions":"Les transactions sont des unités logiques regroupant plusieurs lectures / écritures. Soit elles réussissent, soit elles échouent et alors le client peut réessayer en toute sécurité. Il s’agit d’abstraire tout un pan d’échecs partiels qu’il faut gérer sinon à la main.\nPresque toutes les BDD relationnelles, et certaines non relationnelles utilisent les transactions pour encapsuler les requêtes. Cependant avec la hype récente du NoSQL, on a un certain nombre de BDD qui arrivent avec l’idée que pour la scalabilité et la high availability, les transactions doivent être abandonnées ou donner des garanties beaucoup plus faibles.\nACID signifie Atomicity, Consistency, Isolation and Durability. Malheureusement il y a de l'ambiguïté sur chacun des termes, surtout sur l’isolation.\nAtomicity aurait pu être appelé abortability, parce qu’il s’agit d’annuler une partie des requêtes d’une même transaction si la partie suivante échoue. Comme ça on peut recommencer la transaction entière sans soucis.\nConsistency est ici entendu comme étant la cohérence des données du point de vue applicatif. Contrairement aux 3 autres termes, la consistency relève bien de la responsabilité du code applicatif. Il s’agit de règles liées au domaine en question, par exemple les débits et les crédits doivent s’annuler.\nIsolation consiste à gérer les transactions concurrentes : chaque transaction doit pouvoir s'exécuter sans être parasitée par d’autres transactions en plein milieu. On parle aussi de serializability, pour dire qu’il faut la même garantie que si les transactions étaient exécutées en série les unes à la suite des autres. La plupart des BDD ne fournissent cependant pas ce niveau de garantie.\nDurability veut dire qu’une fois la transaction commitée, elle ne peut pas disparaître toute seule mais reste dans la BDD. Ca implique par exemple la technique du log write-ahead pour les B-Tree ou LSM-Tree, pour ne pas perdre les données. Cela implique aussi la réplication dans le cas de systèmes distribués.\n\n\nL’atomicité et l’isolation concernent les transactions avec plusieurs écritures (plusieurs objets), mais aussi les “transactions” avec une seule écriture. Si un problème survient en plein milieu de l’écriture, il faut s’assurer que la base de données ne se retrouve pas dans un état inconsistant.\nOn dit parfois qu’on supporte les transaction (et même qu’on est ACID) quand on assure l’intégrité pour une seule écriture, mais c’est une erreur, la transaction désigne principalement le groupe de plusieurs écritures.\nLa garantie pour les écritures sur un seul objet est parfois suffisante, mais dans pas mal de cas il faut une garantie sur plusieurs objets :\nDans les BDD relationnelles (ou de graphe), les clés étrangères (ou les edges) doivent être mises à jour en même temps que l’objet change.\nDans les BDD de document, les données à mettre à jour sont en général dans le même document, donc pas de besoin de multi-object transaction de ce côté. Cependant les BDD de document encouragent aussi la dénormalisation à la place des jointures, et dans ce cas les données doivent être mises à jour conjointement dans plusieurs endroits pour ne pas que la BDD devienne inconsistante.\nQuand on a des index secondaires, alors il faut mettre à jour aussi cet index, et ces index sont des objets différents du point de vue de la BDD, donc on doit bien avoir des transaction multi-objets.\n\n\nConcernant l’annulation des transactions, c’est dans cette philosophie qu’est construite la notion d’ACID : si ça échoue on recommence la transaction.\nCertaines BDD ne sont pas du tout dans cette philosophie : les BDD répliquées en mode leaderless sont plutôt sur du “best effort”. La BDD exécute ce qu’elle peut, et si on est dans un état inconsistant, c’est à l’application de gérer les erreurs.\nCertains ORM comme celui de Rails et Django ne réessayent pas les transactions automatiquement, alors que c’est là le but même de l’ACIDité de celles-ci.\nCertains problèmes peuvent quand même survenir quand une transaction est abandonnée :\nIl se peut qu’elle ait fonctionné mais qu’on ne reçoive pas la réponse.\nSi l’erreur est due à une surcharge de requêtes, réessayer la transaction n’arrangera pas les choses, au contraire.\nIl ne faut pas réessayer si l’erreur est de nature permanente (par exemple une violation de contraintes, ie. une transaction qui fait quelque chose d’interdit), mais seulement si l’erreur est de nature temporaire (réseau, crash d’un node, etc.).\nSi la transaction a d’autres side-effects que sur la BDD (par exemple l’envoi d’un email), alors réessayer juste après peut refaire les side-effects. On parlera des Atomic commit et Two-phase commit plus tard.\nSi en réessayant à nouveau on échoue quand même, la requête pourrait être complètement perdue.\n\n\n\n\n\n\n\n\nL’isolation au sens strict de transactions sérialisables est quelque chose de coûteux que les BDD ne veulent souvent pas implémenter. On a donc seulement des weak isolation levels qui ne répondent pas à tous les problèmes posés par les transactions concurrentes. Il faut bien comprendre chaque problème et chaque solution proposée pour choisir ceux qu’on a besoin pour notre application.\nRead commited est le niveau d’isolation le plus basique.\nCa garantit :\nQu’il n’y aura pas de dirty reads : si au cours d’une transaction non terminée une écriture a été faite, une autre transaction au cours de la lecture ne doit pas pouvoir lire ce qui a été écrit.\nQu’il n’y aura pas de dirty writes : si au cours d’une transaction non terminée une écriture a été faite mais pas encore commitée, et au cours d’une autre transaction l’écriture est écrasée, alors il on peut se retrouver avec des données inconsistantes.\n\n\nRead commited est l’isolation par défaut dans de nombreuses bases de données, parmi elles : Oracle 11g, PostgreSQL, SQL Server 2012, MemSQL.\nCôté implémentation :\nPour les dirty reads, l’objet tout entier est bloqué avec un lock par la transaction, jusqu’à ce qu’elle soit commitée ou abandonnée.\nPour les dirty rights, on pourrait aussi mettre un lock, mais c’est perdre beaucoup en efficacité parce que certaines requêtes lentes vont empêcher de simples lectures. Alors la plupart du temps 2 valeurs sont conservées : l’ancienne valeur de l’objet qu’on donne aux nouveaux lecteurs, et la nouvelle valeur qui sera la valeur finale quand la transaction en cours sera terminée.\n\n\n\n\nSnapshot isolation and repeatable read. Le read committed garantit que sur une même donnée il n’y aura pas des lectures / écritures de transactions différentes, mais ça ne garantit pas que différents objets de la base de données resteront cohérents entre eux au cours d’une même transaction.\nProblèmes :\nOn peut par exemple lire une donnée, puis le temps qu’on lise la suivante celle-ci a été modifiée, et la combinaison des deux lectures donne quelque chose d’incohérent. En général il suffit de refaire la 1ère lecture et on a quelque chose de cohérent à nouveau.\nPlus grave : une copie de BDD peut prendre plusieurs heures, et le temps de la copie des changements peuvent être faits, de manière à ce qu’au final on ait copié au fur et à mesure quelque chose d’incohérent. Même chose avec une requête d’analyse énorme qui met beaucoup de temps à lire un grand nombre de données : si elles sont modifiées en cours de route.\n\n\nLa snapshot isolation est supportée par PostgreSQL, MySQL avec InnoDB, Oracle, SQL Server et d’autres.\nCôté implémentation :\nEn général pour les writes on a un write lock qui bloque les autres writes sur un même objet.\nEn revanche les reads n’utilisent pas de locks, et le principe c’est que les writes ne bloquent pas les reads et les reads ne bloquent pas les writes.\nChaque transaction va avoir son snapshot de données en fonction des données sur lesquelles il opère, et ces données ne seront pas changées de toute la transaction. On appelle ça le multi-version concurrency control (MVVC).\n\n\nLa snapshot isolation est appelée de différentes manières en fonction des BDD :\nDans Oracle elle est appelée serializable.\nDans MySQL et PostgreSQL c’est appelé repeatable read.\nCe terme repeatable read vient du standard SQL qui ne contient pas la notion de snapshot isolation, vu qu’elle n’existait pas à l’époque de System R (sur lequel est basée la norme SQL).\nEt pour compliquer le tout, IBM DB2 utilise le terme de repeatable read pour désigner la serializability, ce qui fait qu’il n’a plus vraiment de sens.\n\n\n\n\n\n\nPreventing lost updates. Jusqu’ici on s’est intéressé aux problèmes de lecture dans un contexte d’écritures dans d’autres transactions. Mais il y a également des problèmes survenant lors d’écritures concurrentes entre-elles. Les dirty writes en sont un exemple, et les lost updates un autre.\nSi deux transactions modifient une même valeur de manière concurrente, la dernière transaction écrasera la valeur écrite dans la première. On dit aussi qu’elle va la clobber.\nExemples : un compteur incrémenté deux fois mais qui se retrouve finalement incrémenté de 1, ou encore deux utilisateurs modifiant la même page wiki en envoyant la page entière, le dernier écrasant les modifications de l’autre.\nCe problème courant a de nombreuses solutions :\nAtomic write operations : vu que le problème des lost updates vient du fait qu’on lit d’abord la valeur avant de la mettre à jour, certaines BDD donnent la possibilité de faire une lecture suivie d’un update avec une atomicité garantie.\nMongoDB fournit aussi la possibilité de faire des modifications locales à un document JSON de manière atomique.\nRedis permet de modifier par exemple des priority queues de manière atomique.\nEn général les BDD le font en donnant un lock sur l’objet concerné par l’écriture.\n\n\nExplicit locking : on peut, en pleine requête SQL, indiquer qu’on prend un lock manuellement sur le résultat d’une partie de la requête, pour le réutiliser dans une écriture juste après.\nOn peut facilement oublier de le faire ou mal prendre en compte la logique applicative.\n\n\nAutomatically detect lost updates : de nombreuses BDD permettent de vérifier la présence de lost updates, et en cas de détection d’annuler la requête et de la retenter juste après.\nL’avantage aussi c’est qu’on peut le faire avec la même fonctionnalité que le snapshot isolation. PostgreSQL, Oracle et SQL Server le font de cette manière. MySQL / InnoDB en revanche ne supportent pas cette fonctionnalité.\n\n\nCompare-and-set : certaines bases de données qui ne fournissent pas de transactions permettent des opérations compare-and-set qui consistent à exécuter un changement seulement si la donnée n’a pas été modifiée depuis la dernière fois qu’on l’a lue, ce qui permet normalement d’éviter les lost updates.\nDans le cas des BDD avec réplication : quand on a de la réplication les locks ne servent à rien, et le compare-and-set non plus. La meilleure solution est d’exécuter les deux requêtes et de garder une copie des deux résultats, puis de faire appel à du code applicatif ou d’utiliser des structures spéciales de fusion pour résoudre le conflit.\nRiak 2.0 fournit des structures qui permettent d’éviter les lost updates à travers les réplicas.\nMalheureusement la plupart des BDD ont par défaut une stratégie last write wins (LWW) qui est provoque des lost updates.\n\n\n\n\n\n\nWrite skews and phantoms : on généralise ici le cas des dirty writes et des lost updates dans la mesure où on va écrire sur des objets différents. Chaque requête concurrente lit les données, puis écrit dans un objet différent, mais comme ils le font indépendamment, le code applicatif ne se rend pas compte qu’ils cassent une contrainte applicative qui devait être garantie par le code applicatif. On appelle ça des write skew.\nExemple : il faut au moins un docteur on-call, il en reste deux et les deux décident de cliquer sur le bouton pour se désister. Les deux transactions se font en parallèle et modifient des objets différents liés au profil de chaque docteur.\nLes solutions sont moins nombreuses :\nLes BDD ne fournissent pas de moyen de mettre des contraintes sur des objets différents. On peut en revanche utiliser du code custom avec les triggers ou les materialized views si c’est supporté.\nOn peut locker les objets concernés par notre logique métier à la main au moment de faire la requête.\nCette solution marche si on a déjà les objets dont on veut que la valeur ne change pas. Mais si dans notre cas la condition c’est qu’une entrée avec une certaine caractéristique n’existe pas pour pouvoir faire quelque chose (par ex insérer un nom d’utilisateur s’il n’est pas déjà pris), alors on ne peut pas locker à la main une absence d’objet.\nDans ce cas où le write skew est causé par une écriture dans une transaction, qui change le résultat d’une recherche dans une autre transaction, le phénomène est appelé un phantom.\nUne solution (peu élégante) peut consister à matérialiser les phantoms en créant une table spéciale avec un champ pour chaque élément possible, et demander au code applicatif de faire un lock manuel sur l’élément matérialisé correspondant à chaque write. Dans la plupart des cas, il vaut cependant mieux privilégier la serializability.\n\n\n\n\n\n\nMalheureusement la snapshot isolation ne suffit pas, il faut une vraie serializability dont on va parler un peu plus loin.\n\n\n\n\n\n\nSerializability : il y a un niveau au-dessus de tous les autres, qui permet de garantir que les transactions vont s’exécuter avec le même niveau de garantie vis-à-vis des race conditions que s’ils étaient exécutés les uns à la suite des autres, sans parallélisme du tout. Il y a 3 techniques pour l’implémenter dans un contexte non distribué :\nActual serial execution : on va exécuter les transactions vraiment les uns à la suite des autres, sur un seul thread.\nCette option est envisagée maintenant alors qu’elle était rejetée auparavant parce que la RAM est peu chère et on peut mettre l’essentiel de la BDD dedans, ce qui permet de rendre les transactions très rapides. Et aussi parce que les transactions OLTP sont courtes et impliquent peu de requêtes, alors que les OLAP sont certes longues mais sont read-only donc peuvent se faire hors de l’execution loop.\nCette approche est utilisée dans VoltDB / H-Store, Redis et Datomic.\nPour que ce soit possible sur un seul thread, il faut qu’il ne soit pas bloqué pendant qu’on demande à l’utilisateur la suite en plein milieu de la transaction. Il faut donc collecter les données qu’il faut pour toute la transaction, et faire la transaction entière en une fois. Pour ce faire, on utilise les stored procedures.\nCes procédures permettent d’exécuter du code écrit dans un langage spécifique : pour Oracle PL/SQL, pour SQL Server T-SQL, pour PostgreSQL PL/pgSQL, mais ces langages sont vieux, peu testables, et n’ont pas beaucoup de fonctionnalités.\nDes BDD modernes permettent cependant d’utiliser des langages modernes pour les stored procedures : VoltDB utilise Java et Groovy, Datomic utilise Java et Clojure, Redis utilise Lua.\n\n\nPour la réplication, VoltDB permet d’exécuter les stored procedures sur chaque machine. Il faut alors que ces procédures soient déterministes.\nDans le cas où on veut scaler en écriture on a besoin de partitionnement. On peut alors créer autant de partitions que de coeurs de processeur sur la machine, et assigner un thread par partition. Chaque partition exécutera bien les transactions de manière séquentielle.\nAttention par contre aux requêtes qui ont besoin d’effectuer des opérations à travers plusieurs partitions (à peu près tout sauf les données key/value), ça provoque des ralentissement de plusieurs ordres de grandeur.\n\n\nDonc les contraintes pour utiliser l’exécution en série :\nChaque transaction doit être petite et rapide.\nLa BDD doit entrer en RAM. Une partie peu utilisée de la BDD peut rester sur disque, mais si on doit aller la chercher dans le thread unique c’est chaud au niveau perf. Une solution pourrait être d’abandonner la transaction, mettre la donnée dont on a besoin en RAM, et la retenter.\nLa charge en écriture doit être assez faible pour être traitée par une machine, ou alors il faut un partitionnement sans requêtes qui s’exécutent sur plusieurs partitions.\n\n\n\n\nTwo-Phase Locking (2PL) : c’est l’algorithme qui a été utilisé pendant 30 ans. Il s’agit de mettre un lock sur la donnée dès lors qu’on est en présence d’une transaction qui fait un write, même vis-à-vis de transactions qui ne font que des reads. En revanche s’il n’y a que des transactions qui font des reads, pas besoin de lock.\nComparé au snapshot isolation où les writes ne bloquaient pas les reads, et les reads ne bloquaient pas les writes, ici les writes bloquent aussi les reads.\n2PL est utilisé dans MySQL (InnoDB), SQL Server et DB2.\nFonctionnement : il y a les shared locks et les exclusive locks. A chaque fois qu’un read est fait sur un objet, la transaction prend un shared lock, qui permet de la faire attendre au cas où l’exclusive lock serait pris. Si une transaction veut faire un write, alors elle prend l’exclusive lock dès qu’elle peut, et tout le monde doit attendre pour accéder à cet objet que sa transaction entière soit terminée (d’où le 2-phase : on prend le lock, puis on termine le reste de la transaction de manière exclusive).\nPour être vraiment comme des transactions sérialisées, il faut aussi résoudre le problème des phantoms (un write qui modifie le résultat d’une recherche). On le fait en créant des locks sur des prédicats : si une transaction a besoin de faire une query pour chercher quelque chose, alors elle déclare un shared lock sur un prédicat, et si un write modifie le résultat correspondant à ce prédicat, alors ils se bloqueront mutuellement.\nLe lock sur des prédicats étant très mauvais d’un point de vue performance, on approxime souvent les prédicats sous forme de lock d’index, en s’assurant qu’on lock éventuellement plus d’objets, et pas moins pour respecter la sérialisabilité.\n\n\n\n\nLe souci de cette méthode c’est la performance, en partie du fait de nombreux locks, mais surtout du fait que n’importe quelle transaction peut faire attendre toutes les autres. Donc on a un flow assez imprédictible, et des high percentiles mauvais.\nLes deadlocks sont détectés et résolus en annulant l’une des transactions, mais s’ils sont nombreux, ça fait d’autant moins de performance.\n\n\n\n\nSerializable Snapshot Isolation (SSI) : il s’agit d’un algorithme très prometteur qui fournit la sérialisabilité, et en même temps n’a que très peu de différence de performance avec la snapshot isolation.\nLa SSI est à la fois utilisée par les BDD single node (PostgreSQL depuis la version 9.1) et distribuées (FoundationDB).\nFonctionnement : contrairement à l’idée de faire des locks pour protéger la transaction d’un conflit éventuel, qui est une approche dite pessimiste, ici on adopte une approche optimiste et on réalise toutes les transactions dans un snapshot à part. Au moment du commit on vérifie qu’il n’y a pas eu de conflits. S’ils ont eu lieu, on annule la transaction et on laisse l’application recommencer.\nIl y a une difficulté vis-à-vis du fait de détecter si une transaction avec lecture initiale suivie d’une écriture devient invalide parce que la donnée lue est modifiée par une autre transaction. Il y a 2 solutions pour régler ça :\nDétecter les lectures faites sur le MVCC (multi version concurrency control) qui ne sont plus à jour au moment où la transaction veut être commitée. Si on détecte, on annule la transaction.\nDétecter les writes qui affectent les reads d’une autre transaction en plaçant une balise sur l’index concerné pour indiquer que plusieurs transactions utilisent la donnée. Au moment de commiter, la BDD vérifie qu’il n’y a pas de conflit par rapport au write fait par la transaction qui avait été marquée. Si oui on annule la dernière qui veut commiter. Le marquage peut être enlevé quand la situation de concurrence est résolue.\n\n\n\n\nAu niveau de la performance, plus la BDD est précise sur quelle transaction doit être annulée, et plus ça lui prend du temps. D’un autre côté si elle en annule trop ça fait plus de transactions annulées.\nComparé au 2PL on a quelque chose de plus performant mais aussi de plus prédictible, vu que les requêtes n'ont pas à attendre qu’une longue requête ait terminé. Et si on a une forte charge de lectures c’est parfait aussi puisqu’elles ne sont jamais bloquées.\nComparé à l’exécution vraiment en série, on n’est pas limité au CPU d’une seule machine, FoundationDB distribue la détection des conflits sur plusieurs machines.\nGlobalement, vu qu’une transaction peut vite voir ses prémisses invalidées par d’autres, pour qu’on n’ait pas beaucoup d’annulation de transactions, il faut que celles-ci soient assez courtes et rapides. Mais d’un autre côté, 2PL et l’exécution sériale ne font pas mieux avec les transactions longues.","8---the-trouble-with-distributed-systems#8 - The trouble with distributed systems":"Les fautes partielles :\nLe souci avec les systèmes distribués c’est qu’ils peuvent agir de manière non déterministe, et qu’une partie du système peut être en échec alors que le reste fonctionne. C’est une chose dont on n’a pas l’habitude dans un seul ordinateur.\nLes superordinateurs choisissent en général d’écrire des checkpoints en DD, et d’arrêter tout le système pour réparer le composant problématique en cas de panne, pour ensuite reprendre là où ça en était à partir du checkpoint.\nLes systèmes distribués de type “cloud” ou “web” sont à l’opposé :\nils sont trop gros pour tolérer d’éteindre à chaque panne, et ils ne peuvent de toute façon pas tolérer d’arrêter le service\nils utilisent du matériel bon marché pour scaler\nils sont répartis à travers le globe, utilisant le réseau internet qui est très peu fiable comparé à un réseau local.\n\n\n\n\nIl faut que la gestion des problèmes matériels fasse partie du design de notre système.\n\n\nLe réseau :\nLe réseau internet (IP) est construit de manière à être peu fiable de par sa nature asynchrone. Un paquet peut à tout moment être perdu, corrompu, mettre beaucoup plus de temps à arriver etc. pour diverses raisons, parce qu’il passe par des dizaines de nœuds divers et variés qui peuvent être surchargés, débranchés, mal configurés etc.\nOn a des protocoles comme TCP construits par dessus pour corriger ça et renvoyer les paquets perdus ou corrompus.\nQuand on envoie un paquet, on ne sait pas s’il a été reçu ou pas. Au mieux on peut demander au destinataire de répondre, mais s’il ne répond pas on ne sait pas ce qu’il s’est passé. Tout ce qu’on peut faire c’est avoir un timeout, et considérer l’échec après le timeout.\n\n\nC’est le cas d’internet qui est peu fiable, mais le réseau ethernet local est également asynchrone. Donc les messages échangés entre les ordinateurs d’un même datacenter sont aussi prompts aux corruptions et pertes.\nUne étude a trouvé qu’il y a 12 fautes réseau par mois dans un datacenter moyen.\nAjouter de la redondance ne règle pas autant de problèmes qu’on le croit puisqu’il y a aussi les erreurs humaines des ops qui sont nombreuses\n\n\nLa détection des machines en état de faute est difficile, mais il y a des moyens :\nSi le processus applicatif est mort mais que l’OS tourne, la machine répondra peut-être par un message TCP indiquant qu’elle refuse les connexions.\nDans le même cas, la machine peut aussi avertir les autres nœuds que son processus applicatif est mort. HBase fait ça.\nDans le cas spécifique d’un datacenter, on peut avoir accès aux switches réseaux pour avoir certaines informations sur l’état connu de certaines machines qui ne répondent plus depuis un certain temps.\nDe même avec les routeurs qui peuvent immédiatement répondre que telle ou telle machine est injoignable si on les interroge.\n\n\nLa question de la valeur du timeout est une question particulièrement épineuse et pas simple. Une des manières est de tester en environnement réel et d’ajuster en fonction des performances.\nCet ajustement peut être automatique, Akka et Cassandra font ça.\n\n\nLa congestion du réseau est souvent causée par des problèmes de queuing diverses :\nau niveau des switchs\nau niveau des machines si tous les CPU sont occupés\nTCP qui fait du queuing pour éviter la corruption de paquets, et qui retente l’envoie du paquet de manière transparente (ce qui prend du temps)\n\n\nNe pourrait-on pas rendre la communication fiable du point de vue matériel ?\nPour ce faire, il faudrait qu’elle soit synchrone. C’est le cas du réseau téléphonique à commutation de circuit, qui alloue une ligne permettant d’envoyer une quantité fixe de données de manière régulière. Les divers switch et autres éléments réseaux qui établissent cette communication allouent cette quantité pour que le transfert puisse se faire.\nDans le cas des communications autres que stream audio / vidéo, on ne sait pas à l’avance quelle quantité de données on voudra, ni quand on voudra faire le transfert. La commutation par paquets permet de ne rien envoyer quand il n’y a pas besoin, et d’envoyer des paquets de taille variable quand c’est nécessaire. Le prix c’est que le réseau n’est pas en train de nous allouer de la place en permanence, et qu’il y a du queuing.\nC’est donc bien un choix d’allocation dynamique et non pas de réservation statique des ressources réseaux qui fait qu’on utilise toutes nos ressources disponibles mais avec des délais variables. On fait ce genre de choix aussi pour l’allocation dynamique des CPU vis à vis des threads.\n\n\n\n\nLes clocks : les clocks des ordinateurs sont globalement peu fiables, et d’autant moins dans un contexte d’ordinateurs distribués.\nIl y a 2 types de clocks sur un ordinateur :\nLes time-of-the-day clocks : ils renvoient le temps courant, en général sous forme d’entier depuis l’epoch (1er janvier 1970).\nVu qu’ils sont synchronisés par NTP (network time protocol), on peut régulièrement avoir des sauts dans le temps, et donc pour mesurer des durées c’est pas le top.\n\n\nLes monotonic clocks : ils renvoient une valeur arbitraire, mais garantissent qu’après un certain temps, la valeur renvoyée sera l’ancienne + le temps écoulé\n\n\nA propos de la précision :\nGoogle suppose que les clocks de ses machines se décalent de l’équivalent de 17 secondes pour un clock resynchronisé une fois par jour.\nLe protocole de mise à jour des clocks NTP ne peut pas être plus précis que le temps de latence d’envoi/réception des messages (une expérimentation a montré un minimum de 35 ms pour une synchronisation via internet).. Et en cas de congestion du réseau c’est pire.\nDans les machines virtuelles le CPU est partagé, donc on peut se retrouver avec des sauts bizarres dans le clock à cause de ça.\nEn cas de besoin, on peut mettre en place des infrastructures de haute précision qui se mettent à jour par GPS, mais c’est coûteux. C’est ce qui est fait sur les machines de trading à haute fréquence.\nEn fait, il faudrait voir le clock plutôt comme un intervalle que comme un temps. Malheureusement la plupart des API ne le présentent pas comme ça.\nUne exception est constituée par l’API TrueTime de Google Spanner, qui renvoie un groupe de 2 valeurs : [earliest, latest].\nDans le cas particulier de Google, en partant du principe que les intervalles de confiance sont fiables, si deux intervalles pour deux requêtes ne se chevauchent pas, alors on est sûrs que la requête avec l’intervalle plus récent a eu lieu après l’autre. Google utilise ça pour faire de la snapshot isolation dans un environnement distribué, mais pour ça il équipe chaque datacenter d’une réception GPS ou d’une horloge atomique, sans quoi les intervalles seraient trop grands. En dehors de Google cette solution basée sur le temps n’est pour le moment pas viable.\n\n\n\n\n\n\nContrairement à un CPU ou une carte réseau, quand un clock est défectueux la machine peut quand même donner l’impression que tout va bien, et faire des erreurs qui se voient beaucoup plus difficilement.\nC’est en particulier problématique si on se sert des clocks pour faire des timestamps pour vérifier quelle transaction a eu lieu la 1ère dans un système distribué. Et c’est encore plus problématique avec du LWW (last write wins) : si on noeud a son clock qui retarde, tous ses messages finiront par être rejetés en faveur de ceux des autres nœuds parce que considérés comme anciens.\nPlutôt que les clocks physiques, il faut utiliser des clocks logiques, c'est-à-dire des techniques pour détecter l’ordre des choses plutôt que le moment où elles ont eu lieu.\n\n\nUn thread peut se mettre en pause pendant un temps indéterminé pour des raisons très variées : le garbage collector du langage, la machine virtuelle, l’OS qui a besoin de le mettre en pause pour faire autre chose etc. Dans un système distribué “shared nothing” il n’y a pas de mémoire partagée, donc il faut partir du principe qu’un nœud peut se retrouver arrêté pendant que le monde autour de lui aura continué.\nIl existe des systèmes appelés temps réel (real time, ou hard real time pour bien insister sur l’aspect contrainte de temps à respecter absolument). Ces systèmes sont pensés et testés sous tous les angles pour respecter un certain nombre de contraintes de temps de réponse. On les utilise principalement dans les machines où le temps est crucial (par exemple le déclenchement d’un airbag).\nPour le problème spécifique du garbage collector, certains systèmes demandent à leur nœud de prévenir quand il y a un besoin de garbage collection, et au besoin redirigent le trafic vers d’autres nœuds en attendant que ce soit fait. Ça permet de réduire pas mal les problèmes de pause non voulue de l’application.\n\n\n\n\nSavoir, vérité et mensonge :\nDans des conditions aussi difficiles que les systèmes distribués où on ne peut rien savoir de certain sauf à travers les messages qu’on reçoit ou ne reçoit pas, on peut quand même créer des systèmes qui fonctionnent : il est possible d’avoir quelque chose de fiable construit sur des bases offrant peu de garanties, à conditions que le modèle de système qu’on a choisi convienne.\nLa vérité dans un contexte distribué est déterminée par la majorité. Pour éviter la dépendance à un noeud particulier, et étant donné qu’un noeud, quel qu’il soit, ne peut pas faire confiance à sa propre horloge vu qu’il peut entrer en pause à tout moment sans le savoir, on décide de mettre en place des quorums pour qu’une majorité de noeuds décident par exemple si un noeud est mort ou non.\nIl faut bien s’assurer que and on noeud pense être doté d’une responsabilité (il est le leader, il a le lock sur un objet etc.), il se fie quand même à ce que disent la majorité des noeuds : s' ils lui disent qu’il n’a plus la responsabilité en question, alors il faut qu’il accepte de se comporter comme tel, sous peine d’inconsistances dans le système.\nPour garantir qu’un lock soit bien respecté, on peut utiliser un lock service qui fournit un token incrémental à chaque lock. Si le nœud a son temps alloué qui a expiré, et qu’il essaye d’écrire alors qu’un autre a déjà écrit à sa place, son token sera rejeté par le lock service. On parle de fencing token.\nZooKeeper permet de fournir ce genre de fencing token s’il est utilisé comme lock service.\n\n\n\n\nJusqu'ici on est parti du principe que les noeuds peuvent de plus répondre, ne pas savoir qu’ils n’ont plus une certaine responsabilité, ou échouer. Mais qu’ils restent “honnêtes” au sens où ils ne vont pas dire qu’ils ont reçu un message alors qu’ils ne l’ont pas reçu, ou encore falsifier un fencing token. De tels cas de corruption s’appellent une Byzantine fault.\nÇa vient du Byzantine Generals Problem où on imagine dans la ville antique de Byzance, des généraux de guerre essayent de se mettre d’accord, et communiquant par messager, mais où certains généraux mentent sans se faire découvrir.\nOn imagine donc que certains nœuds peuvent être complètement corrompus jusqu’à ne plus suivre le protocole attendu du tout, par exemple dans le cas d’un logiciel dans un contexte aérospatial soumis à des radiations.\nOu alors se mettent carrément à tricher intentionnellement, soit à cause d’un piratage, soit plus classiquement un contexte de communication inter-organisations, où les organisations ne se font pas confiance.\nC’est le cas par exemple pour la blockchain où les participants ne se font pas confiance puisque n’importe lequel pourrait essayer de tricher.\n\n\nDans notre cas habituel de serveurs web, on part du principe que le client final derrière son navigateur pourrait être malicieux, mais sinon les serveurs de l'organisation sont fiables. Et on ne met pas en place de mécanismes contre les problèmes de fautes byzantines, parce que c’est trop compliqué.\nLa plupart des algorithmes contre les fautes byzantines comptent sur le fait que la majorité des nœuds ne vont pas être infectés par le problème, et donc pourront garder le contrôle contre la minorité du réseau corrompue. Donc ça peut être utile dans un contexte d’application peer-to-peer, mais si on charge notre version du logiciel dans tous les nœuds, ça ne nous protégera pas des bugs. Et de même si un hacker prend le contrôle d’un nœud, on imagine qu’il pourra aussi prendre le contrôle des autres nœuds.\nOn peut néanmoins se prémunir contre des formes modérées de mensonges avec quelques astuces :\nFaire un checksum des paquets pour vérifier qu’ils n’aient pas été corrompus. TCP/UDP le font mais parfois laissent passer.\nVérifier la validité de toutes les données entrées par l’utilisateur.\nDans le cas de la mise à jour depuis des serveurs NTP, faire des requêtes auprès de plusieurs serveurs, pour que ceux qui n’ont pas une bonne valeur soient rejetés.\n\n\n\n\nNotre système doit prendre en compte les problèmes matériels qu’on a décrits, mais il ne doit pas non plus être complètement dépendant du matériel exact sur lequel il tourne pour pouvoir changer le matériel. On va donc créer une abstraction qui est le system model.\nConcernant les considérations liées au temps, on en a 3 :\nSynchronous model : on part du principe que les erreurs réseau, de clock ou les pauses de processus sont limités à certaines valeurs définies; En pratique la réalité ne colle pas à de modèle.\nPartially synchronous model : on part du principe que le système se comporte de manière synchrone la plupart du temps, sauf parfois où il déborde. Ce modèle correspond beaucoup mieux à nos systèmes web distribués.\nAsynchronous model : on est beaucoup plus restrictif puisqu’on considère qu’aucune notion de temps ne peut être fiable. Et donc on ne peut pas utiliser de timeouts non plus.\n\n\nConcernant les considérations liées aux échecs de noeuds, il y en a aussi 3 :\nCrash-stop faults : on considère que si un nœud fait une faute, on l’arrête et c’en est fini de lui.\nCrash-recovery faults : les nœuds peuvent être en faute, puis revenir en état correct un peu plus tard. C’est ce modèle qui nous est en général le plus utile pour nos systèmes web.\nByzantine (arbitrary) faults : les nœuds peuvent faire absolument n’importe quoi.\n\n\nPour définir qu’un algorithme d’un system model distribué est correct, il peut avoir deux types de propriétés :\nsafety : il s’agit d’une propriété qui dit que rien de mal ne doit se passer. Par exemple, la uniqueness de quelque chose. Si une telle propriété est rompue, c’est parce que chose a été violée et qu’il y a eu un dommage non réparable.\nliveness : il s’agit d’une propriété qui dit qu’une chose attendue doit arriver. Par exemple l’availability, le fait de recevoir une réponse. Si une telle propriété est rompue, c’est que ce qui était attendu n’a pas eu lieu, mais pourrait avoir lieu plus tard\n\n\nIl est courant de demander à ce que les caractéristiques de safety soient respectées dans tous les cas, même si tous les nœuds crashent, un mauvais résultat ne doit pas être retourné. Pour les caractéristiques de liveness, on peut demander à ce qu’elles soient respectées seulement dans certains cas, par exemple si un nombre suffisant de nœuds est encore en vie.\nEnfin, il faut bien garder en tête qu’un system model n’est qu’un modèle. Dans la réalité, on sera amené à rencontrer des erreurs non prévues. Et à l’inverse, sans raisonnement théorique, on pourrait avoir des erreurs dans nos systèmes pendant longtemps sans s’en rendre compte. Les deux sont aussi importants l’un que l’autre.\nC’est la différence entre le computer science (théorique), et le software engineering (pratique).","9---consistency-and-consensus#9 - Consistency and Consensus":"Pour rendre un système tolérant aux fautes, il faut introduire des abstractions. C'est ce qu’on a fait avec les transactions par exemple en partant du principe qu’une transaction est atomique. Une autre abstraction intéressante est le consensus : faire en sorte que les nœuds se mettent d’accord.\nLa consistency est une question importante à laquelle on peut apporter différents niveaux de garantie. Comme avec l’isolation où il s’agissait de traiter la concurrence entre deux transactions, avec la consistance il s’agit de coordonner l’état des réplicas vis-à-vis des délais (replication lag) et des fautes.\nLa linearizability consiste en une abstraction qui donne l’illusion que le replication lag n’existe pas, qu’il n’y a en fait qu’une seule copie des données : dès qu'une copie a été faite, le système doit se comporter comme si cette donnée la plus récente était lisible depuis partout.\nUne des conséquences c’est qu’il faut que quand une lecture a été faite avec une valeur, ce soit cette valeur qui soit retournée par tous les réplicas à partir de ce moment. Si une écriture a lieu entre temps ça peut être cette nouvelle valeur écrite, mais certainement pas une valeur plus ancienne.\nOn doit pouvoir éviter le cas où une personne recharge la page et voit que le match a été gagné par telle équipe, et juste après une autre personne affiche la page, et voit que le match est toujours en cours.\nEn revanche, il n’y a pas de contraintes de délais : si l’écriture prend du temps c’est pas grave. Et si deux transactions sont concurrentes et que l’une arrive avant l’autre c’est pas grave non plus.\n\n\nLinearizability vs serializability : la serializability est une notion d’isolation pour pouvoir garantir la manipulation de plusieurs objets au sein d’une même transaction, sans être gêné par les autres transactions. La linearizability consiste à renvoyer systématiquement le résultat le plus récent à chaque lecture une fois que celui-ci a été lu au moins une fois.\nLe 2-phase locking et l’actual serialization garantissent aussi la linearizability. En revanche la Serializable snapshot isolation ne la garantit pas puisqu’elle va créer des snapshots pour les transactions, et ne pas inclure les writes récents dans ces snapshots (ce qui peut facilement résulter à ce que certaines transactions aient un write et d’autres pas).\n\n\nParmi les applications de la linearizability :\nL’élection d’un nouveau leader est un problème où dès que le lock a été pris, il faut que personne d’autre ne puisse le prendre.\nApache ZooKeeper et etcd sont souvent utilisés comme système de lock pour implémenter l’élection de leader.\nApache Curator ajoute des choses par-dessus ZooKeeper.\n\n\n\n\nLe fait de garantir qu’un nom d’utilisateur ne sera pas pris deux fois, ou encore qu’un compte en banque ne va pas en dessous de 0.\nDans le cas où on a 2 canaux de communication, l’un des canaux peut être plus rapide que l’autre : par exemple si on écrit une image, et qu’on enqueue un message pour qu’une version thumbnail de l’image soit générée. Si le traitement du message dans la queue est plus rapide que le temps qu’on met à écrire l’image entière, la thumbnail risque d’être faite à partir d’un fichier partiel. Il faut donc s’assurer de l’ordre de ce qui est fait dans ces 2 canaux.\n\n\nLa linearizability parmi les systèmes de réplication connus :\nSingle-leader replication : ça pourrait être linearizable si la BDD n’utilise pas de snapshot isolation. Mais il reste le problème de savoir qui est le leader, et dans le cas de réplication asynchrone on peut perdre des données au failover.\nConsensus algorithms : ces algorithmes permettent d’implémenter la linearizability en répondant aux problèmes soulevés dans la single-based replication. On va y revenir.\nC’est comme ça que fonctionnent ZooKeeper et etcd.\n\n\nMulti-leader replication : ces systèmes ne sont pas linearizable puisqu’il y a des écritures concurrentes qui sont résolues après coup.\nLeaderless replication : certains affirment qu’en respectant la règle du quorum consistency, on peut obtenir une linearizability sur des BDD Dynamo-style. Mais ce n’est en général pas vrai.\nRiak ne fait pas de read repair à cause du manque de performance de cette technique, et il la faudrait pour la linearizability.\nCassandra fait le read repair, mais il perd la linearizability à cause de son algo last write wins qui cause des pertes de données.\n\n\n\n\nSi on part de l’exemple de la multi-leader replication, on constate que c’est pratique parce que si la connexion est rompue entre deux datacenters, les deux peuvent continuer indépendamment, et se resynchroniser dès que la connexion est rétablie. On a alors une grande availability du système, mais on ne respectera pas la linearizability. A l’inverse si on reste en single-leader, le datacenter déconnecté du datacenter leader se verra inopérant jusqu’à rétablissement du réseau. Mais on garde la linearizability.\nLe CAP theorem décrit cette problématique et a permis en son temps d’ouvrir la discussion, mais il est fondamentalement inutile de nos jours.\n\n\nDans la pratique, de nombreuses BDD n’implémentent pas la linearizability parce que ça coûte trop cher en performance. Il n’y a malheureusement pas d’algorithme qui permette d’avoir de la linearizability sans ce problème de performance qui est d’autant plus grand qu’il y a beaucoup de délais dans le réseau.\n\n\nGaranties d’ordre d’exécution :\nLa causal consistency (causalité) est au cœur des problématiques des systèmes distribués faisant fonctionner des applications qui ont du sens.\nRespecter la causalité n’implique pas forcément un total order (ordonnancement total) de tous les éléments, mais uniquement de ceux liés entre eux par une relation cause / conséquence.\nLa linearizability quant à elle, implique un total order. Elle est donc une contrainte plus forte que la causal consistency.\nC’est trop récent à l’époque du livre pour être dans des systèmes en production, mais il y a de la recherche sur des techniques permettant de détecter la causalité sans total order. Par exemple une généralisation des version vectors.\n\n\n\n\nLa causal consistency coûte quand même cher s’il faut traquer toutes les transactions et leurs relations. On peut sinon utiliser des sequence numbers pour créer un clock logique permettant de définir un ordre total.\nSur une configuration single-leader, il suffit d’incrémenter un compteur à chaque opération au niveau du leader.\nDans le cas où il y a plusieurs leaders, on a d’autres solutions:\nGénérer des sequence numbers différents pour chaque nœud (par exemple pair pour l’un, impairs pour l’autre).\nUtiliser un clock physique.\nAllouer des plages à chaque nœud, par exemple 0 à 1000 pour l’un, 1000 à 2000 pour l’autre etc.\nMalheureusement certains nœuds peuvent aller plus vite que d’autres, et ces techniques ne garantissent pas la causalité dans le système.\n\n\nLa causalité peut être assurée dans un environnement multi-leader grâce aux Lamport timestamps. Il s’agit d’une idée de Leslie Lamport dans un des papiers les plus cités des systèmes distribués.\nLe principe est d’avoir un compteur normal par nœud, et pour le rendre unique on l’associe à un chiffre représentant le nœud lui-même. Et l’astuce de la technique consiste à ce que chaque nœud et chaque client garde en mémoire la valeur la plus élevée de compteur qu’il connaisse. Et quand il a connaissance de la valeur d’un autre compteur plus élevé que celui qu’il connaissait au détour d’une opération, il met immédiatement à jour le compteur du nœud sur lequel il fera la prochaine opération avec cette valeur-là.\nCette technique permet de respecter la causalité, mais aussi un total ordering.\nMalheureusement ça ne règle pas tous nos problèmes : même avec un total order, on ne peut pas savoir sur le moment si un nom d’utilisateur unique est en passe d’être pris par un autre nœud ou non pour savoir sur le moment s’il faut l’autoriser soi-même ou non. Avec le temps et les opérations, on finira par avoir un ordonnancement total, mais pour le moment non.\nC’est l’objet du total order broadcast.\n\n\n\n\n\n\nLe total order broadcast nécessite qu’aucun message ne soit perdu, et que tous les messages soient délivrés à tous les nœuds dans le même ordre.\nLa connexion peut être interrompue, mais les algorithmes de total order broadcast doivent réessayer et rétablir l’ordre des messages dans tous les nœuds quand le réseau est rétabli.\nZooKeeper et etcd implémentent le total order broadcast.\nA noter aussi que le total order broadcast maintient l’ordre tel qu’il est au moment de l’émission des messages, donc c’est plus fort que le timestamp ordering.\nOn peut voir ça comme un log de messages transmis à tous les nœuds dans le bon ordre.\nOn peut ainsi implémenter la linearizability à partir d’un système respectant le total order broadcast.\nPour l’écriture :\nOn ajoute un message au log disant qu’on voudrait écrire\nOn lit le log et on attend que notre message nous parvienne\nSi le premier message concernant ce sur quoi on voulait écrire est le nôtre, alors on peut valider l’écriture dans le log.\n\n\nPour la lecture :\nOn peut faire pareil qu’avec l’écriture : ajouter un message indiquant qu’on veut lire, attendre de le recevoir, puis faire la lecture en fonction de l’ordre indiqué dans le log.\nC’est comme ça que ça marche dans les quorum reads, dans etcd.\n\n\nOn peut demander à avoir tous les messages de log liés à une lecture puis faire la lecture à partir de là.\nC’est comme ça que fonctionne la fonction sync() de ZooKeeper.\n\n\nOn peut lire à partir d’un réplica synchrone avec le leader (en cas de single-leader), dont on est sûr qu’il a les données les plus récentes.\n\n\n\n\nA l’inverse, on peut aussi implémenter un système total order broadcast à partir d’un système linéarisable : il suffit d’avoir un compteur linéarisable qu’on attache à chaque message envoyé via total order broadcast.\n\n\nOn peut enfin noter qu’à la fois la linearizability et le total order broadcast sont tous deux équivalents au consensus.\n\n\nLe consensus consiste en la possibilité pour les nœuds de se mettre d’accord sur quelque chose (on pense par exemple à l’élection de leader, ou à l’atomic commit problem où il faut choisir entre garder ou non une transaction présente sur certains nœuds), alors même que des noeuds peuvent être en faute à tout moment.\nC’est un sujet très subtil et complexe.\nLe FLP result est un résultat théorique montrant que le consensus est impossible dans un system model asynchrone. Dans la pratique, à l’aide de timeouts (même s’ils peuvent être parfois faussement positifs), on arrive à atteindre le consensus.\nQuand une transaction est écrite en BDD, il est hors de question de la retirer par la suite parce qu’elle a pu être prise en compte par d’autres transactions. Il faut donc bien réfléchir avant d’entrer définitivement l’écriture en BDD.\nLe two-phase commit (2PC) est un algorithme de consensus implémenté dans certaines BDD.\n2PC n’est pas très bon, des algorithmes plus modernes existent chez ZooKeeper (Zab) et etcd (Raft).\nAttention à ne pas confondre 2PC avec 2PL (2 phase lock) qui permet l’isolation pour la sérialisation, le mieux est d’ignorer le rapprochement de leur nom.\nFonctionnement :\non a besoin d’un nouveau composant : le coordinator.\nLors d’une transaction, après les lectures / écritures, quand on veut inscrire vraiment tout ça en BDD, le coordinator va procéder en 2 étapes :\ndemander successivement à chaque nœud si il est prêt à faire un commit et attendre leur réponse.\nsi oui, faire le commit, sinon annuler la transaction.\n\n\nL’idée c’est que lors de la 1ère phase, quand le coordinator demande si les nœuds sont prêts, en fait il leur demande aussi de tout préparer pour que même en cas de crash rien ne soit perdu de leur côté. La seule chose qui leur resterait à faire alors serait de valider les données déjà mises en forme pour aller dans la BDD.\nLorsque le coordinator prend sa décision finale de faire s’exécuter ou d’annuler la transaction en phase 2, alors il l’écrit localement et passe un point de non-retour. A partir de là il réessayera en permanence de faire finaliser la transaction auprès de tout nœud qui deviendrait indisponible à partir de ce moment-là.\nDans le cas où le coordinator crash juste après avoir demandé aux noeuds de se préparer fait que les noeuds doivent rester en attente. Ils ne peuvent pas unilatéralement prendre de décision de valider ou annuler une transaction chacun de leur côté. La solution est d’attendre que le coordinator revienne, lise ce qu’il avait décidé sur son fichier de log, et envoie les messages qui conviennent.\nIl existe un autre algorithme appelé 3-phase commit (3PC) qui résout le problème de l'aspect bloquant lié à l’attente du commit du coordinator, mais il implique des temps de réseaux bornés. Or nos réseaux habituels sont imprévisibles. Pour cette raison, c'est le 2PC qui continue d’être utilisé.\nLe souci de ce cas c’est surtout le lock au niveau de la BDD, souvient sur les entrées concernées par la transaction. Si le coordinator ne revient jamais ou que les logs sont perdus, alors on peut se retrouver face à des locks orphelins, et un administrateur humain devra manuellement résoudre ces conflits, puisque les locks sont censés survivre même à un redémarrage de la BDD.\n\n\n\n\n\n\nEn pratique, les transactions distribuées sont souvent décriées parce qu’elles coûteraient trop par rapport à ce qu’elles apporteraient.\nLes transactions distribuées utilisant MySQL sont connues pour être 10 fois plus lentes que les mêmes transactions sur un seul nœud.\nIl y a deux types de transactions distribuées : celles qui sont implémentées par une même BDD qui tourne sur plusieurs nœuds, et celles qui consistent à faire communiquer des technologies hétérogènes d’un nœud à un autre. Les dernières sont bien plus compliquées.\nLes transactions hétérogènes peuvent faire communiquer par exemple une BDD et un message broker, et ne commiter que si tout a marché, et annuler tout sinon.\nXA (eXtended Architecture) est justement un protocole qui permet d’implémenter le 2PC dans des technologies hétérogènes. Il s’agit d’une API en C qui se connecte aux programmes qui s’exécutent sur une machine.\nIl est supporté par de nombreuses BDD : PostgreSQL, MySQL, DB2, SQL Server, Oracle.\nEt par plein de message brokers : ActiveMQ, HornetQ, MSMQ, IBM MQ.\nXA a cependant des limitations, par exemple il ne permet pas de détecter les deadlocks, et ne supporte pas le SSI (serializable snapshot isolation).\n\n\n\n\nIl faut noter quand même que le coordinateur est souvent un single point of failure vu qu’il contient lui-même des données persistantes cruciales pour le fonctionnement du système. Mais étonnamment les possibilités de le rendre réplicable sont en général rudimentaires.\n2PC a quand même un point problématique aussi, c’est qu’il a tendance à amplifier les failures, puisque dès qu’un nœud ne répond pas on va annuler la transaction. C’est pas très “fault tolerant” tout ça.\n\n\nLe consensus tolérant les fautes :\nOn peut citer 4 propriétés définissant le consensus :\n3 de safety :\nUniform agreement : tous les nœuds doivent arriver au même choix.\nIntegrity : aucun nœud ne décide deux fois.\nValidity : le choix décidé est valide.\n\n\nEt une de liveness :\nTermination : les nœuds ne se retrouvent pas bloqués, même en cas de crash de certains d’entre eux. Ils évoluent vers la terminaison du processus de choix.\n2PC ne remplit pas cette condition puisque le coordinator peut bloquer le système en cas de faute.\n\n\n\n\n\n\nLes algorithmes de consensus tolérants aux fautes sont difficiles à implémenter (donc on ne va pas les implémenter nous-mêmes mais utiliser des outils qui les implémentent).\nCe sont les suivants :\nViewstamped replication (VSR)\nPaxos\nRaft\nZab\n\n\nCes algorithmes sont de type total order broadcast.\nA chaque tour les nœuds décident du prochain message à traiter, et décident par consensus.\nPour le remplacement des leaders les nœuds utilisent des timeouts, et lancent une élection avec un quorum. Et c’est seulement quand le nœud a bien reçu le message de la majorité qu’il sait qu’il est bien le leader.\nCes algorithmes sont encore un sujet de recherche, et ont parfois des edge cases problématiques qui basculent le leader entre 2 nœuds, ou qui forcent en permanence le leader à renoncer.\n\n\n\n\nServices de coordination :\nLes services comme ZooKeeper sont rarement utilisés directement par les développeurs. On va plutôt les utiliser à travers d’autres services comme HBase, Hadoop YARN, OpenStack Nova et Kafka.\nCe sont en gros des stores de clé-valeur qui tiennent en RAM.\nZooKeeper a notamment ces caractéristiques :\nLinearizable atomic operations : à l’aide d’un lock, une seule opération parmi les opérations concurrentes peut réussir.\nTotal ordering of operations : les fencing tokens permettent de préserver l’ordre des transactions.\nFailure detection : les nœuds ZooKeeper et les autres nœuds s’envoient des messages régulièrement, et en cas de timeout déclarent le nœud échoué.\nChange notifications : les clients (les autres nœuds) peuvent s’abonner à des changements spécifiques des autres nœuds à travers ZooKeeper, ce qui évite de faire des requêtes pour voir où ça en est.\n\n\nZooKeeper est pratique pour des informations qui changent toutes les minutes ou heures comme l’association d’une adresse ip à un leader.\nSi on veut répliquer l’état d’une application qui peut nécessiter des milliers ou millions de changements par seconde, on peut utiliser des outils comme Apache BookKeeper.\n\n\nZooKeeper fait partie des membership services, issu d’une longue recherche depuis les années 80. En couplant le consensus avec la détection de fautes, ils permettent d’arriver à une certaine connaissance de qui composent les membres du réseau.\n\n\n\n\nIntégrer des systèmes disparates ensemble est l’une des choses les plus importantes à faire dans une application non triviale.\nLes données sont souvent classées en 2 catégories qu’il est de bon ton d'expliciter :\nLes systems of record qui sont les données de référence.\nLes derived data systems qui sont en général des données dénormalisées, par exemple stockées dans un cache.","10---batch-processing#10 - Batch Processing":"Il existe 3 types de systèmes :\nServices (online systems) : un client envoie un message et reçoit une réponse. En général, le temps de réponse et la disponibilité (availability) sont très importants.\nBatch processing systems (offline systems) : des tâches de fond, souvent exécutées périodiquement, durant plusieurs minutes voire plusieurs jours. La performance se mesure par la quantité de données traitées.\nStream processing systems (near-real-time systems) : il s’agit d’une forme particulière de batch processing. On ne répond pas à une requête d’un client humain, mais on réagit à un événement assez rapidement après qu’il ait eu lieu.\n\n\nLe batch processing avec les outils Unix :\nL’outil sort d’Unix va automatiquement prendre en charge des données plus grandes qu’il n’y a de mémoire vive, mettre ça en disque pour faire les opérations, et paralléliser au niveau CPU.\nLa philosophie unix est très proche de l’agile et du devops. On casse les gros problèmes en petits, on fait de petits programmes qui font une chose et la font bien. On fait des itérations courtes.\nUne des clés de la puissance des outils unix est l’interface uniforme, permettant de les composer ensemble. De nos jours c’est plutôt l’exception que la norme parmi les programmes.\nOn a également une séparation entre la logique et le câblage des données grâce à stdin et stdout.\nLes outils unix sont très pratiques pour l’expérimentation : les entrées sont immuables, et on peut envoyer la sortie vers un less par exemple.\nMais le plus souci c’est que les outils unix ne marchent que sur une machine, pas sur des architectures distribuées.\n\n\nMapReduce est un modèle assez bas niveau de batch processing, connu pour être l’algorithme qui permet à Google d’être aussi scalable.\nIl ressemble aux outils unix mais sur des architectures distribuées.\nIl prend des inputs, et envoie le résultat dans des outputs.\nLes inputs ne sont normalement pas modifiés et il n’y a pas de side-effects autre que les outputs.\nLes fichiers d’output sont écrits de manière séquentielle.\nAlors que les outils unix écrivent dans stdout, MapReduce écrit dans un système de fichiers distribués.\nHadoop utilise HDFS (Hadoop distributed File System), qui est une implémentation open source de Google File System.\nIl en existe d’autres comme GlusterFS, Quantcast File System (QFS).\nD’autres services sont similaires : Amazon S3, Azure Blob Storage, OpenStack Shift.\nFonctionnement de HDFS :\nHDFS est basé sur une approche shared-nothing, c'est-à-dire qu’il lui suffit d’ordinateurs connectés par un réseau ip classique.\nUn démon tourne sur chaque nœud et expose les fichiers qui sont sur ce nœud. Et un serveur central appelé NameNode contient des références vers ces fichiers.\nIl y a de la réplication entre les nœuds.\nDe cette manière HDFS est capable de faire fonctionner des dizaines de milliers de machines et des petabytes de données.\n\n\n\n\nLe fonctionnement se fait en 4 étapes :\n1- On lit des fichiers et on les structure sous forme d’entrées.\nC’est le parser qui s’en charge.\n\n\n2- on appelle la fonction mapper pour extraire des clés-valeurs\nIl s’agit ici d’une fonction où on peut ajouter du code à nous. La fonction est appelée une fois par entrée et permet d’extraire de la manière souhaitée les clés-valeurs.\n\n\n3- on trie les clés-valeurs par clés\nC’est fait automatiquement.\n\n\n4- on appelle la fonction reducer pour faire notre action sur les clés-valeurs\nLà encore on peut ajouter du code à nous. On a en paramètre toutes les valeurs associées à une clé et on peut en faire ce qu’on veut en sortie.\n\n\n\n\nOn peut aussi enchaîner plusieurs MapReduce, l’un préparant les données en entrée pour l’autre.\nMapReduce permet aussi de paralléliser les opérations de manière transparente pour le code. Comme il y a de nombreuses entrées à traiter, chacune peut s’exécuter localement sur la machine du réplica où elle est. Cela permet aussi d’éviter les transferts réseau en localisant les calculs.\nConcernant le code custom des fonctions mapper et reducer, dans Hadoop elles sont écrites en Java, alors que dans MongoDB et CouchDB elles sont écrites en Javascript.\nContrairement aux outils Unix, MapReduce ne permet pas de chaîner directement ses jobs. Il faut plutôt écrire le résultat d’un job dans un dossier, puis donner ce dossier comme entrée au MapReduce suivant. C’est du moins comme ça que ça se passe dans Hadoop.\nDu coup tout un tas d’outils permettent de coordonner les jobs MapReduce dans Hadoop : Oozie, Azkaban, Luigi, Airflow, Pinball.\nD’autres outils haut niveau autour de Hadoop permettent également de gérer ce genre de choses : Pig, Hive, Cascading, Crunch, FlumeJava.\n\n\nA propos des reduce-side joins avec MapReduce :\nOn ne va envisager les jointures que sur des tables entières pour notre cas qui concerne les batchs, typiquement quand on traite des BDD destinées à l’analyse des données.\nPar exemple : si on a d’un côté des événements avec un user id, et de l’autre côté la table des users avec certaines de leurs caractéristiques. On va vouloir corréler les deux pour ne sélectionner que les faits d’un certain type d’utilisateurs.\nPour des raisons de performance, on va opter pour le plus de localité possible, et donc on ne va pas faire des accès random en traitant chaque entrée une par une là où elle est. On va plutôt copier la table des users dans le même filesystem HDFS que la table des faits, puis on va lire les deux conjointement.\nLa technique des sort-merge joins permet à plusieurs mappers de trier des données par la même clé (par exemple l’id de l’utilisateur pour des événements dont il est l’objet, et pour des données personnelles sur l’utilisateur), puis à un reducer de récupérer ces données et de les merger ensemble pour faire l’action qu’on voulait avec cette jointure.\nUne fois que les mappers ont fait leur travail, chaque clé agit comme une adresse au sens où les valeurs d’une même clé vont être envoyées au même nœud pour que le reducer soit exécuté avec ces valeurs-là. Il y a bien une localité des données pour l’exécution du traitement.\nD’une certaine manière on a séparé l’obtention des données du traitement des données, ce qui contraste avec la plupart des applications où on fait des requêtes en BDD en plein milieu du code.\n\n\nDans certains cas on peut se retrouver avec des hot keys par exemple des données liées aux followers de célébrités. Ceci peut donner trop de charge à un nœud de reducer, et les autres devront alors l’attendre pour que l’opération de MapReduce soit terminée.\nPour éviter ça on va détecter les hot keys et les traiter différemment des autres clés. On va les séparer dans plusieurs reducers différents sur plusieurs nœuds, et ensuite on fusionnera le résultat final.\nPig fait d’abord une opération pour déterminer les hot keys, et ensuite fait le traitement de la manière décrite.\nCrunch a besoin qu’on lui dise explicitement les hot keys.\nHive a aussi besoin que les hot keys soient spécifiés explicitement dans une table de metadata séparée.\n\n\n\n\n\n\nA propos des map-side joins :\nLes reduce-side joins sont pratiques parce que les mappers lisent les données quelles qu’elles soient, préparent et trient et donnent ça au réducers. Mais tout ceci coûte en terme de copies au niveau du DD.\nSi nous avons des informations sur la structure des données, nous pouvons faire des map-side joins, où il s’agit simplement de tout faire dans les mappers et se débarrasser des reducers.\n\n\nUn cas où c’est utile est quand on doit faire une jointure entre un grand dataset et un petit dataset, suffisamment petit pour que ça puisse être chargé dans la RAM de chaque mapper. Chaque mapper aura alors à disposition l’ensemble du petit dataset pour chercher les entrées qui l’intéressent.\nOn appelle cet algorithme le broadcast hash join.\nCette méthode est supportée par Pig, Hive, Cascading et Crunch, ainsi que la data warehouse Impala.\n\n\nDans le cas où on a deux tables partitionnées de la même manière et qu’on veut faire une jointure dessus, on peut faire le map-side join sur chacune des partitions, ce qui permet de ne charger en mémoire qu’une faible quantité de données.\nSi les deux datasets sont en plus triés selon la même clé, alors on n’a pas besoin que l’une des deux entre en mémoire. Les mappers pourront chercher les données qui les intéressent du fait qu’elles sont triées de la même manière.\nLe choix d’un map-side join ou d’un reduce-side join a un impact sur les données résultant du MapReduce : avec le map-side les données seront partitionnées de la même manière qu’elles l’étaient à l’input, alors qu’avec le reduce-side, les données seront partitionnées selon la clé de la jointure.\n\n\nLe batch se rapprocherait plus des OLAP que des OLTP dans la mesure où il scanne une grande quantité de données, mais le résultat d’un batch sera une forme de structure et non pas un rapport à destination de data analysts.\nUn exemple de batch est l’utilisation initiale de MapReduce par Google pour faire des indexes pour son moteur de recherche. Encore aujourd’hui MapReduce est un bon moyen de créer des indexes pour Lucene/Solr.\nUn autre exemple est de construire des BDD key-value pour du machine learning, ou pour des systèmes de recommandation.\nOn pourrait penser que la bonne solution serait d’orienter la sortie du MapReduce vers notre BDD, entrée par entrée, mais c’est une mauvaise idée, à la fois pour des raisons de performance (localité des données, pas d’utilisation réseau, parallélisation des tâches) et d’atomicité du batch job.\nLa bonne solution consiste plutôt à créer une toute nouvelle BDD sur le filesystem distribué, de la même manière qu’on crée le fichier d’index.\nPlusieurs systèmes de BDD supportent le fait de créer des fichiers de BDD à partir d’opérations MapReduce : Voldemort, Terrapin, ElephantDB, HBase.\nCes fichiers sont écrits une fois et demeurent ensuite read-only.\nLes systèmes de BDD qui les supportent vont servir les anciennes données, commencer à copier ce fichier depuis le filesystem distribué vers le disque local, et dès que c’est fait switcher vers le fait de servir ces données-là.\n\n\n\n\n\n\nMapReduce suit la philosophie Unix :\nOn peut rejouer une opération MapReduce autant de fois qu’on veut sans dommages pour les données d’entrée.\nSi les données sont corrompues pour une raison éphémère, on retente l’opération.\nSi c’est un bug logiciel, on le résout, et on refait la même opération encore.\n\n\nOn a une séparation de la logique, et du câblage pour décider où vont les données.\nPar contre là où les outils unix font beaucoup de parsing parce que le format est le texte, Hadoop et compagnie peuvent utiliser Avro et Parquet pour permettre une évolutivité des schémas.\n\n\nComparaison entre Hadoop et les BDD distribuées :\nLes BDD distribuées implémentant le massively parallel processing (MPP) avaient déjà la capacité de faire des jointures distribuées en parallèle depuis 10 ans quand MapReduce est sorti. La différence c’est qu’elles obligent les données à respecter un schéma prédéfini.\nPar contraste, le modèle MapReduce a permis de collecter n’importe quelles données, y compris du texte, des images etc. et de les mettre tels quels, transférant alors le problème de l’interprétation de ces données au consommateur.\nÇa s'appelle le sushi principle : raw data is better. Et ça permet par exemple de consommer la même donnée différemment selon les contextes.\nOn peut par exemple collecter les données, et dans une étape séparée utiliser un MapReduce pour réorganiser ces données de manière à les transformer en data warehouse.\n\n\nLes BDD MPP sont efficaces pour le cas d’utilisation qu’elles prévoient : la manipulation des données via des requêtes SQL. En outre, ça fournit un bon niveau d’abstraction pour ne pas avoir à écrire de code.\nD’un autre côté, tout ne peut pas être traité avec des requêtes SQL. Si on a des utilisations particulières comme du machine learning, des systèmes de recommandation, de recherche dans du texte etc. alors on a probablement besoin d’exécuter du code custom sur ces données. C’est ce que permet MapReduce.\nSi on a MapReduce, on peut construire un modèle SQL par dessus. C’est ce qu’a fait Hive.\n\n\nLa versatilité permise par les raw data dans du Hadoop permettent d'implémenter du SQL, du MapReduce, mais aussi d’autres modèles encore.\nOn a des BDD OLTP comme HBase\nOn a des BDD analytiques comme Impala\nLes deux utilisent HDFS mais pas MapReduce.\n\n\nDeux autres différences :\nLa manière de gérer les fautes n’est pas la même : les systèmes MPP annulent la requête en cas de faute, alors que MapReduce va annuler une partie du job, peut être le mapper ou le reducer, et réessayer pour le terminer.\nLa gestion de la mémoire n’est pas la même : les systèmes MPP vont avoir tendance à stocker beaucoup en mémoire vive, alors que MapReduce va plutôt écrire sur disque dès que possible.\nCeci est en partie dû au fait que MapReduce a été fait par Google dans un contexte où les jobs de grande priorité et de faible priorité tournent sur les mêmes machines. En moyenne un job batch a 5% de chances d’être arrêté parce que ses ressources sont préemptées par un processus plus prioritaire. C’est aussi pour cette raison qu’on écrit sur disque dès que possible et qu’on tolère beaucoup les fautes.\nSans ce genre de contraintes de préemption, MapReduce pourrait se révéler moins pertinent dans sa manière de fonctionner.\n\n\n\n\n\n\n\n\nMalgré le succès de MapReduce dans les années 2000, il y a d’autres modèles intéressants.\nMapReduce, bien que simple à comprendre, n’est pas simple à mettre en œuvre. Par exemple, le moindre algorithme de jointure a besoin d’être refait from scratch.\nIl existe un ensemble d’outils construits par-dessus MapReduce, et qui fournissent d’autres abstractions (Pig, Hive, Cascading, Crunch).\nIl existe aussi des modèles complètement différents de MapReduce, et qui permettent d’obtenir de bien meilleures performances pour certaines tâches.\nContrairement aux programmes Unix, MapReduce fait de la matérialisation des états intermédiaires, c'est-à-dire que la sortie d’un MapReduce doit être complètement écrite avant de pouvoir être consommée par un autre processus. A contrario les programmes Unix mettent en place un buffer sous le forme du pipe qui permet au programme suivant de démarrer en consommant la sortie du précédent bout par bout au fur et à mesure.\nCeci a plusieurs désavantages :\nLe fait de devoir attendre qu’un job MapReduce soit complètement terminé avant d’entamer le suivant est source de lenteur.\nSouvent, le mapper ne sert qu’à lire le code déjà formaté correctement et est donc inutile. On pourrait alors chaîner plusieurs reducers.\nLe fait que les états intermédiaires matérialisés soient sur le filesytem distribué veut dire qu’ils sont aussi répliqués, ce qui est plutôt overkill pour l’usage qu’on en fait;\n\n\nPour répondre à ces problèmes, des dataflow engines ont été développés.\nParmi les plus connus il y a Spark, Tez et Flink.\nTez est relativement petit, alors que Spark et Flink sont des frameworks plus gros, avec leurs propres couches réseau, scheduler, API.\n\n\nIls permettent :\nde ne pas nécessairement faire l’étape de tri, ce qui permet de faire des économies quand l’ordre des entrées n’importe pas.\nde chaîner les operators (qui remplacent les mappers et reducers) dans l’ordre souhaité, ce qui permet aussi d’éviter les mappers inutiles.\ndes optimisations locales, sans faire appel au réseau, et sans écrire dans le filesystem distribué HDFS quand ce n’est pas nécessaire. On ne matérialise donc pas forcément les états intermédiaires.\nde commencer la prochaine opération dès que des données sont disponibles, et sans attendre que la précédente soit terminée.\n\n\nOn peut les utiliser pour faire les mêmes opérations qu’avec MapReduce, et comme les operators sont une généralisation des mappers et reducers, on peut switcher de MapReduce vers Spark ou Tez dans Pig, Hive ou Cascading.\n\n\nAlors qu’avec MapReduce on avait une bonne tolérance aux fautes, avec Spark, Flink et Tez on doit trouver d’autres astuces :\nSi la machine qui faisait le calcul est perdue, on trouve d’autres données qui permettent de reconstruire la donnée perdue : la liste des opérations appliquées, et un état précédent, ou au pire la donnée originale qui est sur HDFS.\nConcernant le problème du déterminisme, si une opération était non déterministe et que la donnée a été transmise à un autre acteur alors qu’on a une faute, alors il faut tuer l’acteur en question. Et de manière générale il faut éviter les opérations non déterministes.\n\n\n\n\nOn peut également utiliser les batch pour des données sous forme de graphs.\nPageRank est un exemple connu de système sous forme de graph.\nPour les parcourir et y faire des opérations, un MapReduce ne suffit pas puisqu’il ne peut faire qu’une lecture/écriture. Mais on peut répéter ce genre d’opérations sous forme itérative, tant qu’on n’a pas atteint le but recherché.\nCependant MapReduce n’est pas très efficace pour itérer plusieurs fois avec de petits changements.\nOn a alors un modèle appelé bulk synchronous parallel (BSP), aussi connu sous le nom de Pregel model, popularisé par un papier de Google.\nIl est implémenté par Apache Giraph, Spark GraphX API, Flink Gelly API.\nC’est la même chose qu’avec MapReduce sauf que les données sont conservées en mémoire, et en cas de faible changement, il n’y a que peu de choses à recréer.\nIl est résistant aux fautes, en vérifiant l’état de tous les vertices régulièrement, et en l’écrivant sur disque dur.\nLe calcul est parallélisé, et ça cause beaucoup de communication réseau. Dans la mesure du possible, si les données peuvent tenir en RAM sur un seul nœud, ou même sur son DD, il vaut mieux tenter l’approche non distribuée qui sera plus rapide, sinon le Pregel model est inévitable.\n\n\n\n\n\n\nA mesure que le temps passe, des couches sont construites par dessus MapReduce, permettant d’avoir des abstractions.\nLes jointures peuvent ainsi être faites par des opérateurs relationnels, permettant à l’outil de décider de la manière de l’implémenter. C’est supporté par Hive, Spark et Flink.\nGrâce à ces diverses abstractions, les batch processings se rapprochent des BDD distribuées d’un point de vue performance, tout en permettant quand c’est nécessaire, d’exécuter du code arbitrairement pour plus de flexibilité.","11---stream-processing#11 - Stream Processing":"L’idée derrière les streams c’est de faire la même chose que les batchs, mais de manière beaucoup plus récurrente, et jusqu'à la plus petite unité possible : plutôt que de faire le traitement une fois par jour on fait le traitement dès qu’on a des données nouvelles.\nLes données dans le stream processing sont des events. Ils sont mis à disposition par un producer, à destination de consumers. Ils sont groupés dans un stream d’events.\nComment transmettre les event streams :\nOn peut imaginer un mécanisme de polling où le producer met à disposition et les consumers vérifient régulièrement s’il n’y a pas de nouveaux events. Mais ça fait beaucoup de messages à envoyer si on veut être réactif. Il vaut mieux que les consumers soient notifiés à chaque event.\nEn général les BDD supportent mal cette technique. On a bien les triggers qui permettent d’exécuter du code à chaque requête, mais ça reste assez limité. Les BDD ne sont pas conçues pour ça.\n\n\nLa bonne solution est d’utiliser un messaging system.\nPour différencier ces systèmes, il faut regarder deux points :\nQue se passe-t-il si le producer crée plus d’events que les consumers ne peuvent consommer ?\nSoit les consumers sautent ces messages.\nSoit les messages sont mis dans un buffer qui grossit, et dans ce cas que se passe-t-il si ça continue de grossir jusqu’à dépasser la RAM ?\nSoit les consumers empêchent le producer de produire tant qu’ils n’ont pas fini les events déjà produits.\n\n\nQue se passe-t-il si le système est down ou que des nœuds crashent ? est-ce qu’on perd des events, ou est-ce qu’ils sont persistés / dupliqués ?\n\n\nUne première possibilité est la communication directe entre producer et consumers :\nDes librairies de messaging brokerless comme ZeroMQ et nanomsg utilisent TCP/IP pour communiquer.\nUDP multicast est un protocole qui permet d’envoyer des events sans garantie de réception.\nStatsD et Brubeck utilisent UDP pour envoyer des métriques en tolérant des pertes.\nLe consumer peut exposer une API REST ou RPC appelée par le producer. C’est l’idée des webhooks. Dans le cas où les consumers sont HS, il se peut simplement qu’ils ratent l’event.\n\n\nUne autre solution est l’utilisation de message brokers (ou message queues).\nCe sont en fait des BDD, soit in memory, soit avec une forme de persistance, qui mettent en relation les producers et consumers en général de manière asynchrone.\nIls tolèrent donc les crashs côté consumer, puisque le message pourra être traité plus tard.\n\n\nPar rapport aux BDD :\nIls ont des similarités, et peuvent même participer à des protocoles 2PC utilisant XA.\nMais il y a des différences :\nLes BDD gardent les données, alors que les brokers les effacent quand ça a été traité.\nLes brokers partent du principe que le nombre de messages à avoir en mémoire est faible. S’il grossit les performances peuvent se dégrader.\n\n\n\n\n\n\nLorsqu’il y a plusieurs consumers, on peut trouver deux stratégies pour leur envoyer les events :\nLoad balancing : si les messages coûtent cher à traiter, on donne chaque message à un consommateur.\nLes protocoles d’encapsulation AMQP et JMS supportent tous deux cette pratique.\n\n\nFan-out : Chaque consumer reçoit le message et peut le traiter indépendamment des autres.\nLà encore AMQP et JMS supportent cette pratique.\n\n\n\n\nPour que le broker sache quand il faut enlever le message de la queue et éviter de l’enlever en cas de crash du consumer, le consumer qui a traité le message doit faire un acknowledgement. Sinon le message reste et devra être traité.\nCes crashs peuvent causer un traitement des messages dans un ordre différent de celui d’arrivée. Si on veut éviter ça, on peut faire une queue par consumer.\n\n\n\n\nLes brokers traditionnels se distinguent des BDD ou des batches par le fait que les données sont détruites une fois traitées. Mais on peut très bien combiner la faible latence de traitement des messages (streaming) avec de la persistance durable : on a alors les log-based message brokers.\nIl s’agit d’écrire les events dans un fichier de log, comme on le ferait pour les LSM-Tree, ou les write-ahead logs. Les consumers peuvent alors traiter le fichier séquentiellement, et une fois à la fin être notifiés à chaque nouveau message.\nPour pouvoir scaler avec ce modèle au-delà de ce que peut supporter la lecture d’un seul disque, on peut utiliser le partitionnement : les messages sont partitionnés sur différentes machines représentant des producers, et des consumers viennent traiter les messages sur chaque partition.\nAu sein de chaque partition, on peut avoir un identifiant séquentiel indiquant l’ordre. Par contre, ça ne marche pas à travers les partitions.\nGrâce au partitionnement, ce type de log-based brokers, malgré le fait d’écrire sur disque, arrivent à traiter plusieurs millions de messages par seconde.\n\n\nCe type de broker est implémenté par Apache Kafka, Amazon Kinesis Streams et Twitter’s Distributed Log. Google Cloud Pub/Sub est architecturé de cette façon, mais expose une JMS-style API.\nLes log-based brokers supportent le fan-out messaging puisque les logs sont conservés et peuvent être lus un grand nombre de fois.\nPour ce qui est du load-balancing messaging, c’est mis en place à l’aide des partitions, qui sont assignés à des consumers spécifiques.\nIl y a des désavantages :\nOn n’a qu’un consumer par partition.\nLes messages lents d’une même partition vont ralentir les autres messages de cette partition.\n\n\n\n\nQuand utiliser les brokers classiques vs log-based :\nQuand le processing des messages peut être coûteux, et qu’on a envie de paralléliser message par message (et quand l’ordre des messages n’est pas très important), on peut utiliser les brokers de type JMS / AMQP.\nQuand en revanche les messages sont rapides à traiter, et que l’ordre importe, alors les log-based brokers sont pertinents.\nVu que l’ordre est respecté seulement au sein des partitions, on peut très bien choisir comme clé de partitionnement la chose dont on veut que les événement liés gardent le bon ordre. Par exemple l’id d’un utilisateur.\n\n\n\n\nÉtant donné que l’ordre est respecté au sein de chaque partition, on n’a plus besoin d'acknowledgement quand le traitement est fait pour chaque event. On sait que ce sera fait dans l’ordre et on peut regarder régulièrement le log offset de chaque consumer.\nSi un consumer échoue, un autre reprendra au dernier log offset connu. Et si des messages avaient été traités mais dont le log offset n’était pas connu, ils seront traités deux fois (il va falloir régler ce problème).\n\n\nA propos de l’espace disque :\nA force d’écrire des logs sur le DD, il finit par être plein, et il faut alors supprimer des données ou les bouger vers un espace d’archivage.\nCela veut donc dire que si on consumer est vraiment trop lent, il pourrait finir par ne plus avoir accès aux messages non lus qui ont été déplacés.\nIl faut quand même relativiser ça : un DD typique fait 6To, et en écrivant séquentiellement à la vitesse max on écrit en moyenne à 150 Mo/s. Ce qui fait 11 heures pour remplir le disque dur. Et sachant qu’on n’écrit pas en permanence à la vitesse max, en général des events de plusieurs jours vont pouvoir être stockés sur une même machine productrice.\nSi un consumer est trop en retard, on peut aussi lever une alerte pour qu’un être humain gère. Vu les délais, il aura normalement le temps de régler la situation.\n\n\nOn peut noter aussi que pour les log-based brokers, vu qu’ils écrivent toujours sur DD, le temps de traitement reste à peu près constant, alors que pour les brokers plus classiques, si on dépasse la RAM et qu’on doit écrire sur DD, les performances se dégradent.\n\n\nOn peut remarquer que les log-based brokers sont plus proches des batches que les brokers classiques. Les données anciennes étant conservées, on peut les rejouer à loisir pour faire des tâches dessus.\n\n\n\n\nLes streams et les bases de données :\nLes principes des streams peuvent aussi être utiles pour les BDD.\nPar exemple, le replication log envoyé par le leader n’est rien d’autre qu’un stream.\nOn peut aussi considérer que chaque opération d’écriture en BDD est un événement, et qu’on peut reconstruire la BDD à partir du log d’events déterministes.\n\n\nOn se retrouve souvent avec des copies des données sous différents formats pour différents usages (cache, data warehouse etc.). Mais comment garder ces données synchronisées ?\nUne solution est d’utiliser les batches. Mais c’est lent, et on n’aura pas de données à jour rapidement.\nUne autre solution serait d’écrire en même temps dans la BDD principale et dans ces autres copies. Mais dans des systèmes distribués il peut survenir des inconsistances entre ces copies.\nPour régler ce problème, on pourrait transformer les copies en suiveuses de la BDD principales comme avec le modèle leader / follower.\nMalheureusement pendant longtemps les logs des messages allant dans la BDD ont été considérés comme des API privées. Récemment on a un intérêt vers le fait de les exploiter comme des streams qu’on appelle change data capture (CDC).\nLa solution est d’utiliser un log-based broker pour transporter les events d’écriture de la BDD (leader) vers les datasets qui sont des followers (search index, data warehouse etc.).\nC’est utilisé par Databus de LinkedIn, Wormhole de Facebook et Sherpa de Yahoo.\nBottled Water le fait pour PostgreSQL en lisant son write-ahead log.\nMaxwell et Debezium le font pour MySQL.\nMongodriver le fait pour MongoDB.\nGoldenGate le fait pour Oracle.\nKafka Connect Framework offre des connecteurs CDC pour divers BDD.\nRethinkDB, Firebase, CouchDB, MongoDB et VoltDB permettent aussi d’avoir un mécanisme pour exporter le stream des données hors de la BDD.\nEn général, cette solution est utilisée dans un mode de réplication asynchrone.\n\n\nCertains outils permettent de commencer un dataset suiveur avec un snapshot initial des données, plutôt que de récupérer la totalité des logs pour reconstruire la BDD.\nCertains outils comme Apache Kafka permettent aussi de récupérer les logs compactés, au sens de la compaction des LSM-Tree : seules les logs représentant la dernière version des entrées sont gardées. Si une entrée est supprimée à un moment, toutes les logs précédentes de cette entrée peuvent être supprimées aussi par la compaction par exemple.\n\n\n\n\nEvent sourcing : c’est une idée développée par la communauté domain-driven design (DDD).\nCela consiste à stocker tous les changements d’état d’une application sous forme de logs de change events\nLa différence entre le change data capture de la BDD et l’event sourcing c’est que le change data capture permet d'ajouter / enlever / modifier des choses dans la BDD et d'en faire un log, alors que l’event sourcing décourage ou interdit la modification, mais consiste plutôt à accumuler des events qui représentent des choses qui se produisent plutôt que de simples changements d’état qui s’annuleraient entre eux.\nLa conséquence est qu’on ne peut pas vraiment faire de compaction pour les events de l’event sourcing, parce qu’ils ne s’annulent pas entre eux à proprement parler. Il faut garder ces events immuables.\n\n\nL’event sourcing est un modèle très puissant pour représenter clairement ce qui se passe dans l'application, et permet aussi d’avoir des facilités pour débugger.\nIl existe des BDD spéciales pour l’event sourcing comme Event Store, mais en réalité n’importe quelle BDD ou message broker serait adapté.\nL’event sourcing sépare bien les events des commands. Quand une requête arrive de l’utilisateur c’est d’abord une command. Elle doit être traitée et validée, et c’est seulement quand on est sûr qu’elle l’est qu’elle devient un event immuable. Elle est alors transmise à divers systèmes consommateurs et ne peut pas être supprimée, mais seulement changée par un autre event d’annulation par exemple.\n\n\nLes streams et les états vis à vis de l’immuabilité :\nOn peut voir la BDD comme étant un sous ensemble, ou une version cache la plus récente des données que sont les logs d’events. Avec le mécanisme de compaction des SSTables c’est encore plus évident puisqu’on a les logs, et on vient enlever ce qui est “inutile” pour obtenir la BDD qui est l’état le plus actuel des données.\nUn des avantages à avoir les logs des changements immuables comme source de vérité principale à partir de laquelle on peut construire diverses formes de dataset est que même si on fait une opération malheureuse qui corrompt les données, si c’est juste sous forme de log il suffira de revenir en arrière dans les logs et c’est réglé. Avec une vraie BDD si on a corrompu les données on risque de ne pas pouvoir défaire.\nOn peut dériver diverses formes de données à partir des logs :\nPar exemple, Druid ingère les données de Kafka, de même pour Pistachio qui utilise Kafka comme un commit log, et Kafka Connect peut exporter les données de Kafka vers diverses BDD ou indexes.\n\n\nStocker les données est facile si on n’a pas à se préoccuper de le faire dans un format qui permettra une lecture optimisée en fonction de notre contexte. On peut donc séparer l’écriture de la lecture, en créant de nouveaux dataset dérivés quand on a besoin des données pour faire quelque chose de spécifique.\nCette idée de séparer les données d’écriture et de lecture est connue sous le nom CQRS (Command Query Responsibility Segregation).\nDans cette approche la question de “faut-il vraiment dénormaliser ?” ne se pose plus vraiment : il est logique de dénormaliser pour optimiser en lecture, vu que de toute façon les données seront présentes sous une forme plus canonique dans la version écrite.\n\n\nAvantages et inconvénients de l'event sourcing et du change data capture :\nUn des inconvénients est que si l’écriture se fait de manière asynchrone pour gagner du temps (ce qui est souvent le cas), on risque de ne pas avoir la garantie de read after your writes par exemple. Pour remédier à ça on pourrait rendre la copie synchrone, ou utiliser des transactions distribuées, ou un total order broadcast.\nUn des avantages est que ça peut faciliter la concurrency control : à chaque fois qu’une requête a besoin de modifier plusieurs objets, on peut très bien écrire un event dans le log qui implique l’ensemble de ces objets. Et donc on aurait des opérations atomiques écrites en une fois.\n\n\nA propos de l'immuabilité :\nElle est utile si les données ne changent pas tant que ça, mais si elles changent beaucoup on risque de se retrouver avec des logs énormes pour pas beaucoup de données.\nOn a aussi des contraintes légales qui imposent parfois de supprimer certaines données.\nOn peut alors réécrire l’historique pour enlever certaines données. Datomic appelle ça l’excision.\nIl faut savoir aussi qu’étant donné les diverses copies de dataset, backups et autres, c’est assez difficile de complètement supprimer les données.\n\n\n\n\n\n\n\n\nTraitement des streams.\nOn peut faire 3 choses avec un stream :\nL’écrire en BDD ou autre forme de persistance.\nLe donner directement à l’utilisateur en lui affichant.\nLe modifier pour fabriquer un nouveau stream à travers un operator comme avec les batchs, dont le résultat ira à nouveau dans une persistance ou chez l’utilisateur.\n\n\nTout ceci est assez similaire à ce qui se passe avec les batchs.\nLa différence c’est que le stream ne se finit pas, et donc on ne peut pas faire de sort ou de jointures sort-merge comme avec les batchs.\nLa tolérance aux erreurs aussi n’est pas la même : on peut difficilement rejouer un stream qui tourne depuis des années comme on rejouerait un batch qui vient d’échouer.\n\n\nA propos des usages du streaming :\nOn l’utilise pour du monitoring quand on veut être averti de choses particulières, par exemple avec la détection de fraudes, le statut des machines d’une usine etc.\nLes complex event processing (CEP) permettent de déclarer des patterns à trouver (souvent avec du SQL), et créent des complex events à chaque fois que ça match, il s’agit de trouver une combinaison d’events.\nC’est implémenté dans Esper, IBM InfoSphere Streams, Apama, TIBCO StreamBase, SQLstream.\n\n\nLes stream analytics qui ressemblent aux CEP mais sont plus orientés vers le fait de trouver des résultats agrégés à partir des données streamées. Par exemple calculer une moyenne, une statistique.\nOn utilise souvent des fenêtres de données pour faire les calculs dessus.\nOn utilise parfois des algorithmes probabilistes comme les Bloom Filters pour savoir si un élément est dans un set et d’autres. Ces algorithmes produisent des résultats approximatifs mais utilisent moins de mémoire.\nParmi les outils on a Apache Storm, Spark Streaming, Flink, Concord, Samza et Kafka Streams. Et parmi les outils hostés on a Google Cloud Dataflow et Azure Stream Analytics.\n\n\nLes dataset dérivées des logs comme dans l’event sourcing peuvent être vus comme des materialized views, dans ce cas il faut prendre en compte l’ensemble des logs et pas juste une fenêtre.\nSamza et Kafka Streams font ça.\n\n\nOn peut faire aussi un peu pareil que les CEP mais en recherchant un seul event qui match un critère de recherche. Alors que d’habitude on doit indexer avant de faire une recherche, là il s’agit de rechercher en plein streaming.\nLa feature percolator d’Elasticsearch permet de faire ça.\n\n\n\n\nLa notion de temps dans la gestion des stream processing :\nAlors que dans les batch processing ce qui compte c’est éventuellement le timestamp des events analysés, et pas le temps pendant le quel le batch s’exécute (ce qui rend la réexécution du batch transparente d’ailleurs), dans le cadre du stream processing le temps pendant lequel le processing s’exécute peut être pris en compte, par exemple pour faire des fenêtres.\nAttention cependant aux lags : il est possible que lors du stream processing un event soit processé bien après avoir été émis. Et dans ce cas on peut se retrouver avec des events traités dans un ordre qui n’est pas le bon vis-à-vis de leur émission.\n\n\nQuand on stream avec des fenêtres de temps contenant des events pour y faire des opérations, on ne peut jamais être sûr que tous les events de la fenêtre sont arrivés : ils ont peut être été retardés (qu’on appelle straggler)\nDans ce cas, soit on dit tant pis et on annule les events retardataires, en levant éventuellement une alerte s’il y en a trop.\nSoit on publie plus tard un correctif avec les events retardataires.\n\n\nQuand on veut prendre en compte le temps, le temps de la création de l’event est souvent plus précis (par exemple un event peut être créé offline par un mobile, et envoyé seulement quand il est connecté), mais aussi moins fiable vu que la machine n’est pas sous notre contrôle contrairement au serveur.\nUne des solutions est de relever (1) le temps de l’event indiqué par le client, (2) le temps de l’envoi de l’event indiqué par le client, et (3) le temps de la réception de l’event par le serveur. De cette manière on peut comparer les horloges du client et du serveur vu que le (2) et le (3) doivent être très proches.\n\n\nIl y a plusieurs types de fenêtres temporelles :\nTumbling window : Les fenêtres sont fixes, et chaque event appartient à une fenêtre.\nHopping window : Les fenêtres font la même taille mais se chevauchent, certains events qui sont entre les deux sont dans les deux fenêtres.\nSliding window : Les fenêtres font la même taille mais se déplacent dans le temps, et donc les events les plus anciens sont exclus au fur et à mesure, remplacés par des events plus récents.\nSession window : Les fenêtres n’ont pas la même taille, elles regroupent des events proches dans le temps où un même utilisateur a été actif.\n\n\n\n\nLes streams étant une généralisation des batchs, on a ici le même besoin des jointures.\nOn peut dénombrer 3 types :\nLe stream-stream join (window join) consiste à joindre deux ensemble streams d’events ensemble. Par exemple dans le cadre d’une recherche, joindre les events de recherches faites aux events clics qui s’en sont suivis (ou à l’absence de clics après timeout).\nLe stream-table join (stream enrichement) consiste à “enrichir” les events issus d’un stream avec le contenu d’une BDD. Par exemple les actions d’un utilisateur enrichis (complétés ou triés) avec des infos issus de son profil.\nPour ce faire il nous faut une copie de la BDD sur le disque local de préférence, et si suffisamment petit on peut même la mettre en RAM. C’est très similaire aux Map-side joins des batchs.\nVu que les données de la table risquent d’être mises à jour, on peut utiliser le change data capture pour récolter les mises à jour de la table régulièrement.\n\n\nLe table-table join (materialized view maintenance) consiste à matérialiser une requête de jointure entre deux tables, à chaque fois qu’il y a un changement dans ces deux tables qui risque d’affecter le résultat de cette jointure.\nOn peut prendre l’exemple de twitter qui, en même temps qu’il stocke les tweets et followers, construit une timeline en cache au fur et à mesure.\n\n\n\n\nOn remarque que dans la plupart des cas, le temps est important, et que deux événements, ou un événement et une mise à jour en BDD pourraient arriver avant ou après l’autre (du fait du partitionnement). Ceci rend la jointure non déterministe (si on la refait on risque d’avoir un résultat différent).\nDans les data warehouses ce problème s’appelle _slowly changing dimension (SCD) _et la solution à ça peut être d’ajouter un identifiant qui est changé à chaque event. Mais la conséquence c’est qu’on ne peut plus faire de compaction.\n\n\n\n\nA propos des fautes dans le cadre des streams :\nL’avantage avec les batchs c’était le fait de pouvoir réexécuter en cas d’erreur, et d’avoir au final le job exécuté comme s’il l’avait été une seule fois.\nUne des solutions est le microbatching : on fait des petites fenêtres de données (souvent d’1 seconde) et on les traite comme des batchs.\nSpark Streaming fait ça.\nUne variante consiste à faire des checkpoints réguliers sur DD, et en cas de crash on recommence à partir du checkpoint.\nFlink fait ça.\n\n\nAttention cependant au moment où on fait autre chose avec ces données, comme écrire en BDD ou envoyer un email. Dans ce cas, il s’agit de side effects qui pourront être réexécutés en cas de réexécution du microbatch.\nPour régler ce problème, il faut tout préparer, et exécuter tout ce qui est validation des opérations, side-effects et autres en une seule fois et de manière atomique.\nC’est un peu de la même manière que le 2PC (two phase commit) des transactions distribuées.\nC’est utilisé par Google Cloud Dataflow, VoltDB et Apache Kafka.\n\n\nUne autre solution pour ce problème est de créer de l’idempotence, c’est-à-dire faire en sorte qu’une chose faite plusieurs fois donne le même résultat.\nOn peut le faire par exemple en retenant un offset qui fera en sorte de ne rien faire si on tente de refaire l’opération.\nAttention au fait que cela implique qu’il faut rejouer les messages dans le même ordre (un broker log-based permet ça), de manière déterministe, et sans concurrence.\n\n\n\n\n\n\n\n\nOn peut aussi vouloir que des states (par exemple compteurs, moyennes etc.) soient reconstruites en cas de faute.\nDans certains cas ça peut être fait à partir des events, par exemple parce qu’il s’agit d’un état qui porte sur peu d’entre eux.\nSinon une solution peut être de les sauvegarder régulièrement quelque part pour aller les chercher en cas de besoin.\nFlink capture régulièrement ces valeurs et les écrit sur du HDFS.\nSamza et Kafka Streams répliquent les changements des states vers un stockage persistant avec compaction.\nVoltDB réplique les states en faisant le processing des messages sur plusieurs nœuds.\nIl faut voir que la sauvegarde en local avec accès au disque ou la sauvegarde via le réseau peuvent chacun être plus ou moins performants en fonction des cas.","12---the-future-of-data-systems#12 - The Future of Data Systems":"Chaque outil a ses avantages et inconvénients, et il n’y a pas d’outils parfaits.\nCertaines personnes disent que tel ou tel type d'outil n’a aucune utilité, mais ça reflète surtout le fait qu’eux ne l’utilisent pas, et qu’ils ne voient pas plus loin que le bout de leur nez.\n\n\nIl convient souvent de combiner plusieurs outils pour plusieurs usages :\nParmi ceux-ci on peut trouver :\nUne BDD relationnelle pour la persistance de données structurées. (ex : PostgreSQL)\nUn index de recherche pour une recherche performante, mais qui est moins bon sur la persistance des données (ex : Elasticsearch)\nUn système d’analyse du type data warehouse ou batch / stream processing.\nParmi les batchs / streams on pourrait vouloir alimenter un système de machine learning, de classification, de ranking, de recommandations, de notification basée sur le changement de données.\n\n\nUn cache ou des données dénormalisées issues des données initiales.\n\n\nPar exemple, on peut avoir une BDD et un search index, avec les données écrites d’abord dans la BDD, puis propagées dans le search index via change data capture (CDC).\nSi on décide qu’on veut écrire à la fois dans la BDD, et dans le search index, alors on risque d’avoir des latences qui causent des différences d’ordre d’écriture entre les deux.\nUne solution à ça c’est d’utiliser un système d’entonnoir qui force l’ordre, dans l’idée d’un total order broadcast.\n\n\n\n\nQue choisir entre les données dérivées (CDC, event sourcing) et les transactions distribuées (2PC) pour faire communiquer plusieurs outils entre eux ?\nSelon l’auteur, XA, le protocole qui permet de faire communiquer les outils via les transactions distribuées a une mauvaise tolérance aux fautes et une faible performance. Et en l’absence d’un autre protocole aussi répandu (ce qui ne risque pas d’arriver rapidement), il est plus pertinent d’opter pour les datasets dérivés.\nCependant, les transactions distribuées supportent la linearizability et donc permettent par exemple le “read your own writes”, alors que les données dérivées sont en général asynchrones et donc n’apportent pas ces garanties. Cette eventual consistency est à mettre dans la balance.\nPlus tard on parlera d’un moyen de contourner ce problème.\n\n\n\n\n\n\nAttention au fait de vouloir du total ordering :\nPour avoir du total ordering il faut que les données passent par une seule machine (par exemple single leader). Sinon on peut créer des partitions mais on aura des ambiguïtés entre partitions.\nDans le cas de plusieurs datacenters on a en général besoin de 2 leaders => on n’aura donc pas de total ordering.\nQuand on fait du micro-service, il est courant de déployer le code sur des machines avec chacune son stockage et sans que ce stockage soit donc partagé => on se retrouve là aussi donc à ne pas respecter le total ordering.\nPour être clair : le total ordering implique le total order broadcast, qui est équivalent au consensus. Et la plupart des algorithmes de consensus ne sont pas faits pour marcher si le throughput dépasse les données que peut gérer un seul serveur. Le fait de pouvoir gérer un tel throughput avec des datacenters distribués dans le monde est un sujet de recherche.\n\n\nA propos de l’ordre causal :\nPour les événements qui touchent le même objet, celui-ci étant sur la même partition on peut ordonner ces actions et respecter la causalité.\nEn revanche, pour les événements qui portent sur plusieurs objets il n’y a pas de solution facile. Quelques pistes :\nLes clocks logiques peuvent aider.\nSi on log des events pendant pour les lectures, alors les autres évents peuvent les utiliser pour identifier le moment où un événement ne s’était pas produit et créer un ordre comme ça.\nLes structures de résolution automatique de conflit (fusion des objets par exemple) peuvent aussi aider.\n\n\n\n\nA propos des batches et streams :\nUne des raisons pour lesquelles il est pratique d’avoir des dataset dérivés par batch/stream plutôt que transactions distribuées est qu’on peut fauter quelque part et ne pas tout annuler, mais seulement retenter la construction du batch/stream.\nUn des avantages des batchs/streams c’est qu’avec les datasets dérivés, on peut changer le schéma de nos données pour un dataset, et reprocesser le tout, ou continuer pour le stream. On n’a pas à faire d’opérations destructives pour faire évoluer notre code.\nOn peut d’ailleurs faire les changements graduellement, blocs de données par bloc de données.\n\n\nLa lambda architecture consiste à avoir un batch et un stream qui vont processer la même chose pour avoir la donnée immédiatement, mais avoir un process mieux tolérant aux erreurs plus tard. Le stream fait une approximation, alors que le batch fait un calcul plus précis régulièrement.\nIl y a cependant plusieurs problèmes :\nMaintenir la logique dans le batch et le stream est difficile.\nIl faut merger les deux régulièrement, et ça peut être difficile si les opérations appliquées sont difficiles.\nReprocesser toutes les données avec le batch est très coûteux, donc on peut à la place reprocesser une seule heure de données et y ajouter le stream. Cependant, rendre le batch incrémental le fragilise.\n\n\nMais plus récemment on a d’autres solutions pour rendre la lambda architecture plus utilisable grâce à certaines features qui sont de plus en plus supportés par les outils.\n\n\n\n\n\n\nA propos de BDD :\nLes BDD et les filesystem font la même tâche.\nMais ils ont certaines différences : les filesystem Unix offrent une API bas niveau pour traiter avec les fichiers, alors que les BDD offrent une API plus haut niveau avec SQL.\nD’une certaine manière certaines BDD NoSQL tentent d’ajouter la philosophie Unix aux BDD.\n\n\nLes BDD et les batchs / streams ont des choses en commun :\nPar exemple, la construction de search indexs dans les batchs/streams sont un peu la même chose que la construction d’index secondaires.\nEt du coup on en arrive à la conclusion qu’en fait les batchs/streams ne sont que la continuation d’une même base de données transformée pour l'adapter aux besoins, distribuée sur d'autres machines et administrée éventuellement par d’autres équipes.\nL’auteur spécule que les données seront organisées en deux grands axes, qui sont en fait deux faces de la même pièce :\nFederated databases (unifying reads) : il s’agit de fournir une API de lecture pour accéder à toutes les données existantes du système, tout en laissant les applications spécialisées accéder directement aux datasets spécifiques dont elles ont besoin. L’idée est donc de connecter toutes les données ensemble.\nPostgreSQL et son foreign data wrapper permet de faire ça.\n\n\nUnbundled databases (unifying writes) : il s’agit de traiter les écritures pour qu’on puisse écrire dans n’importe quelle version des données, et qu’elles soient quand même synchronisées avec le reste. Alors que les BDD supportent les indexes secondaires, ici on a différents datasets interconnectés et donc on doit en quelque sorte maintenir nos indexes à la main.\nOn est en plein dans la tradition Unix où des petits outils font une chose bien, et peuvent s’interconnecter.\nAlors que la fédération des données n’est pas trop difficile, maintenir les données synchronisées est plus compliqué à faire.\nPour accomplir ces données synchronisées on recourt traditionnellement aux transactions distribuées, mais selon l’auteur c’est la mauvaise approche. L’approche sous forme de données dérivées depuis un event log asynchrone, et l’utilisation de l’idempotence est bien plus solide.\nUne des raisons déjà évoquée est que faire communiquer des systèmes de données hétérogènes via un mauvais protocole marche moins bien que via une meilleure abstraction avec des logs d’event et de l’idempotence.\nLe gros plus de l’approche avec les event logs est le couplage faible entre les composants :\nLa nature asynchrone de cette approche permet de tolérer bien mieux les fautes (par exemple, un consommateur fautif va rattraper son retard plus tard via les messages accumulés) alors qu’avec les transactions distribuées synchrones par nature, les fautes ont tendance à être amplifiées.\nAu niveau des équipes, chacune peut se spécialiser dans un type de dataset pour un usage, et le faire indépendamment des autres.\n\n\n\n\nEntre utiliser un système de BDD intégré et un système composé de datasets dérivés, le choix des datasets dérivés n’est pas forcément systématique. Ça peut être une forme d’optimisation prématurée, et d’ailleurs si un système de BDD répond à nos besoins, autant l’utiliser lui seul.\nCe qui manque dans l’histoire c’est une manière simple et haut niveau d’interconnecter ces systèmes, par exemple “declare mysql | elasticsearch” comme équivalent de “CREATE INDEX” dans une BDD.\nIl y a des recherches à ce sujet mais pour le moment rien de tel, on doit faire beaucoup de choses à la main.\n\n\n\n\n\n\n\n\nPour continuer sur l’idée de l’unbundling databases, et des applications autour du dataflow :\nOn peut trouver des ressemblances avec le concept d’unbundling des BDD et des langages de dataflow comme Oz, Juttle, les langages fonctionnels réactifs comme Erlang, et les langages de programmation logique comme Bloom.\nL’idée de l’unbundling est aussi présente dans les tableurs quand ils mettent à jour toute une colonne dès qu’une donnée est écrite. Il faut juste faire la même chose mais dans un contexte distribué, et avec des technologies disparates.\nOn a différentes formes de données dérivées, mais en gros dès que la dérivation est spécifique à notre métier, il faut écrire du code applicatif pour gérer ce dataset-là. Et les BDD ont en général du mal à permettre ça. Il y a bien les triggers / stored procedures, mais ça reste une feature secondaire.\nIl est devenu un pattern courant et une bonne pratique de séparer le code applicatif du state (ie. la persistance), en ayant des serveurs stateless qui accèdent à une BDD commune.\nLes développeurs fonctionnels disent qu’ils sont pour “la séparation de l’église et de l’état”.\nCependant, de même que dans la plupart des langages il n’y a pas de système de souscription (sauf à le faire avec le pattern observer), dans les BDD il n’y en a pas non plus sauf récemment avec les CDC par exemple.\n\n\nVu qu’on veut sortir la logique de mise à jour automatique par exemple d’un index dans la BDD hors de celle-ci, on peut partir du principe que la donnée n’est pas une chose passive utilisée par l’application, mais que les changements dans un dataset peuvent déclencher du code applicatif pour créer un autre dataset.\nA cet effet, on peut utiliser des log message brokers (et non pas des message brokers traditionnels qui servent à exécuter des jobs de manière asynchrone).\nL’ordre des messages est souvent important pour maintenir des datasets dérivés.\nOn doit être tolérant aux fautes et ne pas perdre de messages, sous peine d’inconsistance.\nLes message brokers permettent au code applicatif de s’exécuter sous forme d’operators, ce qui est pratique.\n\n\n\n\nLe stream processing et les services :\nL’architecture sous forme de services est plutôt à la mode, son avantage principal est de permettre une forme de scalabilité dans l’entreprise, en permettant à plusieurs équipes de déployer séparément.\nIl y a cependant une différence entre les services qui vont envoyer un message pour recevoir une réponse du service qui a les données, et le stream processing qui va construire et maintenir à jour un dataset local à la machine qui a le code applicatif, qui n’aura plus de requête réseau à faire => la méthode avec le stream processing est donc plus performante.\n\n\n\n\nLecture des données dérivées :\nLes données dérivées sont construites et mises à jour en observant la donnée initiale et la faisant passer à travers des operators, tout ceci pendant la phase d’écriture. On a ensuite du code exécuté qui lit ces données dérivées et qui répond à une requête d’un client, pendant la phase de lecture donc.\nCe point de rencontre représente en quelque sorte le point d’équilibre entre la quantité de travail qu’on souhaite faire à l’écriture, et la quantité de travail qu’on souhaite faire à la lecture.\nOn peut déplacer ce point de rencontre pour faire plus de travail à l’écriture, ou plus à la lecture.\nPar exemple pour la recherche, on peut très bien ne pas créer de search index, et tout faire à la lecture.\nOu alors on peut non seulement créer un search index à l’écriture, mais aussi créer tous les résultats de recherche possibles, comme ça à la lecture on n’aura plus qu’à lire un cache (aussi appelé materialized view).\nSi créer l’ensemble des résultats de recherche serait sans doute excessif, on peut très bien imaginer mettre en cache les résultats des recherches les plus fréquentes.\n\n\nOn voit qu’on retrouve aussi notre exemple de twitter qui avait choisi de mettre en cache toutes les timelines, sauf pour les célébrités où il faisait la recherche en BDD.\n\n\nAutre exemple de lecture de données dérivées : les applications web sur mobile qui gagnent de plus de capacité d’autonomie, y compris offline, peuvent stocker une forme de dataset dérivé au sein même du mobile, permettant au code sur le client d’en faire quelque chose offline.\nLes outils frontend comme le langage Elm et le framework React / Redux permettent de souscrire à des events de l’utilisateur, en mode event sourcing.\nIl serait tout à fait naturel de faire la même chose dans la relation client / serveur : permettre au client de faire une requête, puis de réceptionner non pas une réponse mais un stream de messages réguliers.\n\n\n\n\nLes log message brokers passent en général leur contenu à une forme ou une autre de BDD spécialisée, mais il y a aussi une certaine persistance des events eux-mêmes (les logs) dans le message broker. En général seuls les events d’écriture y sont consignés, ce qui est raisonnable mais n’est pas la seule manière de faire possible.\nIl est possible qu’en fonction du besoin applicatif, on ait aussi intérêt à consigner les events de lecture. Ça permet notamment de faire un stream-table join entre les lectures et les données existantes.\nC’est utile en particulier dans le cas où on a plusieurs partitions qu’il faut traverser pour obtenir notre résultat.\nLa feature de distributed RPC de Storm implémente cette fonctionnalité.\n\n\nÇa prend bien sûr plus de place donc il faut y réfléchir.\nUn des avantages est que ça permet aussi de régler le problème de causalité vis-à-vis d’écritures sur des objets différents.\n\n\n\n\n\n\nA la recherche des données correctes :\nOn a tendance à avoir un mouvement vers une plus grande performance, availability et scalability, avec une consistency qui est parfois délaissée.\nExemple : la réplication leaderless.\nOn peut aussi noter le rapport hasardeux à l’isolation et l’implémentation de faibles niveaux d’isolation dans beaucoup de BDD.\n\n\nOn peut répondre à certaines problématiques de corruption de données à l’aide de la serializability et des atomic commits, mais c’est vraiment coûteux et ça marche surtout sur un seul datacenter, avec des limites de scalabilité.\nIl y a aussi les transactions qui permettent de régler certains problèmes, mais ce n’est pas la seule solution.\n\n\nN’oublions pas non plus les erreurs et bugs applicatifs qui peuvent endommager les données de manière définitive, même en présence de serializability…\nPour lutter contre ces problèmes voici quelques solutions :\nL’immutabilité des données (du genre event sourcing et autres) permet d’être sûr que même en écrivant des données corrompues, on pourra toujours les annuler pour retrouver l’état d’avant.\nRendre les opérations idempotentes pour qu’elles ne puissent être exécutées qu’une seule fois au plus est une forme de protection contre la corruption de données.\nDe manière générale, il est intéressant d’implémenter des mécanismes end-to-end qui vont suivre la requête de bout en bout. Par exemple TCP fournit ce genre de garanties à son niveau, mais une connexion TCP peut sauter et on peut en établir une autre pour refaire la même transaction, on a alors besoin de quelque chose qui suit notre transaction.\nMalheureusement, implémenter de tels mécanismes end-to-end au niveau applicatif n’est pas simple. Pour l’auteur, il faudrait qu’on trouve la bonne abstraction pour rendre ça facile, mais il y a de la recherche à faire.\n\n\n\n\nAppliquer des contraintes :\nLa contrainte de uniqueness dans un système distribué nécessite le consensus, et donc une forme de fonctionnement synchrone. Si les writes se faisaient de manière asynchrone, alors on ne saurait pas immédiatement si on peut écrire en respectant cette contrainte ou pas, et on aurait le conflit plus tard.\nUn bon moyen pour garantir cette contrainte est de partitionner en fonction de la clé qui doit avoir la contrainte d’unicité. Mais là aussi bien sûr on ne pourra pas écrire dans la BDD de manière asynchrone.\n\n\nPour les contraintes au sein des log-based message brokers il s’agit aussi de faire en sorte que les requêtes avec possibilité de conflit soient dans la même partition, et de vérifier séquentiellement, message par message, que la requête respecte bien la contrainte vis-à-vis de la BDD locale.\nDans le cas où les entrées qui sont l’objet de contraintes sont localisées dans des partitions différentes ça se complique un peu.\nOn peut utiliser un atomic commit (par exemple 2PC).\nMais on peut aussi faire sans (exemple de débit / crédit d’un compte) :\nOn attribue un id à la requête.\nLe stream processor crée 2 messages : un pour décrémenter le compte qui a un débit, et un autre pour incrémenter l’autre compte (qui sont chacun sur leur partition).\nLes processors suivants consomment les messages, appliquent le débit ou le crédit, et dédupliquent en fonction de l’id de la transaction initiale.\nVu qu’on est dans un log based broker avec l’ordre des messages préservés et de la persistance, en cas de crash de l’un des consommateurs, il redémarre et réapplique les messages non processés dans l’ordre prévu.\nNous avons donc réussi à réaliser une transaction multi-partition sans utiliser de protocole de type atomic commit, en cassant la transaction en plusieurs morceaux s’exécutant chacun sur leur partition, et en ayant un mécanisme end-to-end (ici l’id) assurant l’intégrité du tout (le fait qu’un bout ne sera pas exécuté 2 fois).\n\n\n\n\n\n\n\n\nA propos de l’intégrité et de la relation au temps :\nLe terme consistency englobe en réalité deux enjeux :\nLa timeliness qui consiste à s’assurer que l’observateur voit une donnée à jour. C’est tout l’objet du terme eventual consistency quand la timeliness n’est pas respectée.\nL’integrity qui consiste à préserver les données d’une corruption permanente des données, y compris dérivées. Pour la régler il faudrait réparer et non pas juste attendre ou réessayer.\nSi le non-respect de la timeliness est embêtant, le non-respect de l’integrity peut être catastrophique.\n\n\nAlors que dans les transactions ACID la timeliness et l’integrity sont confondues, on vient de voir que dans le stream processing on peut les décorréler, et arriver à garantir l’integrity tout en ayant un fonctionnement asynchrone et donc ne garantissant pas la timeliness. Et le tout sans utiliser les transactions distribuées coûteuses;\nSelon l'auteur, cette technique est particulièrement prometteuse.\n\n\nOn peut aussi se demander si toutes les applications ont vraiment besoin d’un respect intransigeant de la timeliness, et donc d’un respect de la linearizability (dès qu’une écriture est faite, elle impacte les lecteurs) :\nOn peut très bien faire une compensating transaction dans le cas où on a accepté une transaction côté client mais qu’il se révèle qu’elle ne respecte pas les contraintes.\nD’ailleurs un processus d’excuse et de compensation peut être pertinent dans de nombreux cas, par exemple pour la réservation, souvent on propose plus de places que disponibles en partant du principe qu’il y aura des désistations. Et dans le cas où on a mal prévu, il faut pouvoir avoir un processus de compensation.\n\n\nOn peut créer un système qui pour l’essentiel évite la coordination :\n1 - on préserve l’intégrité des données dans les systèmes dérivés sans atomic commit, linearizability, ou coordination synchrone entre partitions.\n2 - la plupart des applications peuvent se passer de contraintes temporelles fortes pour la timeliness.\nSelon l'auteur, ce type de système sans coordination a beaucoup d’avantages. On peut très bien utiliser la coordination synchrone pour certaines opérations importantes qui ne permettent pas de retour en arrière, et garder le reste sans cette coordination.\nFinalement on peut voir la chose de cette manière : avoir de fortes garanties synchrones du type transactions distribuées réduit le nombre d’excuses qu’il faudra faire pour les données inconsistantes présentées, mais ne pas les utiliser réduit le nombre d’excuses qu’il faudra faire pour toutes les indisponibilités du système dues à la faible performance induite par la coordination.\n\n\n\n\nVis-à-vis des erreurs matérielles et logicielles :\nIl y a des corruptions probables contre lesquelles notre système model prévoit des parades, et des corruptions contre lesquelles non, comme par exemple faire confiance aux opérations du CPU. Pourtant tout peut arriver avec plus ou moins de probabilité.\nIl ne faut pas oublier non plus que les BDD ne sont que des logiciels qui peuvent avoir des bugs, et pour nos codes applicatifs c’est encore pire.\nLa corruption des données finit par arriver qu’on le veuille ou non. Il faut une forme d'auto-auditabilité. Il faut vérifier régulièrement que nos données sont bien là et intègres, de même que nos backups.\nL’approche représentée par l’event sourcing permet d’auditer plus facilement les données.\nEt si on a bien fait la séparation entre les données sources et dérivées c’est encore plus clair.\nOn peut faire un checksum sur le log d’events pour le vérifier, et on peut rejouer les batchs pour recréer des données dérivées propres.\n\n\nUne bonne pratique dans la vérification des données est de le faire sur des flows end-to-end. Cela permet d’inclure tout le hardware et le software dans ce qui est vraiment vérifié.\nLes techniques cryptographiques de vérification de l’intégrité introduites par la blockchain est un mécanisme très intéressant pour l’avenir de l’intégrité des données."}},"/books/learning-domain-driven-design":{"title":"Learning Domain Driven Design: Aligning Software Architecture and Business Strategy","data":{"":"","introduction#Introduction":"Le problème principal qui met en échec la plupart des projets de dev c’est la communication. Et c’est le cœur de ce qu’est le DDD.\nLe but du DDD c’est d’aligner le Design logiciel avec le Domaine business.\nLe DDD se décompose en 2 parties :\nLe design stratégique : créer une compréhension commune du domaine, et prendre des décisions haut niveau sur le projet.\nC’est les questions “Quel logiciel on crée ?” et “Pourquoi on le crée ?”\n\n\nLe design tactique : écrire du code qui épouse le domaine.\nC’est la question “Comment on crée chaque partie du logiciel ?”","part-i--strategic-design#Part I : Strategic Design":"","1---analyzing-business-domains#1 - Analyzing Business Domains":"Pour développer un bon logiciel qui réponde au besoin de notre entreprise, il faut que nous les devs connaissions la stratégie du business : différencier les parties importantes des parties moins importantes pour adapter nos techniques.\nLe business domain est le domaine d’activité principal de l’entreprise.\nExemple : FedEx fait de la livraison.\nPour une grande entreprise qui fait plusieurs choses très différentes (comme Google ou Amazon), on peut parler de plusieurs domains.\n\n\nUn subdomain est une activité que l’entreprise fait pour mener à bien son business.\nExemple : Starbucks fait des cafés, mais il doit aussi recruter, gérer la tréso etc.\nIl existe 3 types de subdomains :\nLes core subdomains sont les seules activités qui donnent à l’entreprise un avantage concurrentiel par rapport à ses concurrents. Soit par une innovation produit, soit par une optimisation qui permet de produire à moindre coût.\nC’est une activité par nature complexe étant donné qu’il faut qu’elle ne soit pas facilement reproductible par les concurrents.\nLe statut de “core” est aussi par nature temporaire : dès que les concurrents rattrappent ce n’est plus core.\nA noter aussi qu’un core subdomain peut très bien ne pas résider dans du logiciel. Par ex une bijouterie va vendre mieux que les concurrents grâce au style donné par les artisans bijoutiers (qui donne l’avantage compétitif et qui est donc le core subdomain), la boutique en ligne dans ce cas est un generic subdomain.\n\n\nLes generic subdomains sont des problèmes communs déjà résolus et largement disponibles et utilisés par les concurrents (soit en open source, soit sous forme de service payant).\nIls sont complexes comme les core subdomains, mais ne donnent juste pas d’avantage compétitif.\nExemple : un mécanisme d’authentification est complexe, mais des solutions open source et payantes existent, et personne ne recode son système à soi.\n\n\nLes supporting subdomains sont des activités simples mais nécessaires et dont on n’a pas de solution générique à réutiliser (ou alors ça coûte moins cher de le faire soi-même) : c’est du ETL (extract, load, transform), ou encore CRUD.\nVu la simplicité, ils ne peuvent pas procurer d’avantage compétitif.\nVu qu’il n’y a pas d’avantage compétitif, on préfère appliquer l’effort sur les core subdomains qui apporteront plus de valeur business.\n\n\n\n\nCôté stratégie :\nLes core subdomains doivent être développés au sein de l’entreprise, par les développeurs les plus expérimentés, et en appliquant le maximum de qualité et les techniques d’ingénierie les plus avancées.\nOn ne peut pas les acheter sinon on perd la notion d’avantage compétitif, et il ne serait pas très malin de les sous-traiter.\n\n\nLes generic subdomains étant difficiles mais déjà résolus, il est plus rentable de les acheter (ou de faire appel aux services d’un consultant spécialisé), ou d’utiliser une solution open source.\nLes supporting subdomains sont simples et changent peu, donc ils peuvent être implémentés avec moins de qualité, ou moins de techniques sophistiquées (design patterns, techniques d’architecture etc.). Un simple framework rapide suffit.\nOn peut laisser les débutants se charger de ça, ou alors on peut le sous-traiter.\n\n\n\n\n\n\nPour trouver les subdomains, il va falloir faire l’analyse nous-mêmes.\nOn peut déjà partir des départements qui composent l’entreprise, mais ça nous donne des subdomains grossiers.\nOn peut alors “distiller” les subdomains en plus petits subdomains : on prend un département et on liste les sous-activités, puis on analyse pour chacune d’entre elles si elles sont core, generic ou support.\nOn peut distiller au maximum jusqu’à arriver à “un subdomain comme un ensemble cohérent de cas d’usages” : un même acteur qui fait plusieurs tâches précises.\nIl faut faire la distillation maximale pour les subdomains core, pour pouvoir éliminer les petits bouts génériques ou support à l’intérieur et se concentrer uniquement sur ce qui a le plus de valeur.\nPour les autres on s’arrête de distiller à partir du moment où une distillation donne des activités du même type (generic ou support), aller au-delà ne nous permettra de toute façon pas de prendre des décisions plus stratégiques.\n\n\n\n\nLes devs vont devoir collaborer avec les domain experts, mais qui sont-ils ?\nCe ne sont pas les analystes qui recueillent le besoin, ni les ingénieurs qui créent le système. Ces derniers transforment le modèle mental des domain experts pour en faire du logiciel.\nCe sont soit ceux qui arrivent avec les besoins (qui sont là depuis le début, qui ont créé l’activité etc.), soit les utilisateurs finaux dont on résout le problème.\nA noter que les domain experts peuvent très bien n’être experts que d’un sous-domaine seulement.","2---discovering-domain-knowledge#2 - Discovering Domain Knowledge":"Généralement, les gens du business (les domain experts) communiquent les besoins à des intermédiaires (system/business analysts, product owners, project managers), qui vont ensuite communiquer ça aux ingénieurs logiciel qui créent le logiciel.\ndomain experts -> gens au milieu -> software engineers\nOn assiste aussi à une ou plusieurs transformations :\ndomain knowledge -> analysis model -> software design model\n\n\nLe DDD propose d’arrêter les transformations, et que tous les acteurs utilisent le même langage pour se parler : l’Ubiquitous Language.\nIl doit pouvoir être compris par les domain experts, donc il va se baser sur les termes qu’ils utilisent déjà.\nChaque terme doit être précis et sans ambiguïté, et il ne doit pas y avoir de synonymes.\nLa raison est que si dans les conversations courantes on arrive à se comprendre même avec des ambiguïtés en fonction du contexte, dans le cadre du logiciel ça marche moins bien.\n\n\nLe but de l’ubiquitous language est d’être un modèle du business domain, qui va représenter le modèle mental des domain experts.\nUn modèle n’a pas à représenter toute la réalité mais seulement une partie de manière à être utile. Et donc ici aussi il ne s’agit pas de représenter tout le business domain, ni de faire de tout le monde un domain expert.\n\n\nCôté outils :\nIl faut un glossaire sous forme de wiki, que tous font évoluer au fur et à mesure. On y met les termes de l’ubiquitous language.\nPour la logique il faut d’autres outils comme des description de cas d’usage, ou des tests Gherkin.\nExemple Gherkin :\nScenario: Notifier l'agent d'un nouveau cas\nGiven: Jules qui crée un nouveau cas \"Nouveau cas\"\nWhen: Le ticket est assigné à M. Wolf\nThen: L'agent reçoit une notification\n\nCeci dit, le plus important est l’usage quotidien de l’ubiquitous language par les acteurs. Les outils ne sont qu’un plus.\n\n\n\n\nPour que la compréhension du business domain soit bonne, il faut une communication régulière entre les domain experts et ceux qui sont en charge de réaliser le logiciel.\nCette communication se fait bien sûr dans l’ubiquitous language.\nÇa implique de poser beaucoup de questions aux domain experts.\nA force, on va se rendre compte qu’on pourra aider les domain experts à mieux comprendre leur champ d’expertise sur certains points, par exemple en pensant à des edge cases et pas seulement aux “happy paths”.\nOn peut alors aller vers une co-construction de la compréhension du domaine (même si les experts en sauront toujours plus).\n\n\n\n\nA propos de la langue de l’ubiquitous language, le conseil de Vlad est d’utiliser l’anglais au moins pour les noms d’entités du business domain.","3---managing-domain-complexity#3 - Managing Domain Complexity":"On se rend parfois compte qu’un même mot est utilisé par plusieurs domain experts avec une signification différente.\nExemple : le mot lead peut désigner un simple prospect intéressé dans le département marketing, alors que ça peut désigner une entité plus complexe avec des histoires de process de vente dans le département sales.\n\n\nEn général on a naturellement tendance à agrandir notre modèle pour que chaque version du mot puisse être représentée, mais ça finit par donner un modèle très complexe et difficile à maintenir.\nLa solution DDD est de créer deux bounded contexts qui auront chacun leur ubiquitous language.\n\n\nUn bounded context doit avoir des limites (boundaries).\nUne manière de les trouver est de repérer les incohérences de terminologies utilisées par les domain experts : un bounded context ne pourra pas être plus grand que ce que ces incohérences permettent.\nMais on peut diviser ces ensembles en bounded contexts plus petits. Il s’agit là d’une décision stratégique, qui dépend du contexte.\nAvoir des bounded context trop grands impliquera une plus grande complexité du modèle, alors que s’ils sont trop petits on aura un plus grand effort d’intégration à faire.\nLes raisons qui peuvent pousser à découper en bounded contexts plus petits peuvent être :\nd’ordre organisationnels : par exemple une nouvelle équipe qui va prendre en charge un morceau du logiciel qui va se développer sur un rythme différent du reste.\nd’ordre non fonctionnels : par exemple le fait de devoir scaler certaines parties du logiciel indépendamment.\n\n\nDe manière générale, on va essayer de trouver des fonctionnalités cohérentes qui opèrent sur un même jeu de données, et les laisser dans un même bounded context.\n\n\n\n\nLa différence entre subdomains et bounded contexts c’est que les subdomains sont là de fait et ressortent avec l’analyse. Ils sont de la responsabilité du business. Alors que les bounded contexts sont le résultat d’un choix stratégique, et sont de la responsabilité des ingénieurs logiciel.\nChaque bounded context a une séparation physique : il devra être implémenté avec sa codebase, en tant que projet autonome, et éventuellement service autonome.\nDans le cas où il s’agit d’un service autonome, on peut aussi choisir le langage, la stack, indépendants des autres.\n\n\nUn bounded context ne doit être géré que par une seule équipe de dev.\nUne équipe peut par contre en avoir plusieurs en charge.\n\n\nSi on se tourne vers la vie de tous les jours, on peut remarquer plein d’exemples où des mots ont des significations différentes selon les contextes, et où un modèle différent serait utile pour chaque :\nUn carton rectangulaire qu’on met au sol permet de représenter un réfrigérateur pour savoir s'il va bien s’insérer. Ce modèle est partiel mais il est utile pour ce qu’on veut faire, et c’est tout ce qui compte.\nEt si on a besoin de vérifier aussi la hauteur, on peut utiliser un autre modèle du réfrigérateur qui suffit : un simple mètre. => On a là une vision de l’approche DDD des modèles.","4---integrating-bounded-contexts#4 - Integrating Bounded Contexts":"Chaque bounded context permet de représenter un modèle au sens DDD, c'est-à-dire une abstraction utile pour résoudre un problème particulier.\nLes bounded contexts doivent quand même avoir des points de contact pour que le logiciel forme un tout, ces points sont appelés contracts.\nOn parle ici de points de contact au niveau du modèle (terminologie, concepts), mais le modèle se traduit à chaque fois en implémentation aussi.\n\n\nLa nature de ces contracts dépend de la nature de la relation entre les équipes qui gèrent les bounded contexts, il y en a de 3 types :\nCooperation : il s’agit soit de la même équipe, soit d’équipes qui ont une bonne communication et collaboration (par ex le succès de l’une dépend du succès de l’autre).\nPartnership : aucune équipe ne dicte sa loi à l’autre. Elles collaborent pour faire des changements dès qu’il y en a besoin d’un côté ou de l’autre.\nIl faut une intégration continue pour que les problèmes soient vite résolus.\nIl faut une communication vraiment au top entre équipes, et pas d'histoires de rivalités.\n\n\nShared Kernel : il s’agit d’un cas à part où les boundaries des bounded contexts sont violés : deux bounded contexts partagent une partie de leur modèle.\nPar exemple, ça peut être la partie d’authentification maison qu’ils auront en commun.\nLa partie partagée peut être changée par chaque bounded context, et affectera l’autre immédiatement. Il faut donc que des tests d’intégration soient déclenchés à chaque changement.\nBien réfléchir avant de l’utiliser : on l’utilise quand le coût pour appliquer les changements du modèle sous forme d’implémentation dépasse le coût de coordination entre équipes. C’est donc pertinent pour des modèles qui changent beaucoup.\nCas d’usage courants :\nQuand deux équipes n’ont pas une assez bonne communication, et que le modèle partnership donnerait lieu à des problèmes d’intégration.\nQuand on a un système legacy qu’on essaye de découper en bounded contexts, la partie pas encore découpée peut être temporairement partagée par les autres bounded contexts.\nQuand une même équipe a deux bounded contexts en charge, il est possible qu’avec le temps les frontières de ces deux bounded contexts s’estompent. Pour éviter ça on peut introduire une partie partagée qui va bien définir les frontières des deux autres.\n\n\n\n\n\n\nCustomer-Supplier : contrairement au modèle de type cooperation, la réussite de chaque équipe en charge de chaque bounded context n’est pas liée. On a donc un rapport de force qui va pencher vers l’une ou l’autre partie (le customer qui consomme ou le supplier qui fournit).\nConformist : la provider n’a pas vraiment de motivation à satisfaire le customer. Soit parce que c’est un provider externe, soit il est interne et c’est la politique de l’entreprise qui fait ça. Le customer va donc se conformer au modèle et aux changements de modèle du provider.\nLe supplier le fait soit parce que c’est un standard bien établi, soit simplement parce que ça lui convient.\n\n\nAnticorruption Layer : le provider a toujours le pouvoir, mais cette fois le consumer ne va pas se conformer, il va vouloir se protéger par une couche d’abstraction entre ce qui lui arrive et son propre modèle.\nLe consumer peut vouloir cette couche quand :\nIl contient un core subdomain, et que le modèle du provider pourrait être gênant.\nLe modèle du provider est bordélique, et on veut s’en protéger.\nLe modèle du provider change souvent, et on veut s’en protéger.\n\n\nGrâce à cette couche filtrante, le modèle du consumer n’aura pas à être pollué par des concepts dont il n’a pas besoin.\n\n\nOpen-Host Service : cette fois c’est le consumer qui a l’avantage parce que le provider a une motivation à satisfaire le consumer. Le provider va donc mettre en place une couche d’abstraction de son côté pour découpler son modèle interne du modèle public qu’il expose.\nC’est comme l’anticorruption layer, mais côté provider.\nLe provider peut éventuellement maintenir plusieurs OHS le temps que les customers migrent au nouveau modèle public.\n\n\n\n\nSeparate ways : les deux bounded contexts n'interagissent pas du tout. Si elles ont des fonctionnalités communes, elles les dupliquent de leur côté.\nÇa peut être quand la communication entre équipes est impossible.\nOu encore quand ça coûte plus cher de partager une fonctionnalité que de la refaire chacun de son côté. Parce que la fonctionnalité est trop simple, ou que les deux modèles sont trop différents.\n\n\n\n\nLe context map est une représentation graphique de chaque bounded context, avec les relations que chacun entretient avec les autres.\nÇa peut permettre de faire des choix stratégiques haut niveau.\nÇa peut faire apparaître des problèmes organisationnels :\nPar exemple si une équipe provider a tous ses consumers qui mettent en place un anticorruption layer. Ou encore si une équipe n’a que des relations separate ways avec les autres.\n\n\nLe context map doit être maintenu par l’ensemble des équipes.\nOn peut utiliser des outils comme Context Mapper, qui consiste à avoir un fichier source dans un dépôt de code, et permet de visualiser le graphique résultant avec une extension VSCode.","part-ii--tactical-design#Part II : Tactical Design":"","5---implementing-simple-business-logic#5 - Implementing Simple Business Logic":"Ce chapitre présente deux patterns tactiques simples. Ce sont des patterns connus, décrits notamment dans Patterns of Enterprise Application Architecture de Martin Fowler.\n1 - Transaction Script : il s’agit pour le provider de fournir une interface publique avec des endpoints qui ont un comportement transactionnel : soit une requête réussit entièrement, soit elle est entièrement annulée.\nMartin Fowler écrit à propos de ce pattern : Organizes business logic by procedures where each procedure handles a single request from the presentation.\nNotre fonction peut accéder à la base de données pour faire des manipulations, soit à travers une mince couche d’abstraction, soit directement. (Donc forte dépendance avec la BDD).\nDans le cas simple où on a une BDD relationnelle, il s’agit simplement d’utiliser la fonctionnalité native de transaction de sa BDD : on crée la transaction au début de la fonction, et on commit ou rollback à la fin.\nLa chose se complique quand notre BDD ne supporte pas les transactions, qu’on doit mettre à jour des données dans plusieurs BDD, ou encore qu’on est dans un contexte distribué où les transactions ne sont pas effectives immédiatement (eventual consistency).\n\n\nPotentiels problèmes qu’on rencontre souvent :\nOubli de transaction : parfois on oublie tout simplement d’utiliser le mécanisme de transaction au début de notre fonction. En exécutant plusieurs écritures en BDD on peut avoir la 1ère qui réussit et la 2ème qui échoue. On se retrouve alors potentiellement dans un système incohérent.\nSystème distribué : souvent dans les systèmes distribués on met à jour une donnée, et on prévient d’autres serveurs en leur envoyant un message. Il suffit que l’envoi de message ne marche pas pour que le système devienne incohérent parce qu’une partie du travail ne sera pas faite.\nIl n’y a pas de solution simple à ce problème, mais de nombreuses solutions qui font chacune du mieux qu’elles peuvent, en fonction du contexte.\n(pas dit dans le livre, mais c’est l’objet de Designing Data-Intensive Applications de Martin Kleppmann)\n\n\n\n\nTransaction Script est le pattern le plus simple, et convient pour les opérations simples de type ETL (extract, transform, load) où la logique business est elle-même simple et change peu souvent.\nIl convient bien :\nDans les supporting subdomains.\nComme couche d’adapter pour intégrer un generic subdomain.\nDans un anticorruption layer vis-à-vis d’un autre bounded context.\n\n\nPar contre sa simplicité fait que plus la logique business est complexe, plus il y a de la duplication de logique entre les diverses fonctions, et donc un risque d’incohérence au fil du temps. Donc il ne conviendra pas aux core subdomains.\n\n\nTous les autres patterns plus avancés, sont d’une manière ou d’une autre basés sur Transaction Script.\n\n\n2 - Active Record : on garde les caractéristiques transactionnelles du Transaction Script, mais on va ajouter des objets (les fameux Active Records) qui vont être en charge de transformer les données depuis la BDD vers des structures en mémoire qui conviennent mieux à ce qu’on veut faire dans nos fonctions.\nMartin Fowler écrit à propos de ce pattern : An object that wraps a row in a database table or view, encapsulates the database access, and the domain adds domain logic on that data.\nLa conséquence est que les fonctions appelées par les endpoints ne vont plus faire d’appel à la BDD, mais vont passer uniquement par les Active Records pour manipuler les données.\nL’Active Record va fournir des méthodes pour accéder aux données et les modifier, et va faire la traduction à chaque fois.\nOn encapsule la complexité de la transformation des données pour éviter de dupliquer cette logique-là un peu partout.\n\n\nActive Record est utile pour les cas d’usage simples, mais qui impliquent des transformations de données potentiellement complexes.\nIl convient donc lui aussi seulement aux supporting subdomains, ou à des couches d’adapter pour intégrer un generic subdomain ou un autre bounded context.\nQuand il est utilisé pour les logiques business complexes comme pour les core subdomains, on parle parfois d’un anemic domain model antipattern (et c’est déconseillé).\n\n\n\n\nIl s’agit cependant de rester pragmatique : bien qu’il soit souhaitable de protéger les données, il faut se demander à chaque fois quelles seraient les implications de la corruption de données dans tel ou tel projet.\nPar exemple, si on ingère des millions d’entrées venant de devices IOT dans notre base, risquer d’en corrompre 0.001% peut ne pas forcément être très grave.","6---tackling-complex-business-logic#6 - Tackling Complex Business Logic":"Ce chapitre présente le pattern du DDD pour les logiques business compliquées (à utiliser dans les core subdomains donc) : le domain model pattern.\nCe pattern est aussi dans le bouquin de Fowler, mais il a été développé plus en détail dans celui d’Eric Evans : Domain Driven Design: Tackling Complexity in the Heart of Software, sorti un an après (2003).\nAu niveau de l’implémentation :\nLe code du domain model est là pour représenter le domaine qui est déjà complexe, donc il ne doit pas introduire de complexité supplémentaire liée aux technos ou à l’infrastructure. => on utilisera donc du code pur (par exemple du TypeScript pur), pas de framework, d’ORM ou autre chose d’extérieur dans ce code.\nDébarrassé de tout ce qui est lié aux technos, le code peut utiliser l’ubiquitous language et épouser le modèle mental des domain experts.\nA noter que ce livre présente les choses d’un point de vue orienté objet, mais une approche fonctionnelle serait tout aussi valide, d’autres ressources dans la communauté du DDD vont dans ce sens.\n\n\nLes blocs qui composent le domain model sont :\n1- Value objects :\nCe sont des objets immutables identifiés par les valeurs de chacune de leurs propriétés. Si on veut le même objet avec une valeur différente pour une de ses propriétés, on crée de fait un autre objet.\nPar exemple, dans Python ou JavaScript, le string est un value object : on ne peut pas modifier une valeur string en elle-même, mais on peut créer de nouveaux strings avec des valeurs différentes (et qui auront bien une référence en mémoire différente).\nLes value objects n’ont pas d’attribut ID, puisque leur identification se fait par la composition des valeurs de leurs propriétés qui les rend uniques.\nEn revanche, il faut implémenter un opérateur d’égalité pour pouvoir comparer deux value objects entre eux.\n\n\n\n\nLe value object sera utilisé comme type pour les données à la place des types primitifs (comme string, number etc).\nL’utilisation systématique de types primitifs est connue comme “the primitive obsession”.\nPar exemple, plutôt que d’avoir email: string, on peut avoir email: Email, avec le value object Email qui va faire la validation et refuser toute valeur qui ne correspond pas.\nÇa évite de dupliquer la logique de validation partout, mais aussi d’oublier de faire cette validation à certains endroits.\n\n\nEn plus de la simple validation de données, le value object peut encapsuler en un endroit unique du code business lié au concept précis qu’il décrit. (Et c’est là qu’il “brille” vraiment)\nConcept qui est normalement dans le lexique de l’ubiquitous language de notre bounded context.\nPar exemple, un value object représentant une couleur peut avoir une méthode pour mixer la couleur avec une autre, et en obtenir une 3ème, selon des règles business bien particulières.\n\n\nEt ça augmente la clarté du code en rendant plus explicite l’intention.\n\n\nOn utilise les value objects dès que possible. Typiquement comme type des attributs des entities.\n\n\n2.1- Entities :\nCe sont des objets dont les propriétés peuvent changer (de type value object ou de type primitif), identifiés par leur ID qui lui ne change pas.\nExemple d’entity : une personne représentée par son nom, son email etc.\nLes entities sont importants, mais ils n’ont vraiment d'existence que dans le cadre d’un aggregate.\n\n\n2.2 - Aggregates :\nUn aggregate est une entity qui en contient d’autres.\nOn choisit un aggregate root qui est l’entity principale, et contient les autres.\nL’idée c’est de regrouper des entities qui sont amenées à changer ensemble.\n\n\nL’aggregate est responsable de garantir la consistance des données en son sein.\nLes objets extérieurs pourront donc seulement lire ses propriétés, mais pour les changer il faudra toujours passer par l’interface publique de l’aggregate.\nLes fonctions de cette interface publique s’appellent les commands.\nIl y a 2 manière d'implémenter ces commands :\nSoit comme méthodes publiques de l’aggregate root.\nSoit la logique se trouve dans des objets de commande spécifiques à l’aggregate et contenant la logique à appliquer. On instancie un tel objet command avec les infos qu’on veut changer, et on le passe à une méthode “execute” de l’aggregate root chargée d’appliquer la commande.\n\n\nLe fait que l’aggregate soit le seul responsable de modifier ses données fait que la logique business le concernant ne peut qu’être en son sein, et pas disséminée ailleurs.\n\n\nLa couche applicative (aussi appelée souche “service” : celle qui forward les actions de l’API publique au domain model) n’a donc plus qu’à :\nCharger l’état actuel de l’aggregate.\nExécuter la commande requise.\nEnregistrer le nouvel état en BDD.\nRenvoyer le résultat à l’appelant.\n\n\nAttention à la concurrence. Pour éviter les problèmes, la couche applicative devra s’assurer que l’état lu au début n’a pas changé entre-temps avant de valider le nouvel état de l’aggregate en BDD.\nPour ce faire, le moyen le plus classique est que l’aggregate ait une propriété “version” incrémentée à chaque modification.\nSi la version a changé entre-temps par une autre action concurrente, on pourra choisir d’annuler notre action, ou d’appliquer un mécanisme de résolution approprié à notre logique business (si on en est capable).\n\n\n\n\nL’aggregate doit avoir un comportement transactionnel : toute commande devra soit être intégralement exécutée, soit annulée.\nEn revanche, aucune opération ne pourra modifier plusieurs aggregates de manière transactionnelle : c’est une transaction par action sur un seul aggregate.\nSi on se retrouve à avoir besoin de modifier plusieurs aggregates en une transaction, c’est que les limites de nos aggregates ont été mal pensées : il faut les revoir.\nExemple d’aggregate : un ticket de support, qui peut contenir un ou plusieurs messages, chacun pouvant contenir une ou plusieurs pièces jointes.\n\n\nLa règle de base pour délimiter son aggregate c’est de le choisir le plus petit possible, et d’y inclure toutes les données qui doivent rester fortement consistantes (strong consistency) avec l’aggregate.\nDonc si en prenant en compte la logique business, certaines données liées à l’aggregate peuvent être un peu désynchronisées (et se mettre à jour un peu plus tard) sans causer de problèmes ( => eventual consistency), alors elles doivent être hors de l’aggregate.\nDans ce cas la bonne pratique est de placer dans l’aggregate une propriété représentant leur ID, là où pour une entity faisant partie de l’aggregate on placera en propriété une instance de celle-ci :\n// le customer ne fait pas partie de l'aggregate\nprivate customer: UserID;\n// les messages en font partie\nprivate messages: Message[];\n\nExemple d’aggregate qu’on délimite : imaginons qu’on a notre ticket de support, avec une règle business qui implique de le réassigner à un autre agent s’il n’y a pas eu de nouveau messages alors qu’il reste moins de 50% du temps restant pour le traiter.\nSi les messages mettent du temps à être à jour par rapport au ticket (c’est-à-dire qu’il y a une eventual consistency entre les messages et leur ticket), on aura du mal à respecter la règle business parce qu’on ne saura pas si il y a eu un message récemment ou pas. => Donc les entities “messages” vont dans le même aggregate.\nEn revanche, si les données des agents ou des produits sont légèrement en retard, ça ne nous empêchera pas de respecter notre règle business. => Donc ceux-là ne vont pas dans le même aggregate.\n\n\n\n\n\n\n2.3 - Domain Events :\nLes domain events sont des événements importants que chaque aggregate peut publier, et auquel d’autres aggregates peuvent souscrire.\nLeur nom est important, et il doit être formulé au passé.\nPar ex : Ticket assigned ou Ticket escaleted\n\n\nLeur contenu doit décrire précisément ce qui s’est passé :\nExemple :\n{\n\"ticketId\": \"7bfaaf0c-128a-46f8-99b3-63f0851eb\",\n\"eventId\": 146,\n\"eventType\": \"ticket-escalation\",\n\"escalationReason\": \"missed-sla\",\n\"escalationTime\": 65463113312\n}\n\n\n\n\n\n3 - Domain services :\nQuand on a de la logique business qui n’appartient à aucun aggregate ou qui semble être pertinente pour plusieurs aggregates, on peut la placer dans un domain service.\nLe mot service ici ne fait pas référence à autre chose qu’on appellerait habituellement des services, c’est simplement des objets sans état qui contiennent de la logique business.\nDans les domain services on peut toucher à plusieurs aggregates, mais seulement pour de la lecture : la contrainte de ne modifier qu’un aggregate par transaction tient toujours.\nExemple de domain service : le temps pour répondre aux tickets de support doit être calculé en fonction de plusieurs paramètres, dépendant de l’agent en charge, d’éventuelles escalations, des shifts horaires etc. On peut donc avoir une fonction qui va lire dans plusieurs aggregates des informations, calcule le temps et le renvoie.\n\n\n\n\nAu fond, l’idée des value objects et des aggregates c’est de réduire le degré de liberté du système en encapsulant des morceaux de logique dans des endroits uniques, pour en réduire la complexité.","7---modeling-the-dimension-of-time#7 - Modeling the Dimension of Time":"On peut appliquer l’event-sourcing au domain model pattern pour créer un event-sourced domain model pattern.\nAvec l’event sourcing on introduit la dimension du temps dans notre modèle.\nIl s’agit de reprendre les mêmes blocs que le domain model pattern, mais avec une différence au niveau de la manière de stocker les informations en base de données (et c’est ça qui caractérise l’event sourcing) : on va stocker les domain events des aggregates comme source de vérité, au lieu de l’état actuel de l’aggregate.\nOn appelle cette BDD d’événements l’event store.\nLa BDD doit au minimum permettre de :\nrécupérer tous les événements d’un même aggregate\najouter des événements à un aggregate\n\n\n\n\n\n\nIl s’agit de rejouer l’ensemble des domain events d’un aggregate à chaque fois qu’on le charge en mémoire pour obtenir son état.\nUne fois qu’on a son état, on peut exécuter la logique business comme avant, en jouant la bonne commande sur l’aggregate.\nNotre aggregate génèrera alors des domain events, mais ne modifiera jamais son state autrement que par ces events.\nLa persistance se fera en persistant les domain event dans l’event store.\n\n\nAvec le domain model pattern classique on avait aussi des domain events qui informaient les composants extérieurs des changements importants, mais là la différence c’est que ces events doivent être émis pour chaque changement d’état de l’aggregate.\nCôté code, on a notre objet aggregate root avec ses propriétés de type value object ou entity, et cet objet a une méthode apply(event: EventType) qui est surchargée plusieurs fois pour chaque type d’event qui peut survenir.\npublic class Person {\npublic id: PersonId;\npublic name: Name;\npublic email: Email;\npublic version: number;\n\nfunction apply(event: PersonCreated) {\nthis.id = event.id;\nthis.name = event.name;\nthis.email = event.email;\nthis.version = 0;\n}\nfunction apply(event: EmailUpdated) {\nthis.email = event.email;\nthis.version += 1;\n}\n}\n\nLe constructeur de la classe prend la liste d’events et la rejoue dans l’ordre à chaque instanciation.\n\n\nOn peut rejouer la projection des événements avec des variantes :\nEn rejouant les X premiers événements on remonte à n’importe quel état précis dans le temps.\nEn nous intéressant seulement à certains événements et en implémentant des actions particulières dans les fonctions apply de ceux-ci, on peut rechercher des données particulières et faire de l’analyse de données.\nPar exemple, si on cherche toutes les personnes qui ont pu avoir une modification de leur email plus de 3 fois en une journée, il suffit de créer une classe qui surcharge apply pour l’event EmailUpdated, et de rejouer les événements en comparant les dates de changement dans notre méthode apply.\n\n\n\n\nAvantages et inconvénients :\nAvantages de la version event-sourced par rapport à la version classique :\nOn peut remonter dans le temps. Par exemple pour analyser, ou encore pour débugger en allant à l’état exact qui a causé le bug.\nOn a du deep insight sur ce qui se passe. Particulièrement utile pour les core subdomains qui sont complexes.\nOn a une forte traçabilité de tout ce qui se passe, très utile dans un contexte d’audit, par exemple si la loi l’oblige comme avec les transactions monétaires.\nQuand deux transactions écrivent la donnée de manière concurrente, on va habituellement annuler une des deux transactions. Ici on a des infos plus fines sur les deux changements concurrents, et donc on peut plus facilement prendre des décisions business sur la manière de les concilier au lieu de les annuler.\n\n\nDésavantages :\nCourbe d’apprentissage pour l’équipe : il faut du temps et la volonté pour l’équipe d’apprendre ce paradigme.\nL’évolution du modèle est plus compliquée : on ne change pas un schéma d’events comme on change la structure d’une BDD relationnelle.\nA ce propos il y a le livre Versioning in an Event Sourced System de Greg Young.\n\n\nLa complexité globale de l’architecture est plus grande.\n\n\nRDV au chapitre 10 pour avoir des règles basiques sur le type de design à choisir.\n\n\nA propos de la performance :\nOui rejouer les events à chaque fois qu’on instancie un objet affecte la performance, mais dans les faits :\nIl faut bien noter que les events sont propres à chaque aggregate, et que ceux-ci ont un cycle de vie.\nPar ex un ticket de support passera par plusieurs étapes, et finira par être fermé, donc n’aura plus de changements d’état.\n\n\nEn deçà de 10 000 events par aggregate, l’impact de performance ne se ressent pas.\nLa moyenne du cycle de vie des objets dans la plupart des systèmes est de l’ordre de 100 events.\n\n\n\n\nDans les rares cas où la performance est un problème, il y a la technique du snapshot.\nIl s’agit de générer de manière asynchrone une projection sous forme de BDD servant de cache, et ayant les données des états de chaque aggregates, mais aussi le numéro du dernier event pris en compte.\nQuand l’application a besoin de charger un aggregate, elle le fait depuis ce cache, puis lit le numéro du dernier event pris en compte, et va chercher les events manquants dans l’event store pour les appliquer elle-même et obtenir l’état à jour de l’aggregate.\nAinsi donc, même si l’aggregate a 100 000 events, l’application n’a à chaque fois qu’à appliquer les quelques events à rattraper qui n’avaient pas encore été appliqués sur la BDD dénormalisée servant de cache.\n\n\nAttention cependant, avant d’utiliser une technique avancée comme celle du snapshot il faut :\nS’assurer qu’on a un vrai problème de performance (+ de 10 000 events par aggregate).\nVérifier les limites de notre aggregate et voir si il n’est pas trop gros et qu’on ne pourrait pas plutôt le scinder.\n\n\n\n\nL’event sourcing scale très bien : vu qu’on a une séparation claire des données d’event par aggregate, on peut très bien sharder par identifiant d’aggregate.\nLe CQRS (décrit au chapitre suivant) peut répondre à la problématique de performance de par sa nature.\n\n\nComment supprimer des données de l’event store (par ex pour des considérations légales) ?\nL’idée de l’event sourcing c’est qu’on ne supprime pas les données d’event, c’est la condition pour toujours pouvoir rejouer, mais aussi la condition pour qu’elles ne soient jamais corrompues.\nOn peut utiliser la technique du forgettable payload : on stocke dans notre event une version chiffrée des donnée sensibles, et on stocke une paire clé de chiffrement / ID de l’aggregate dans un stockage key-value à part. Quand on a envie d’effacer une des données sensibles, il suffit de supprimer sa clé.\n\n\nNe pourrait-on pas continuer à utiliser une BDD relationnelle comme BDD principale, et écrire en plus les events soit dans un log, soit en utilisant un database trigger pour écrire dans une table en même temps ?\nC’est une mauvaise idée parce que le log qui ne serait pas la source de vérité finirait par se dégrader au fil du temps, parce qu’on y ferait moins attention.\nEt en plus il est difficile de se donner du mal à bien représenter le sens des events dans ceux-ci si ils ne sont pas la source de vérité et que cette info n’est pas destinée à la BDD principale.","8---architectural-patterns#8 - Architectural Patterns":"Jusqu’ici on s’est intéressé aux patterns tactiques pour la logique business, ici on dézoome un peu et on s’intéresse à la relation entre les diverses parties du code du système (la logique business est une de ces parties).\nSi on ne maintient pas une limite claire entre le code business et le reste, on risque d’avoir une logique diffuse un peu partout.\nÇa rend le code difficile à changer parce qu’il faut passer partout.\nCa fait qu’on peut oublier certains endroits et avoir des inconsistances.\n\n\nOn voit dans ce chapitre 3 architectural patterns :\nLayered architecture\nC’est un des patterns les plus courants, il s’agit d’avoir 3 couches.\n1- la couche de présentation représente l’interface utilisateur, mais de nos jours c’est plus large : Interface graphique, interface en ligne de commande, API programmatique ou réseau.\nOn l’appelle parfois aussi la user interface layer.\n\n\n2- la couche business : c’est là où on a les patterns décrits avant : active record, domain model pattern etc.\nOn l’appelle parfois aussi le domain layer ou le model layer.\n\n\n3- la couche data access où on stocke et manipule des données dans des bases de données, dans du cloud etc.\nOn l 'appelle parfois aussi l’infrastructure layer.\n\n\nCôté dépendance, la couche de présentation n’a accès qu’à la couche business, et la couche business qu’à la couche data access : presentation -> business -> data access\nCe pattern est souvent étendu avec une couche additionnelle appelée service : elle va venir se placer entre la présentation et le business. presentation -> service -> business -> data access\nLe service layer permet de faire la logique d’orchestration autour de la couche business, et permet de découpler davantage les couches présentation et business.\nPar exemple, on peut avoir un controller (du pattern MVC) qui réceptionne un appel API, et qui va simplement faire appel au bon service, puis renvoyer la réponse de celui-ci. Le service quant à lui va commencer une transaction, faire appel par exemple à un objet Active Record, et soit valider soit annuler la transaction avant de renvoyer le résultat.\nLe service layer peut être utile dans le cas où la partie business est un Active Record, mais si c’est un Transaction Script alors il n’y a rien à abstraire vis-à-vis de la couche présentation.\nOn l’appelle parfois aussi l’application layer.\n\n\nAttention à ne pas confondre la layered architecture avec une architecture N-Tier :\nN-Tier fait référence au fait d’avoir des composants déployables indépendamment et donc qu’on considère séparés “physiquement”, qui n’ont pas le même cycle de vie : par exemple ce qui tourne dans un navigateur, ce qui tourne sur un serveur applicatif, le serveur de base de données etc.\nQuand on parle de layered architecture, on parle bien des couches logicielles au sein d’une même entité physique, avec le même cycle de vie.\n\n\nEtant donné la dépendance entre couche business et couche de data access, la layered architecture est mal adaptée à du code business sous forme de domain model pattern. Elle est par contre adaptée pour du transaction script ou de l’active record.\n\n\nPorts and adapters\nElle est bien plus adaptée au domain model pattern parce que le code business va être découplé du reste.\nOn va mettre le business layer d’abord, sans qu’il ne dépende de rien, et ensuite on va regrouper tout ce qui est communication avec la BDD, frameworks, providers externes etc. dans une couche qu’on appellera infrastructure. Et enfin on ajoute une couche application layer (c’est plus parlant que service layer) au milieu.\nBusiness layer <- Application layer <- infrastructure layer\n\n\nGrâce au Dependency Inversion Principle, on va à chaque fois injecter les dépendances des couches supérieures dans les couches du dessous : la couche business ne connaît pas la nature concrète de la couche application, et la couche application ne connaît pas la nature concrète de la couche infrastructure.\nC’est ici que la notion de “ports and adapters” prend son sens : On a des ports qui sont des interfaces mises en place par les couches du dessus, et des adapters qui sont des implémentations concrètes faites par les couches du dessous, et passés en paramètre des fonctions des couches du dessus.\nExemple :\n// le port (couche business)\ninterface IMessaging {\nvoid publish(Message payload);\n}\n\n// l'adapter (couche infrastructure)\npublic class SQBus implements Imessaging {\nvoid publish(Message payload) {\n// ...\n}\n}\n\nIci la classe adapter qui respecte le contrat (le port) établi par la couche business pourra être donnée en paramètre dans les fonctions de la couche business où elle accepte un objet Imessaging. Elle pourra utiliser l’objet sans savoir exactement ce qu’il fait (publier dans un bus, mettre l’info simplement dans une variable etc..).\n\n\n\n\nOn l’appelle aussi (avec quelques variations) : hexagonal architecture, onion architecture ou encore clean architecture.\nDans ces variantes, l’application est parfois appelée service layer ou encore use case layer.\nEt le business layer est parfois appelé domain layer, ou encore core layer.\n\n\n\n\nCQRS (Command-Query Responsibility Segregation)\nCette architecture est similaire au ports & adapters pour ce qui est de la place centrale du code business indépendant du reste. La différence se trouve dans la manière de gérer les données : on va vouloir adopter un “polyglot modeling”, c’est-à-dire plusieurs formes dénormalisées de la donnée pour la lecture.\nA l’origine le CQRS a été pensé pour répondre au problème de l’event sourcing, en fournissant la possibilité de lire le state actuel des données directement, au lieu d’avoir à repartir de la forme primaire (qui est la liste d’événements) et de rejouer tous les events pour obtenir le state actuel.\nMais il est utile aussi sans event sourcing, c’est-à-dire dans le cas classique où la BDD “source de vérité” est par exemple relationnelle.\n\n\nComme l’indique le nom, on va faire une ségrégation (une séparation) entre les commands (les écritures) et les queries (les lectures).\nD’un côté les commands sont faites vers la base de données principale, source de vérité, et fortement cohérente (les invariants sont respectés, les règles business validées etc.).\nDe l’autre les queries sont faites vers des bases de données dérivées, utiles pour chaque cas particulier.\nPar exemple des data warehouses sous forme de colonnes pour l’analyse, des BDD adaptées à la recherche, des BDD relationnelles pour avoir le state des aggregates tout de suite etc.\nLes BDD de lecture peuvent aussi être des fichiers, ou encore des BDD in-memory. Et on peut normalement les supprimer et les reconstituer à nouveau à partir de la BDD principale.\n\n\n\n\nIl faut que les BDD secondaires soient mises à jour à chaque fois que la BDD principale est modifiée. On appelle ça les projections.\nIl y a les projections synchrones :\nIl s’agit d’un modèle de catch-up subscription.\nQuand une requête est faite à la BDD de lecture, le système de projection va faire une requête vers la BDD principale pour obtenir tous les changements de la donnée voulue jusqu’au précédent checkpoint.\nLes nouvelles données sont utilisées pour mettre à jour la BDD de lecture.\nLe système de projection retient le nouveau checkpoint à partir duquel il faudra faire la prochaine mise à jour de la donnée en question.\n\n\nPour que ça puisse fonctionner, il faut que la BDD principale note toutes les modifications faites à l’ensemble des entrées qu’elle possède, avec des checkpoints (des versions) pour chacune.\nNDLR : Ca ressemble à des domain events parce que chaque changement est noté en BDD. Mais pour que ça en soit il faut aussi que la signification métier de chaque changement soit noté, et pas juste “telle entrée est modifiée à telle valeur”. C’est dans cette mesure que le CQRS peut être utilisé même sans event sourcing.\n\n\n\n\nEt les projections asynchrones :\nIl s’agit d’un mécanisme de souscription :\nLa BDD principale publie chacun de ses changements dans un message bus, auquel souscrivent les projection engines de chaque BDD secondaire..\nA chaque message chaque BDD secondaire est mise à jour.\n\n\nCe modèle est plus scalable et plus performant, mais il est soumis aussi à des problématiques de consistance des données des BDD secondaires, et une plus grande difficulté à les reconstruire.\nIl est donc conseillé de rester sur le modèle synchrone et éventuellement construire une projection asynchrone par dessus.\n\n\n\n\n\n\nLes queries ne peuvent pas écrire de données, par contre les commands qui écrivent les données peuvent aussi retourner des valeurs, notamment le statut de succès ou d’échec de la commande, avec l’erreur spécifique.\nLe CQRS est adapté au domain model pattern, il est utile quand on veut avoir plusieurs modèles de données adaptés à plusieurs besoins. Il est aussi adapté à l’event sourced domain model pattern pour lequel il est quasi obligatoire.\n\n\n\n\nCes patterns doivent être appliqués au cas par cas pour les besoins de chaque brique logicielle. Il ne faut pas choisir un seul pattern pour tout le système, ni même forcément un seul pattern pour un même bounded context, dans la mesure où plusieurs subdomains peuvent exister dans un même bounded context.","9---communication-patterns#9 - Communication Patterns":"Dans ce chapitre il est question des patterns de communication entre briques logicielles distinctes et ayant chacune leur bounded context. C’est la mise en application technique du chapitre 4.\nPour implémenter un anticorruption layer ou un open-host service, on peut procéder de plusieurs manières :\nEn mode stateless : chaque requête est autonome. Dans ce cas, le bounded context contient le layer de transformation.\nOn va utiliser le pattern Proxy pour que la requête passe d’abord par le layer (le proxy), puis soit traduite correctement vers la target.\nLa traduction peut être synchrone, auquel cas le proxy est dans le même composant.\nOn peut aussi vouloir utiliser un API gateway pattern, en ayant un composant externe tenant le rôle du proxy et permettant d’intégrer plusieurs flux vers une même target.\nDans ce cas, on pourra utiliser une solution open source comme Kong ou KrakenD, ou une solution payante comme AWS API Gateway, Google Api-gee ou Azure API Management.\n\n\n\n\nLa traduction peut aussi être asynchrone, auquel cas le proxy est un composant qui souscrit à des events, et envoie des messages.\nIl pourrait en plus filtrer certains messages non pertinents.\nDans le cas d’un Open-Host Service, le modèle asynchrone est très pratique parce qu’il permet de traduire correctement les événements du langage privé vers les événements du langage public.\nEt même de filtrer certains événements qui n’ont d’utilité qu’au sein du bounded context et pas pour l’intégration avec les autres.\n\n\n\n\n\n\nEn mode statefull où le layer possède sa BDD et implémente une logique complexe.\nOn a le cas où on veut agréger les données entrantes. Soit parce qu’on veut les traiter par batchs, soit parce qu’on veut créer un plus gros message à partir de plus petits messages arrivant.\nLe BDD propre au proxy sert alors à gérer ce processus d'agrégation.\nParfois on peut utiliser un outil tout fait comme Kafka, AWS Kinesis, ou une solution de batch comme Apache Nifi, AWS Glue, Spark.\n\n\nOn a aussi le cas où on veut unifier des sources de données. Par exemple un backend for frontend qui va chercher des données chez plusieurs autres bounded contexts pour les afficher.\n\n\n\n\nA propos de l’implémentation technique des domain events émis par les aggregates et consommé par les autres composants.\nUne façon de faire triviale mais mauvaise serait de laisser l’aggregate faire l’ajout de l’event dans le message bus.\nL’event serait émis avant même que la transaction (qui a lieu dans le layer applicatif, après que le code de l’aggregate ait été exécuté) n'ait été commitée. On pourrait alors avoir des composants avertis d’une chose qui n’est pas encore vraie en DB.\nVoir même qui ne sera jamais vraie en DB si la transaction échoue.\n\n\n\n\nUne variante serait d’émettre l’event dans le layer applicatif juste après le commit dans la BDD.\nMais là encore que se passerait-il si la publication de l’event ne marchait pas ? Ou encore si le serveur crashait juste avant la publication de l’event ? On aurait la transaction validée mais pas d’event pour prévenir les autres.\n\n\nLe pattern Outbox permet d’adresser ces cas.\n1- Le state et l’event sont ajoutés dans la BDD en une seule transaction.\nL’event peut être ajouté dans une table différente si c’est possible. Si c’est une database NoSQL qui ne le permet pas, alors on peut le rajouter dans l’état de l’aggregate.\n\n\n2- Un message relay va s’occuper de traiter les events non publiés de la BDD en envoyant le message dans le message bus.\nLe relay peut soit poller régulièrement la BDD (il faut faire attention à ne pas la surcharger), soit on peut utiliser les fonctionnalités proactives de la BDD pour déclencher le relay.\n\n\n3- Une fois que l’event est envoyé, le message relay va enlever l’event de la BDD, ou le marquer comme publié.\nA noter que le relay garantit la publication du message au moins une fois : si il crash lui-même après avoir publié l’event mais avant de l’avoir enlevé de la BDD, il va le republier une 2ème fois.\n\n\n\n\n\n\nLe pattern Saga permet d’adresser les cas où on doit écrire dans plusieurs aggregates, vu que par principe on a décidé qu’une transaction ne devait écrire que dans un seul aggregate.\nLe saga va écouter des events émis par certains composants, et déclencher des commandes sur d’autres. Si la commande échoue, le saga est responsable de déclencher des commandes pour assurer la logique business.\nExemple : imaginons deux entities qu’on ne veut pas mettre dans le même aggregate parce qu'elles sont trop peu liées entre elles : une campagne de publicité et un publisher.\nOn veut que l’activation d’une campagne déclenche sa publication auprès du publisher, et qu’en fonction de la décision du publisher, elle soit soit validée soit annulée.\nLe saga va écouter l’event d’activation de l’aggregate de la campagne, et déclencher la commande de publication auprès de l’aggregate du publisher. Et il écoute aussi les events de validation ou d’annulation auprès de l’aggregate du publisher, pour déclencher une commande auprès de celui de la campagne en retour.\n\n\nLa logique de la saga peut aussi nécessiter de garder en mémoire son état, par exemple pour gérer correctement les actions de compensation. Dans ce cas les events de la saga peuvent être mis en BDD, et la saga peut être implémentée elle-même comme un event-sourced aggregate.\nDans ce cas il faut séparer la logique d’exécution des commandes de la mise à jour du state de la saga, et utiliser le même principe que pour l’outbox pattern. On aura un message relay qui garantira l’exécution de la commande même si on échoue à quelque étape que ce soit.\n\n\nAttention tout de même à ne pas utiliser les saga pour compenser des limites d’aggregates mal pensées. L’action du saga étant asynchrone, les données seront eventually consistent entre-elles. Les seules données fortement consistantes sont celles qui sont dans un même aggregate.\n\n\nLe Process Manager se base sur le même principe que la saga, mais là où la saga associe juste un event à une commande, le process manager va implémenter une logique plus complexe liée à plusieurs events pour choisir les commandes à déclencher.\nIl est implémenté sous forme de state-based ou event-sourced aggregate avec une persistance de son état en BDD.\nLà où la saga est déclenchée implicitement quand un événement qu’elle doit écouter se produit, le process manager est instancié par l’application et s’occupe de mener à bien un flow en plusieurs étapes.\nExemple : la réservation d’un voyage business commence par la sélection du trajet le plus optimal, puis la validation par l’employé. Dans le cas où l’employé préfère un autre trajet, le manager doit valider. Puis l’hôtel pré-approuvé doit être réservé. Et en cas d’absence d’hôtels disponibles, l’ensemble de la réservation est annulée.\nLà encore, vu qu’on a des commandes et un état, il vaut mieux implémenter l’outbox pattern pour jouer les commandes et mettre à jour l’état de manière sûre.","part-iii--applying-domain-driven-design-in-practice#Part III : Applying Domain-Driven Design in Practice":"","10---heuristics#10 - Heuristics":"Ce chapitre donne des heuristiques, c'est-à-dire des règles à appliquer qui marchent dans la plupart des cas.\nBounded contexts\nRecomposer différemment des bounded contexts quand on se rend compte que le découpage n’est pas bon coûte cher. Il vaut donc mieux commencer par des bounded contexts grands, et les redécouper plus finement par la suite.\nCette règle s’applique en particulier pour les bounded contexts contenant un core subdomain, qui change par nature beaucoup. Il peut être sage de laisser d’autres subdomains avec lesquels il réagit beaucoup dans le même bounded context.\n\n\nBusiness logic patterns\nSi le subdomain trace des transactions monétaires, ou doit permettre une traçabilité, ou encore des possibilités d’analyse approfondies des données, alors il faut choisir l’event sourced domain model pattern.\nSinon, si la logique business est quand même complexe (core subdomain), alors il faut choisir le domain model pattern classique.\nSinon, si la logique business inclut des transformations de données complexes, alors il faut choisir l’active record pattern.\nEt sinon, il faut se rabattre sur le transaction script pattern.\nConcernant le fait de savoir si une logique business est complexe :\nElle l’est si elle contient des règles business compliquées, des invariants et des algorithmes compliqués. On peut s’en rendre compte à partir de la validation des inputs (?).\nElle l’est aussi si l’ubiquitous language est compliqué. Et à l’inverse s’il décrit surtout des opérations CRUD, alors elle ne l’est pas.\nSi on se retrouve à avoir une différence entre le pattern qu’on veut utiliser pour répondre à la nature de la logique business, et la catégorie de notre subdomain, alors c’est l’occasion de s’interroger sur la pertinence de notre catégorisation.\nMais il ne faut pas oublier non plus que l’avantage compétitif peut ne pas résider dans la technique.\n\n\n\n\n\n\nArchitectural patterns\nL’event sourced domain model nécessite le CQRS. Dans le cas contraire, on est vraiment limité : on ne pourra qu’obtenir les entities par leur ID.\nLe domain model classique nécessite le ports & adapters, sinon on va avoir du mal à isoler la logique business de la persistance.\nL’active record va bien avec une layered architecture à 4 couches, avec la couche service qui va contenir la logique qui manipule l’active record.\nLe transaction script peut être implémenté avec une simple layered architecture à 3 couches.\nEnfin, même dans le cas où on n’a pas choisi l’event sourcing pour notre logique business, on peut quand même adopter le CQRS dans le cas où on aurait besoin d’avoir la donnée sous plusieurs formes différentes.\n\n\nTesting strategy\nLa testing pyramid (beaucoup de unit tests, moins de tests d’intégration, et peu de tests end to end) est bien adaptée aux domain model patterns (event sourced ou classique). Les value objects et les aggregates font de parfaits units autonomes.\nLe testing diamond (peu de test unitaires, beaucoup de tests d’intégration, et peu de tests end to end) est bien adapté à l’active record pattern. La logique business étant éparpillée à travers le layer service et business, il est pertinent de les tester ensemble. (avec la BDD du coup ?)\nLa reversed testing pyramid (peu de unit tests, plus de tests d’intégration, et beaucoup de tests end to end) est bien adaptée au transaction script. La logique business étant simple, et le nombre de couches faible, on peut directement tester de bout en bout.\n\n\nL’auteur a déjà rencontré des équipes qui utilisaient par exemple l’event sourced domain model pattern partout. Pour autant il ne le conseille pas, et a rencontré beaucoup plus d’équipes qui s’en sortaient bien avec ces règles d’heuristiques.","11---evolving-design-decisions#11 - Evolving Design Decisions":"Identifier les subdomains a des implications importantes. Mais il faut régulièrement questionner leur statut et éventuellement le réévaluer.\nCore -> supporting : Si l’avantage compétitif apporté par le subdomain n’est plus justifié, l’entreprise peut décider de réduire la complexité du subdomain au minimum pour se concentrer sur ceux qui ont plus de valeur.\nCore -> generic : On peut avoir un procédé qui constituait un avantage compétitif, qui se fait dépasser par un acteur qui se met à fournir le fournir sous forme de service sur le marché. Dans ce cas on se retrouve contraint d’utiliser sa solution pour rester à la page, et notre core subdomain devient un generic subdomain.\nSupporting -> core : Il s’agit de cas où un supporting subdomain dont la logique se complexifie. Si elle n’apporte pas d’avantage compétitif, alors il n’y a pas de raison à cette complexification et il faut la corriger. Sinon c’est qu’on se retrouve avec un core subdomain.\nSupporting -> generic : On peut imaginer qu’on ait créé un système simple pour gérer quelque chose. Puis il apparaît un système open source qui fait la même chose, mais a aussi des fonctionnalités que l’entreprise n’avait pas priorisé étant donné la faible valeur apportée par ce système. Mais si c’est à faible coût alors pourquoi pas : l’entreprise abandonne la solution maison pour la solution open source.\nGeneric -> core : Une entreprise utilisant une solution externe se retrouve limitée par celle-ci. Elle décide finalement d’implémenter une solution maison qui réponde vraiment à son besoin, et grâce à ça de mieux rendre son service. Un exemple est Amazon qui a créé une solution d’infrastructure web maison pour ses besoins, et a fini par la commercialiser sous le nom AWS.\nGeneric -> supporting : Pour la raison inverse de passer de supporting à generic, on peut décider que la solution open source ne vaut plus le coup parce qu’elle coûte trop cher à intégrer, et revenir à une solution maison simple.\n\n\nQuand ajouter des fonctionnalités devient douloureux, c’est un signe qu’on se retrouve avec des patterns tactiques qui ne sont plus adaptés à la complexité de notre logique business.\nPar exemple, si un supporting subdomain se met à avoir une logique de plus en plus complexe, il est peut être temps de le transformer en core.\n\n\nSi on a fait nos choix en conscience, et qu’on a connaissance des différents patterns existants, migrer n’est pas si difficile.\nTransaction script -> active record :\nIl s’agit dans les deux cas de logique procédurale. Quand on a des structures de données compliquées, il faut les repérer et les encapsuler la logique de lecture/écriture dans des objets.\n\n\nActive record -> domain model :\nOn pense à le faire quand on remarque que la logique business qui manipule les active records devient complexe, et qu’on se trouve face à des duplications qui mènent à des inconsistances à travers la codebase.\nOn identifie d’abord les value objects immuables dans notre logique. On identifie aussi la logique qui pourrait aller dans ces objets et on la migre dedans.\nEnsuite on essaye de délimiter les données qui doivent être mises à jour ensemble.\nOn peut mettre les setters des active records privés, et constater où ils sont appelés dans le code.\nOn peut alors déplacer tout le code qui est cassé à l’intérieur de l’active record. On obtient un bon candidat pour un aggregate.\nOn va ensuite examiner la hiérarchie d’entities qu’il faut construire, et extraire éventuellement plusieurs aggregates du code : un aggregate sera la plus petite unité dont les données doivent être fortement consistantes.\nEnfin on se débrouille pour que les méthodes de l’aggregate ne soient appelables que par l’aggregate root, et que seule l’interface publique soit appelable de l’extérieur.\n\n\n\n\nDomain model -> event sourced domain model :\nIl faut commencer par modéliser les domain events.\nLa partie la plus compliquée va être de gérer les events passés qui n’existent pas. On a deux manières de le faire :\nGénérer de fausses transitions passées.\nOn va analyser chaque aggregate et on va imaginer une manière réaliste d’obtenir l’état actuel par une succession de domain events successifs.\nOn va alors générer ces events en base et faire comme si notre aggregate avait toujours été event-sourced.\nL’avantage c’est qu’on pourra toujours faire des projections, y compris en partant de zéro.\nLe désavantage c’est que les events avant la migration seront inventés, et pourraient aussi induire en erreur sur une analyse.\n\n\nModéliser des events de migration.\nL’autre solution c’est d’accepter explicitement qu’avant un certain point on n’a pas de données. On va alors créer un event initial pour chaque aggregate, qui va simplement avoir pour effet de mettre la valeur actuelle de l’état de toutes les valeurs de l’aggregate.\nL’avantage c’est qu’on ne pourra pas avoir de fausses projections.\n\n\n\n\n\n\n\n\nLes changements organisationnels peuvent affecter les patterns d’intégration des bounded contexts :\nPar exemple, si une seule équipe gérait un bounded context avec plusieurs subdomains, l’arrivée d’une 2ème équipe mènera à la séparation du bounded context en deux, parce qu’un bounded context ne doit pas être géré par deux équipes différentes.\nAutre exemple : si un des bounded contexts est affecté à une autre équipe, et que la communication n’est pas bonne, on va passer du partnership à du customer-supplier.\nEt quand on a encore plus de problèmes de communication, il vaut parfois mieux dupliquer la fonctionnalité et passer sur du separate ways.\n\n\nIl faut prendre soin de la connaissance du domaine.\nEn particulier pour les core subdomains, la modélisation du domaine est complexe et change souvent. Il faut donc régulièrement revoir les value objects, les aggregates etc.\nParfois la connaissance du domaine se perd, la documentation n’est plus à jour, les gens bougent etc. Il faut alors la retrouver, par exemple avec des sessions d’event storming.\n\n\nA mesure que le code grossit, les décisions de design deviennent obsolètes et doivent être adaptées. Si on ne le fait pas, on va obtenir un big ball of mud.\nIl faut identifier et régulièrement éliminer l’accidental complexity résultant des décisions design obsolètes, et n’adresser que l’essential complexity inhérente au domaine avec les outils tactiques du DDD.\nLes subdomains qui deviennent de plus en plus gros peuvent être distillés pour en mettre en évidence d’autres. Il faut en particulier le faire avec les core subdomains pour que la partie core soit circonscrite à ce qui est vraiment nécessaire.\nLes bounded contexts peuvent aussi être revisités quand ils grossissent trop :\nOn peut simplifier le modèle du bounded context en extrayant un bounded context chargé d’un problème spécifique qui a grossi.\nParfois on se rend compte qu’un bounded context n’arrive pas à faire une action sans être dépendant d’un autre bounded context. On peut alors revoir ses limites pour augmenter son autonomie.\n\n\nLes aggregates doivent rester des unités incluant le plus petit set de données possible qui doivent être fortement consistantes entre elles.\nAu fil du temps on peut être amené à ajouter des choses dans des aggregates existants parce que c’est plus pratique.\nIl faut donc régulièrement revérifier le contenu des aggregates. Et ne pas hésiter à extraire des fonctionnalités dans un nouvel aggregate pour simplifier le premier.\nOn se rend souvent compte que le nouvel aggregate révèle un nouveau modèle et amène à la création d’un nouveau bounded context.","12---eventstorming#12 - EventStorming":"L’event storming est un atelier qui permet de modéliser ensemble un process business particulier.\nIl s’agit de construire la story du business process concerné, à travers une timeline d’events qu’on fait apparaître sur un tableau au cours de l’atelier.\n\n\nLes membres participant à l’atelier doivent être le plus divers possible (devs, domain experts, product owners, UI/UX, CSM etc.). Mais il vaut mieux ne pas dépasser 10 personnes.\nCôté matériel :\nÇa se passe sur un mur, avec un gros rouleau de papier sur lequel on va pouvoir coller des post-its.\nIl faut des post-its de couleur, chaque couleur représentant un concept particulier. Et des marqueurs pour écrire dessus.\nL’atelier dure 2 à 4 heures, donc il faut prévoir de quoi grignoter.\nIl faut une salle spacieuse, et rien qui ne gène pour que chaque participant accède librement au tableau.\nIl ne faut pas de chaises, les participants sont debout.\n\n\n\n\nL’atelier se passe en 10 étapes, pendant lequel le modèle est enrichi collectivement :\nÉtape 1 - Unstructured Exploration : il s’agit d’une étape en mode brainstorming : tous les participants prennent des post-its oranges, et écrivent dessus des events liés au process business auquel on s’intéresse.\nLes events sont collés sur le tableau sans se soucier de l’ordre ou de la redondance.\nLes events sont formulés au passé.\nExemple d’events : “Notification sent”, “Destination chosen”, “Order shipped”.\n\n\nÉtape 2 - Timelines : Les participants vont organiser les post-its dans le bon ordre, en commençant par le “happy path scenario”, puis par les cas d’erreur.\nOn peut tracer des flèches à partir d’un post-it pour mener à plusieurs flows de posi-its possibles.\nC’est aussi le moment d’enlever les doublons.\n\n\nÉtape 3 - Pain Points : On va marquer les points qui requièrent une attention particulière : des bottlenecks, des étapes manuelles à automatiser, de la doc ou du domain knowledge manquant etc.\nOn fait ça avec des post-its roses, tournés de 45° pour être “en diamant”.\nCette étape y est dédiée spécifiquement, mais le facilitateur doit aussi faire attention aux pain points levés tout au long de l’atelier et les marquer.\n\n\nÉtape 4 - Pivotal Events : On va s’intéresser à des events marquant une étape importante dans le processus, et tracer une barre verticale le long du tableau, pour délimiter l’avant et l’après cet event.\nExemple : dans un flow d’achat, “order initialized”, “order shipped” et “order delivered” peuvent être des events importants.\n\n\nÉtape 5 - Commands : On va ajouter des commandes sur des post-its bleus, juste avant l’event qui résulte de la commande.\nLes commandes sont formulées à l’impératif.\nExemple : “Publish campaign”, “Submit order”.\nSi l’actor (la persona du business domain, ex : client, administrateur etc.) qui émet la commande est évident, on l’ajoute sur le post-it de la commande avec un post-it jaune.\n\n\nÉtape 6 - Policies : Presque à chaque fois il y a des commandes qui n’ont pas d’actor associé. On peut alors leur ajouter une automation policy qui consiste à les déclencher lors d’un domain event, sans intervention manuelle.\nOn ajoute ces policies avec des post-its violets, et on va les placer entre la commande et l’event concernés.\nOn peut ajouter d’éventuels critères de déclenchement. Par exemple, si un event “complaint received” doit déclencher la commande “escalate” seulement si ça vient d’un client VIP, on peut mettre sur le post-it de policy que c’est seulement si client VIP.\nDans le cas où l’event et la commande sont loin, ne pas hésiter à tracer une flèche pour les joindre.\n\n\nÉtape 7 - Read Models : On ajoute les “read models”, c'est-à-dire les interfaces utilisateur (écran, rapport, notification etc.) utilisées par les actors pour prendre leurs décisions d’exécuter les commandes.\nIls seront ajoutés sur des post-its verts.\nExemple de read model : “Shopping cart”, pour un actor “customer”, qui va actionner la commande “submit order”.\n\n\nÉtape 8 - External Systems : On va représenter les intéractions avec des systèmes externes, c’est-à-dire qui ne font pas partie du domaine qu’on est en train d’explorer.\nIls seront ajoutés sur des post-its roses.\nÇa peut être un composant qui déclenche une commande, par exemple un CRM qui déclenche l’ordre d’expédier une commande?\nCa peut aussi être un composant qu’on va notifier, par exemple notifier le CRM suite à un event qui dit que l’expédition est approuvée.\nA la fin de cette étape, toutes les commandes devront avoir leur origine associée : soit un actor, soit une policy, soit un système externe.\n\n\nÉtape 9 - Aggregates : On va pouvoir organiser les commandes et events représentés en aggregates, sachant qu’un aggregate reçoit des commandes, et émet des events.\nOn va ajouter de longs post-its jaunes verticaux pour représenter l’aggregate. On va les placer entre les commandes et les events.\n\n\nÉtape 10 - Bounded Contexts : On va finalement essayer de regrouper les aggregates entre eux, soit parce qu’ils représentent des fonctionnalités liées entre elles, soit par leur couplage via des policies.\nCes groupes d’aggregates seront de bons candidats pour des bounded contexts.\nOn peut les matérialiser avec des pointillés entourant les groupes d’aggregates liés.\n\n\n\n\nLe créateur de l’event storming lui-même (Alberto Brandolini) dit qu’il s’agit simplement de conseil sur la manière de mener l’atelier. On peut très bien l’adapter comme ça nous convient.\nL’auteur du livre applique d’abord les étapes 1 à 4 sur le domaine tout entier pour obtenir une vision large du domaine et identifier les process business.\nEnsuite il organise un atelier pour chaque process business pertinent, en suivant cette fois toutes les étapes.\n\n\nLa vraie valeur de l’event storming c’est surtout le processus en lui-même, et le fait qu’il permette la communication entre les différentes parties prenantes, en leur permettant d’aligner leur modèle mental, de découvrir d’éventuels modèles en conflit, et de formuler un ubiquitous language.\nLes objets obtenus sur le tableau sont un bonus. Tous les ingrédients sont là pour implémenter un event-sourced domain model pattern si on est bien face à un core subdomain et qu’on veut faire ce choix.\n\n\nL’utilisation de l’event storming permet non seulement de construire l’ubiquitous language, et construire le domain model, mais il est aussi utile pour explorer de nouvelles fonctionnalités, retrouver de la connaissance du domaine perdue, onboarder de nouvelles recrues en leur donnant de la connaissance du domaine.\nEn revanche, l'event storming sera peu utile si le process business examiné est simple et évident.\n\n\nTips pour les facilitateurs :\nAu début de l’atelier, donner une vision d’ensemble du processus, et présenter les éléments de modélisation qui vont être utilisés tout au long (en construisant une légende avec les post-its de chaque couleur par exemple).\nSi on sent un ralentissement du dynamisme du groupe, on peut relancer avec une question par exemple, ou peut être que c’est le moment de passer à l’étape suivante.\nTout le monde doit participer, si certain(e)s sont en retrait, essayer de les inclure en leur posant une question sur l’état actuel du modèle.\nL’activité étant intense, il y aura au moins un break.\nIl ne faut pas reprendre l’activité tant que tout le monde n’est pas de retour.\nOn peut reprendre en récapitulant le modèle tel qu’il a été défini pour le moment.\n\n\nLes ateliers en remote sont plus difficiles à mener, l’auteur conseille de se limiter à 5 personnes.\nParmi les outils existants, miro.com est celui qui est le plus connu au moment de l’écriture du livre.\n\n\nPas besoin d’être ceinture noire pour faciliter une session : on apprend en faisant.","13---domain-driven-design-in-the-real-world#13 - Domain-Driven Design in the Real World":"Les techniques du domain driven design apporteront le plus de bénéfices aux brownfield projects : les projets ayant déjà un business et s’étant éventuellement embourbés sous forme de big ball of mud.\nPas besoin que tous les devs soient des ceintures noires du DDD, ni d’appliquer toutes les techniques que le DDD propose.\nPar exemple, si on préfère d’autres patterns tactiques que ceux du DDD, c’est tout à fait OK.\n\n\nIl faut d’abord commencer par l’analyse stratégique :\nD’abord comprendre le domaine avec une vue haut niveau (qui sont les clients, que fait l’entreprise, qui sont les compétiteurs etc.).\nPuis identifier les subdomains. Pour ça on peut partir de l’organigramme de l’entreprise.\nPour les core subdomains, on peut se demander ce qui différencie l’entreprise de ses compétiteurs :\nPeut-être un algorithme maison que les autres n’ont pas ?\nPeut être un avantage non-technique comme la capacité à embaucher du personnel top niveau, ou de produire un design artistique unique ?\nUne heuristique qui fonctionne bien est de trouver les composants qui sont dans le pire état : ceux qui sont devenus des big balls of mud, que les devs détestent mais que les dirigeants refusent de faire réécrire à cause du risque business.\n\n\nPour les generic subdomains il s’agit simplement de trouver les solutions prêtes à l’emploi, soit open source, soit payantes.\nLes supporting subdomains sont les composants restants.\nIls peuvent être dans un mauvais état mais suscitent moins de plainte de la part des devs parce qu’ils sont moins souvent modifiés.\n\n\nOn n’est pas obligés d’identifier tous les subdomains. On peut commencer par les plus importants.\n\n\nOn peut ensuite identifier et analyser les différents composants logiciels.\nLe critère ici c’est le cycle de vie : les différents composants sont ceux qu’on peut faire évoluer et déployer indépendamment des autres.\nOn peut alors regarder les patterns d’architecture utilisés pour chaque composant, et vérifier si un pattern complexe serait plus adapté, ou à l’inverse si on pourrait utiliser un plus simple ou même une solution existante.\nOn peut ensuite faire comme si ces composants étaient des bounded contexts, et tracer le context map avec la relation entre chacun d’entre eux.\nOn peut là aussi vérifier si les patterns d’intégration de bounded context sont améliorables : plusieurs équipes travaillant sur le même composant, implémentations de core subdomain dupliquées, implémentation de core subdomain sous-traitée, frictions à cause d’une mauvaise communication entre équipes etc.\n\n\n\n\nOn pourra par la suite utiliser l’event storming pour construire un ubiquitous language, et éventuellement retrouver de la connaissance du domaine perdue.\n\n\nEnsuite on peut mettre en place une stratégie de modernisation :\nIl ne s’agira pas de tout réécrire parce que ça marche rarement, et que c’est supporté par le management encore plus rarement.\nIl faut accepter que dans un grand système, tout ne sera pas bien designé, et se concentrer sur les composants qu’on estime stratégiques.\nMais pour ça il faut déjà s’assurer qu’on a bien une délimitation, au moins logique, entre les subdomains (code séparé sous forme de modules, namespaces, packages etc.).\nNe pas oublier aussi les bouts de code du subdomain qui seraient ailleurs, sous forme de stored procedures d’une BDD, ou sous forme de serverless functions.\n\n\nOn peut alors commencer à extraire des composants logiques en bounded contexts physiques, en commençant par ceux qui apportent le plus de valeur.\nIl faut ensuite bien examiner les composants extraits et réfléchir à comment les moderniser :\nIl faut examiner leur intégration vis-à-vis de la relation entre les équipes qui en ont la charge et de leur niveau de communication (partnership vers customer/supplier, shared kernel vers separate ways etc.).\nCôté tactique, il faut se concentrer sur les composants qui apportent beaucoup de valeur (core) mais dont l’implémentation ne serait pas adaptée et rendrait difficile la maintenance, en utilisant plutôt un domain model pattern.\nIl faut aussi ne pas oublier de construire un ubiquitous language avec les domain experts, notamment au travers d’ateliers d’event storming.\n\n\nConcernant la modernisation de chaque bounded context extrait, on peut procéder de deux manières :\nLe strangler pattern : on va créer un nouveau bounded context à côté de l’ancien, et toutes les nouvelles fonctionnalités iront dedans. On migrera aussi les anciennes progressivement jusqu’à ce qu’il ne reste plus rien du premier.\nOn peut mettre en place une façade devant les deux bounded contexts pour rediriger les appels vers l’un ou l’autre. Elle disparaît quand le composant legacy meurt.\nLes deux bounded contexts peuvent temporairement partager une même base de données (chose qu’on ne fait pas d’habitude pour deux bounded contexts).\n\n\nLe refactoring progressif : on va changer le modèle et les patterns progressivement au sein du bounded context comme expliqué au chapitre 11.\nIl faut procéder par étapes, et ne pas sauter d’un transaction script à un event sourced domain model par exemple.\nIl faut passer du temps à trouver les bonnes limites pour les aggregates, en particulier si on va jusqu’à l’event sourced domain model, les changer ensuite est plus difficile.\nL’introduction du domain model pattern elle-même peut se faire en plusieurs étapes : par exemple commencer par trouver les objets immutables pour les extraire en value objects.\n\n\n\n\n\n\nComment introduire le DDD au sein de mon organisation ?\nÉtant donné les changements importants, y compris organisationnels et d’implication des effectifs hors ingénierie, avoir un appui du top management peut beaucoup aider. Mais c’est plutôt rare.\nCeci dit, l'essentiel du DDD reste des pratiques d’ingénierie logicielle, donc on peut commencer à l’utiliser dans ses activités quotidiennes même sans mise en place à l’échelle de l’organisation.\nOn peut déjà commencer à construire un ubiquitous language et l’utiliser.\nÉcouter les domain experts parler, leur demander des clarifications, repérer les doublons et demander à ce qu’on n'utilise qu’un des termes.\nParler avec les domain experts plus souvent, pas forcément au cours de meetings formels. En général ils sont ravis de parler aux devs qui sont sincèrement intéressés par comprendre le domaine.\nUtiliser la terminologie qu’on a constitué dans le code, et dans tous les autres supports ou échanges.\n\n\nPour les bounded contexts, l’important est de comprendre les principes sous-jacents pour pouvoir les utiliser :\nPourquoi créer plusieurs modèles utiles à chaque problème ? Parce qu’utiliser un très gros modèle est rarement efficace.\nPourquoi un bounded context ne doit pas avoir des modèles en conflit en son sein : à cause de la complexité que ça engendre.\nPourquoi plusieurs équipes ne devraient pas travailler sur un même bounded context ? A cause de la friction et de la mauvaise collaboration que ça engendre.\n\n\nC’est la même chose pour les patterns tactiques : il faut comprendre la logique de chaque pattern et utiliser cette logique pour améliorer son design, plutôt que faire appel à l’argument d’autorité “DDD” qui ne mènera nulle part.\nPourquoi faire des limites transactionnelles explicites ? Pour protéger la consistance de la donnée.\nPourquoi une transaction DB ne peut pas modifier plus d’une instance d’un aggregate à la fois ? Pour être sûr que les limites de consistance sont correctes.\nPourquoi l’état d’un aggregate ne peut pas être modifié directement par un autre composant ? Pour s’assurer que toute la logique business est située au même endroit et pas dupliquée.\nPourquoi ne pourrait-on pas déplacer une partie de la logique d’un aggregate dans une stored procedure ? Pour être sûr de ne pas dupliquer la logique, parce que la logique dupliquée dans un autre composant a tendance à se désynchroniser et mener à de la corruption de données.\nPourquoi essayer d’avoir des limites d’aggregates petites ? Parce que des limites transactionnelles larges augmentent la complexité de l’aggregate et impactent négativement la performance.\nPourquoi, à la place de l’event sourcing, ne pourrait-on pas écrire la donnée dans un fichier de log ? Parce qu’on n’aura pas de garantie de consistance de longue durée pour la donnée.\n\n\nA propos de l’event sourcing en particulier, expliquer le principe aux domain experts, et en particulier le niveau d’insight qu’on acquiert sur la donnée, va en général les convaincre que c’est une bonne idée.","part-iv--relationships-to-other-methodologies-and-patterns#Part IV : Relationships to Other Methodologies and Patterns":"","14---microservices#14 - Microservices":"Un service est une entité qui reçoit des données en entrée et envoie des données en sortie. Ca peut être de manière synchrone comme avec le modèle request/response, ou asynchrone comme avec le modèles basé sur les events.\nIl a une interface publique qui décrit comment on peut communiquer avec. Elle est en général suffisante pour comprendre ce que le service fait.\n\n\nUn microservice est un service avec une petite interface publique.\nEn limitant son interface on le rend plus facilement compréhensible, et on réduit les raisons qu’il a de changer.\nC’est aussi pour ça qu’un microservice possède sa propre base de données et ne l’expose pas.\nNDLR : cette courte définition n’est pas partagée par tout le monde.\nDave Farley définit le microservice comme étant d’abord une unité déployable indépendamment, et donc conçue de telle manière qu’elle n’ait pas besoin d’être testée avec d’autres unités. cf. vidéo 1, vidéo 2\n\n\n\n\nQuand on se pose la question d’à quel point notre service devrait avoir une petite interface, il faut prendre en compte la complexité locale (la complexité interne de chaque microservice) et la complexité globale. (la complexité de l’ensemble résultant de l’interaction entre microservices).\nPour réduire au maximum la complexité globale, il suffit d’implémenter l’ensemble sous forme d’un unique service monolithique. La complexité locale est alors maximale, et le risque est de finir avec un big ball of mud.\nPour réduire au maximum la complexité locale, on pourrait mettre chaque fonction dans un microservice, avec sa base de données. La complexité globale est alors maximale, et on risque alors de se retrouver avec un distributed big ball of mud.\nIl s’agit donc de trouver un juste milieu entre complexité locale et globale.\nL’auteur évoque le concept de depth proposé par John Ousterhout dans son livre The philosophy of Software Design : un deep module a une petite interface mais une plus grande implémentation, alors qu’un shallow module a une grande interface comparé à son implémentation (donc beaucoup de choses sont exposées).\nC'est la même chose pour les microservices, il faut les concevoir avec l’interface la plus petite possible, tout en ayant une implémentation importante en comparaison. Il faut que le microservice en tant qu’entité d’encapsulation encapsule des choses, sinon il ne fera qu’ajouter de la complexité accidentelle.\n\n\n\n\nLes microservices sont souvent confondus avec les bounded contexts.\nIl est vrai qu’un microservice est forcément un bounded context : il ne peut pas être géré par deux équipes, et il ne peut pas y avoir plusieurs modèles en conflit en son sein.\nEn revanche, un bounded context peut être plus large que ce qui serait raisonnable pour un microservice, il peut contenir plusieurs subdomains tant qu’un même modèle (et un même ubiquitous language) permet d’en rendre compte.\nDu coup :\nDes ensembles trop vastes contenant des modèles en conflit mènent à du big ball of mud.\nDes ensembles moins vastes avec un modèle unique sont des bounded contexts.\nDes ensemble suffisamment petits pour avoir une petite interface publique sont des microservices et aussi des bounded contexts.\nSi on découpe au-delà, on tombe sur du distributed big ball of mud.\n\n\n\n\nParfois on a envie d’utiliser des aggregates comme microservices.\nIl s’agit de regarder le couplage entre l’aggregate et les autres composants du subdomain : s'il communique souvent avec d’autres composants, si le changer mènerait souvent à changer d’autres composants etc. Plus ce couplage est fort, plus le microservice résultant sera “shallow”.\nParfois l’aggregate est suffisamment indépendant et ça marche bien, mais la plupart du temps c’est une mauvaise idée : l’aggregate se trouve être un découpage trop petit et mène au distributed big ball of mud.\n\n\nEnfin, une autre possibilité est d’utiliser les subdomains pour les microservices.\nLes subdomains sont de bons candidats pour des deep modules.\nIls sont concentrés sur les use-cases plutôt que sur la manière dont ceux-ci seront implémentés.\nLes use-cases sont dépendants les uns des autres.\nLe subdomain agit sur un même jeu de données.\n\n\nChoisir les subdomains comme heuristique pour ses microservices est donc une bonne idée.\nParfois il sera préférable de choisir un ensemble plus vaste qui sera un bounded context, parfois un ensemble plus restreint qui sera un aggregate. Mais la plupart du temps les subdomains feront l’affaire.\n\n\n\n\nLes Open-Host Services, et Anticorruption Layers peuvent contribuer à réduire davantage l’interface publique des microservices, en n’exposant qu’une version réduite du modèle interne.\nDans le cas de l’ACL, il peut être mis dans un service à part, réduisant donc l’interface publique du service consommateur qui se protège.","15---event-driven-architecture#15 - Event-Driven Architecture":"L’event driven architecture est une architecture dans laquelle les composants souscrivent et réagissent à des événements de manière asynchrone, plutôt que sous forme de requête/réponse de manière synchrone.\nLe pattern saga en est un exemple.\n\n\nOn parle bien ici de communication entre composants (ie. bounded contexts). Alors que l’event sourcing porte sur l’utilisation d’events à l’intérieur du BC, l’event driven architecture s’intéresse aux events utilisés pour communiquer entre BCs.\nL’event et la commande sont tous deux des messages.\nL’event décrit quelque chose qui s’est déjà passé et ne peut pas être annulé ou refusé.\nLa commande décrit quelque chose qui doit être fait, et qui pourrait être refusé, auquel cas on peut déclencher des commandes de compensation.\n\n\nOn peut classer les events en 3 catégories :\n1- L’event notification est un event qui sert à notifier un composant de quelque chose, pour le pousser à faire une query dès qu’il est disponible.\nOn ne va pas mettre toute l’info dans l’event, étant donné qu’on s’attend à ce que le composant refasse une query quand il est prêt.\nAvantages d’obliger à faire une query :\nCa peut être bien niveau sécurité : notifier avec des infos non sensibles dans l’event, et vérifier avec une autorisation plus forte quand la query explicite est faite.\nÇa peut permettre au composant d’être sûr d’avoir des infos à jour au moment du processing vu que la query sera synchrone, contrairement à l’event.\nÇa peut permettre de mettre en place une opération bloquante du point de vue concurrence pour que ce soit fait par une seule instance.\n\n\nExemple : ici on n’a quasi aucune info à part l’ID et un lien pour en savoir plus.\n{\n\"type\": \"mariage-recorded\",\n\"person-id\": \"01b9a761\",\n\"payload\": {\n\"person-id\": \"01b9a761\",\n\"details\": \"/01b9a761/mariage-data\"\n}\n}\n\n\n\n2- L’event-carried state transfer (ECST) est un event qui va donner l’information complète permettant à un composant externe de maintenir un cache de l’état interne de nos objets.\nOn peut soit envoyer à chaque fois un snapshot complet de l’état d’un objet, ou alors ne renvoyer que les modifications de cet état et le receveur se débrouille pour maintenir la cohérence de l’état au fur et à mesure.\nAvantages de maintenir un cache à distance :\nOn est plus tolérant aux fautes : le composant qui nous consomme peut continuer à fonctionner même si on est down.\nOn économise des requêtes : le composant consommateur n’a pas à faire de requête à chaque fois pour obtenir la donnée, et si elle ne change pas il n’y aura pas non plus d’event vu que le cache sera déjà à jour.\n\n\nExemple : ici on a la donnée qui a changé dans le state de la personne concernée, pour qu’on mette à jour notre cache.\n{\n\"type\": \"personal-details-changed\",\n\"person-id\": \"01b9a761\",\n\"payload\": {\n\"new-last-name\": \"Williams\"\n}\n}\n\n\n\n3- Le domain event décrit un événement lié au business domain, et qui s’est produit dans le passé. Il est là dans un but de modélisation du business domain et pas spécialement pour des considérations techniques vis-à-vis des autres composants.\nPar rapport à l’event notification :\nLe domain event inclut toute l’info nécessaire pour décrire l’event, alors que l’event notification non.\nLe domain event est utile en interne même si aucun composant externe s’y intéresse, alors que le but de l’event notification est uniquement l’intégration avec les composants externes.\n\n\nPar rapport à l’ECST :\nLe but est différent : le domain event décrit le fonctionnement du domaine, alors que l’ECST est là pour exposer l’état interne des objets pour des raisons techniques.\nEn s’abonnant à un type précis de domain event, on n’a pas du tout la garantie d’obtenir tous les changements d’un aggregate par exemple, contrairement à ce que permet l’ECST.\n\n\nExemple : on modélise l’événement qui s’est produit au plus près possible du domaine.\n{\n\"type\": \"married\",\n\"person-id\": \"01b9a761\",\n\"payload\": {\n\"person-id\": \"01b9a761\",\n\"assumed-partner-last-name\": \"true\"\n}\n}\n\n\n\n\n\nPour montrer qu’il ne suffit pas de saupoudrer un système d’events pour le transformer en event driven architecture, voici un exemple réel d’architecture produisant un distributed big ball of mud :\nImaginons un composant (bounded context) CRM utilisant l’event sourced domain model. Trois autres composants choisissent de consommer l’ensemble de ses domain events :\nLe BC Marketing les transforme en état, puis les utilise pour se mettre à jour.\nLe BC AdsOptimization fait quelque chose de similaire à Marketing pour d’autres besoins.\nLe BC Reporting les consomme, puis attend 5 mn avant de faire une query vers AdsOptimization, pour espérer qu’AdsOptimization aura déjà traité les données liées à cet event.\n\n\nOn se retrouve avec 3 problèmes :\nUn couplage temporel : Reporting attend 5 mn avant de faire sa requête, mais rien de garantit que ce sera suffisant. Si AdsOptimization est surchargé, qu’on a des problèmes réseau ou autres, Reporting risque de faire sa requête avant qu’AdsOptimization n’ait fini de processer les données liées à cet event.\nSolution : plutôt que de faire consommer les events de CRM à Reporting, on peut faire en sorte que ce soit AdsOptimization qui envoie un notification event à Reporting quand il a processé quelque chose de nouveau, pour que Reporting fasse sa requête.\n\n\nUn couplage fonctionnel : Marketing et AdsOptimization consomment tous deux les mêmes events, qu’ils transforment tous deux en objets avec état exactement de la même manière. Ça crée une duplication de logique dans ces deux bounded contexts.\nSolution : CRM peut implémenter l’open-host service pour y implémenter la logique qui était dupliquée dans Marketing et AdsOptimization. Comme ça les consommateurs ne s'encombrent pas d’implémenter ça.\n\n\nUn couplage à l’implémentation : vu que CRM est event-sourced, les autres BCs sont couplés à l’implémentation de CRM. A chaque fois qu’il change quelque chose, il faut qu’ils mettent à jour leur code aussi.\nSolution : CRM, bien qu’event-sourced, n’a pas besoin d’exposer tous ses domain events. Il peut choisir publiquement d’exposer seulement certains d’entre eux, ou choisir d’exposer un type d’event différent, par exemple ici des ECST vu que Marketing et AdsOptimization sont intéressés par le fait d’avoir un cache de l’état de certains des objets de CRM.\n\n\n\n\n\n\nQuelques bonnes pratiques pour l’event driven architecture :\nIl faut toujours s’attendre au pire : éviter le mindset “things will be ok” et partir du principe que tout peut échouer.\nOn parle d’architecture distribuée : le réseau peut avoir des problèmes, les serveurs peuvent crasher, les events peuvent arriver deux fois ou pas du tout etc.\nIl faut donc s’assurer à tout prix que les events arrivent bien correctement :\nUtiliser l’outbox pattern pour publier les messages.\nS’assurer que les messages pourront être dédupliqués ou réordonnés grâce à leur numéro s' ils arrivent dans le mauvais ordre.\nUtiliser les patterns saga et process manager pour orchestrer des process cross-bounded contexts qui nécessitent des actions de compensation.\n\n\n\n\nIl faut bien distinguer les events publics des privés : ne pas tout exposer tel quel mais traiter les events qu’on expose comme une interface publique classique.\nNe jamais exposer l’ensemble des domain events d’un event sourced bounded context.\nQuand on implémente un open-host service, bien s’assurer que le modèle qu’on veut exposer est bien différent du modèle interne à notre bounded context, ce qui peut impliquer de transformer certains events, et pas seulement les filtrer.\nExposer plutôt des ECST et des notification events, et des domain events avec parcimonie, en distinguant bien ceux qu’on expose.\n\n\nOn peut utiliser le besoin de consistance comme critère supplémentaire pour le choix du type d’event :\nSi l’eventual consistency est OK, on peut utiliser l’ECST.\nSi le BC consommateur a besoin de lire la dernière écriture dans le state du producteur, alors le notification event appelant à faire une query est sans doute plus adapté.","16---data-mesh#16 - Data Mesh":"Les transactions OLTP (T pour transaction) et OLAP (A pour analytics) ont des buts, des consommateurs et des temporalités différents.\nLes OLTP permet de servir les clients sur des opérations plutôt en temps réel, et dont les fonctionnalités sont connues et optimisées.\nLes OLAP sont là pour obtenir des insights à partir des données, et permettre à l’entreprise d’optimiser le business.\nIls prennent en général plus de temps parce qu’ils portent sur de grandes quantités de données, et utilisent des données moins à jour.\nIls portent sur des données normalisées qui permettent une grande flexibilité de requêtes de la part des data analysts.\nNDLR : les OLAP utilisent souvent des BDD orientées colonne plutôt que lignes, c’est-à-dire que les données de toutes les entrées d’une même colonne sont stockées physiquement les unes à la suite des autres, pour faciliter les requêtes qui portent sur peu de colonnes et beaucoup d’entrées. (cf. Designing Data-Intensive Applications).\n\n\n\n\nLes OLTP vont typiquement utiliser des données organisées sous forme relationnelle avec des entités individuelles reliées entre elles, pour faciliter le fonctionnement des systèmes opérationnels.\nLes OLAP en revanche s’intéressent plus aux activités business et vont utiliser un modèle de données basé sur les fact tables et dimension tables.\nLes fact tables sont des tables qu’on va remplir avec des événements liés au business qui se sont produits dans le passé.\nExemple : Fact_Sales peut être un fact table contenant une entrée pour chaque vente qui a eu lieu.\nIls sont choisis pour répondre aux besoins des data analysts qui vont utiliser la BDD.\nSelon les besoins, on peut choisir d’y mettre seulement certaines données, par exemple des changements de statut dont on garde seulement une entrée toutes les 30 minutes, parce que plus serait inutile ou inefficace pour ce qu’on veut.\nLes données y sont ajoutées pour être lues, on n’y fait pas de modification.\n\n\nLes dimension tables sont référencées par des foreign keys à partir de la fact table, et décrivent des propriétés du fact en question.\nExemple : Dim_Agents, Dim_Customers etc. qui vont chacun avoir leurs propres champs qui les décrivent.\nOn remarque bien la forte normalisation avec la fact table au centre, et les dimension tables autour, qui permettent de maximiser la flexibilité des requêtes possibles sur les données autour de ce fact.\n\n\nOn appelle ce modèle fact tables / dimension tables le star schema.\nIl existe une variante appelée le snowflake schema, où les dimensions sont sur plusieurs niveaux : les dimension tables ont elles-mêmes des foreign keys vers d’autres dimensions qui les décrivent.\nL’avantage du snowflake c’est qu’il prend moins de place pour les mêmes données, par contre il faudra faire plus de jointures et donc les requêtes seront plus lentes.\n\n\n\n\nLe data warehouse consiste à extraire les données des systèmes opérationnels, et les mettre dans une grande BDD avec un modèle orienté analytics (star, snowflakes etc.). Les data analysts et ingénieurs BI vont alors consommer cette BDD avec du SQL.\nOn a des flows de type ETL qui vont consommer les BDD mais aussi éventuellement des events, des logs etc. pour construire le data warehouse.\nIl peut y avoir de la déduplication, de l’élimination d’informations sensibles, de l'agrégation etc.\nLe data warehouse pose plusieurs problèmes :\nOn retombe sur la problématique d’un modèle unique pour régler tous les problèmes (dont on s’était sorti par la création de bounded contexts), qui est inefficace.\nUne des solutions qui a été trouvée c’est de créer des data marts qui vont constituer des BDD spécifiques, soit extraites à partir de la data warehouse, soit extraite directement à partir d’un système opérationnel.\nLe problème c’est que si le data mart est extrait de la data warehouse on ne règle pas le problème du modèle unique dont on dépend. Et si on extrait à partir d’un système opérationnel, on a du mal ensuite à faire des requêtes cross-database entre plusieurs data marts, à cause de problèmes de performance.\n\n\nOn a aussi un couplage à l’implémentation des systèmes opérationnels. Or être couplé à l’implémentation de gens à qui on parle peu c’est catastrophique. Dès qu’ils modifient leur schéma de données, ça casse les scripts d’ETL qui nourrit le data warehouse.\n\n\n\n\nLe data lake est censé résoudre certains de ces problèmes, en se plaçant entre les systèmes opérationnels et la data warehouse.\nElle centralise au même endroit les données opérationnelles sans changer leur schéma. Le data warehouse va alors pouvoir être rempli avec un ou plusieurs flows ETL à partir du data lake.\nQuand le modèle du warehouse n’est plus satisfaisant, les data analysts vont pouvoir piocher dans le data lake avec d’autres scripts ETL.\nIl y a cependant des problèmes :\nLe processus est plus complexe, et les data engineers se retrouvent souvent à maintenir plusieurs versions d’un même script ETL pour gérer plusieurs versions d’un système opérationnel.\n=> on n’a pas vraiment réglé le souci de couplage à une implémentation gérée par une autre équipe.\n\n\nLe data lake étant schema-less, il n’apporte pas de garantie vis-à-vis de la consistance des données venant des systèmes opérationnels.\nQuand les données deviennent grandes, le data lake se transforme en data swamp (marécage de données), rendant le travail des data scientists très difficile.\n\n\n\n\n\n\nLe data mesh tente de répondre à son tour à ces problématiques, en adoptant d’une certaine manière une approche DDD appliquée à la data.\nLe data mesh a 4 grands principes :\nDecompose data around domains :\nOn va prendre les bounded contexts qu’on avait créés, et y intégrer une partie data correspondant aux données issues des BDD opérationnelles de ce bounded context.\nL’équipe en charge du bounded context aura alors sous sa responsabilité à la fois la partie OLTP et la partie OLAP de son bounded context.\nOn élimine donc la friction qu’il y avait entre équipe feature et équipe data en intégrant une personne avec les compétences data dans l’équipe feature.\n\n\nData as a product :\nFini les scripts ETL douteux pour construire la data, chaque bounded context expose sa data proprement avec une API publique, à laquelle il accorde le même soin qu’une API publique destinée à être consommée par un composant OLTP du système.\nOn va mettre en place des SLA/SLO, on versionne le modèle de données exposé, les endpoints doivent être faciles à trouver et le schéma clairement défini etc.\n\n\nLa data étant un produit plutôt qu’un élément de seconde classe, chaque équipe gérant le bounded context a la responsabilité d’assurer la qualité et l’intégrité de la data exposée. Mais aussi servir la donnée sous les formats qui pourront intéresser les consommateurs (SQL, fichier etc.).\nL’idée c’est que les data analysts/BI puissent aller chercher facilement des données de plusieurs bounded context, appliquer éventuellement des transformations en local, et faire leur analyse.\n\n\n\n\nEnable autonomy :\nCréer une infrastructure pour gérer la data étant difficile, il faut une équipe centrale dédiée à l'infrastructure de la plateforme data.\nElle sera en charge de maintenir la plateforme qui permet aux feature teams de créer facilement leur produit data sous les différents formats possibles.\nPar contre elle reste la plus agnostique possible par rapport au modèle de données, qui est de la responsabilité des équipes qui produisent la donnée.\n\n\nBuild ecosystem :\nIl faut un corps de “gouvernance fédérale” pour penser l’écosystème data au sein de l’entreprise, et notamment la question de l’interopérabilité.\nCe corps est composé de représentants data et produit des équipes feature, et de représentants de l’équipe plateforme centrale.\n\n\n\n\nCôté relation avec le DDD :\nLe DDD aide à structurer la donnée analytique en amenant l’ubiquitous language et la connaissance du domaine.\nExposer une donnée différente de la donnée utilisée est l’open-host service pattern du DDD.\nGrâce au CQRS, on peut facilement mettre en place une ou plusieurs formes dénormalisées de plus qui auront un modèle analytique.\nLes patterns de relation entre bounded context s’appliquent aussi aux données analytiques (partnership, ACL, separate ways etc.).","appendix-a---applying-ddd-a-case-study#Appendix A - Applying DDD: A Case Study":"Vlad a passé quelques années dans une startup qui appliquait le DDD. Il s’agit ici d’une description de ce qu’ils ont fait, et des erreurs commises.\nL’entreprise portait sur le marketing en ligne, de la stratégie marketing aux éléments graphiques, campagnes marketing et appels des prospects récoltés.\nIl y avait aussi une importante partie data analytics sur l’optimisation des campagnes de marketing, le fait de faire travailler les agents sur les prospects les plus intéressants etc.\n\n\nLes 5 bounded contexts qu’il nous présente pour en tirer des leçons :\n1- Marketing : il s’agit d’une solution de gestion des campagnes.\nQuasiment chaque nom présent dans les requirements était un aggregate. Pour autant la plupart n’avaient que peu de logique, celle-ci étant essentiellement dans un énorme service layer.\nQuand on essaye d’implémenter un domain model mais qu’on finit avec un active record pattern, on appelle ça un anemic domain model antipattern.\n\n\nMalgré l’architecture mal adaptée, le projet a été un succès, grâce à l’ubiquitous language mis en place dès le début, et les conversations très fréquentes avec les domain experts.\n\n\n2- CRM : les sales l’utilisaient pour se répartir les prospects de manière optimisée.\nIls ont d’abord commencé à développer le CRM à l’intérieur du même monolithe que Marketing. Puis voyant que le modèle avait des incohérences, ils ont extrait CRM dans son propre bounded context (au niveau du code seulement).\nIls ont cette fois tenté d’utiliser le domain model pattern en mettant beaucoup de logique dans les aggregates, et chaque transaction n’affectant qu’un aggregate.\nLe tout prenant beaucoup de temps, le management a décidé de donner certaines fonctionnalités à l’équipe database, qui l’a fait sous forme de stored procedures.\nDeux équipes qui ne se parlent que peu, et qui travaillent sur le même bounded context : de la duplication de logique, des corruptions de données etc. la cata.\n\n\n3- Event Crunchers : ils ont remarqué que les événements venant des clients amenaient à modifier les deux bounded contexts, alors ils ont extrait la logique dans un 3ème.\nInitialement pensé comme un supporting subdomain, et développé avec du transaction script, la logique s’est vite complexifiée, et la qualité dégradée.\nFinalement au fil du temps la logique était devenue tellement complexe et bordélique, qu’ils ont dû le refaire sous forme d’event-sourced domain model, avec les autres bounded contexts souscrivant à ses events.\n\n\n4- Bonuses : il s’agit de calculer les bonus des sales.\nLà encore ça partait d’une logique simple, donc un supporting subdomain. Ils ont choisi d’utiliser l’active record pattern.\nLà encore la logique s’est complexifiée assez vite, grâce à l’ubiquitous language qui était en place avec les domain experts, ils ont pu se rendre compte que le modèle ne convenait plus à la complexité plus tôt que pour Event Crunchers : ils l’ont recodé sous forme d’event-sourced domain model.\n\n\n5- Marketing Hub : une nouvelle idée du management : se servir des nombreux prospects acquis pour les vendre à des clients.\nIls ont dès le début catalogué le subdomain comme core, et utilisé l’event-sourcing et CQRS.\nIls ont aussi utilisé les microservices, un concept qui devenait populaire à ce moment. Mais ils ont fait un microservice par aggregate, avec un aggregate event-sourced, et les autres state-based.\nAu fil du temps chaque micro service avait besoin de quasiment tous les autres, et ils se sont retrouvés avec un distributed monolith.\nFinalement, même si le subdomain était une source de profit et donc core, la partie logicielle en elle-même était très simple, et le pattern utilisé s’est révélé être largement overkill, amenant de la complexité accidentelle.\n\n\n\n\nL’ubiquitous language est selon Vlad le “core subdomain” du DDD : à chaque fois qu’ils l’ont bien mis en place, le projet a plutôt marché, et à chaque fois qu’ils ne l’ont pas fait, le projet a plutôt échoué.\nPlus on le met en place tôt, plus on évite des problèmes.\n\n\nLes subdomains sont aussi très importants. Mal les identifier amène à utiliser les mauvais patterns.\nVlad propose d’inverser la relation entre le subdomain et les patterns tactiques : d’abord choisir le pattern tactique qui convient le mieux au requirement, puis qualifier le type de subdomain, et enfin vérifier ce type avec les gens du business.\nSi le business pense que le subdomain est core, mais qu’on peut le réaliser facilement, on l’a peut être mal découpé et analysé, ou il faut se poser des questions sur la viabilité de l’idée business.\nSi le business pense que c’est supporting, mais qu’on ne peut le réaliser qu’avec des patterns complexes, alors :\nSoit le business s’enflamme sur les requirements et ajoute de l’accidental business complexity (mettre beaucoup de ressources sur une activité non rentable).\nSoit les gens du business ne se rendent pas compte qu’ils obtiennent un gain compétitif qu’ils n’avaient pas envisagé avec le subdomain en question.\n\n\n\n\nIl ne faut pas ignorer la douleur : si elle se manifeste, c’est sans doute que les patterns utilisés ne sont pas en phase avec la problématique business. Il ne faut pas hésiter à requalifier le subdomain.\n\n\nA propos des bounded contexts, comme évoqué précédemment, le mieux est de les choisir larges, puis de les découper quand le besoin se fait sentir, et que la connaissance du domaine augmente.\nFinalement la startup a été profitable assez vite, et a fini par être rachetée par un de leur gros client. Pendant ces années, ils ont été en mode startup : changer les priorités et requirements rapidement, des timeframes agressives, et une petite équipe R&D. Pour Vlad le DDD a rempli ses promesses."}},"/books/monolith-to-microservices":{"title":"Monolith to Microservices: : Evolutionary Patterns to Transform Your Monolith","data":{"":"","1---just-enough-microservices#1 - Just Enough Microservices":"Les microservices sont un type particulier de service-oriented architecture (SOA).\nIls exposent une API via le réseau, donc forment une architecture distribuée.\nIl sont déployables indépendamment :\nIl s’agit d’être en mesure de modifier et déployer un seul service sans toucher aux autres.\nLe conseil de l’auteur est d’effectivement déployer les services indépendamment, plutôt que le tout ensemble en espérant une indépendance théorique.\n\n\nIls sont organisés autour d’un business domain.\nLe but est de rendre les changements affectant plusieurs microservices le plus rare possible, et favoriser les changement à l’intérieur du microservice.\n\n\nIls gardent la base de données privée, et ne l'exposent que via une API.\nPartager la DB est une des pires choses à faire pour avoir une déployabilité indépendante.\nNe pas la partager telle quelle permet de décider ce qu’on partage et ce qu’on ne partage pas, et aussi de garder une API publique stable tout en étant libre de faire des changements en interne.\n\n\n\n\nL’exemple utilisé dans ce livre est une entreprise de vente de CD de musique.\nElle a une application organisée en 3 couches techniques : UI, backend, DB.\nChaque couche est sous la responsabilité d’une équipe : équipe front, équipe back, équipe DB.\nEt d’ailleurs cette l’architecture découle probablement de l’organisation des équipes cf. loi de Conway.\n\n\nOn a donc une forte cohésion au niveau technique : s’il faut faire un travail sur un aspect technique (par exemple moderniser la UI), une seule équipe sera impactée.\nMais on a une faible cohésion par domaine business, puisque l’ajout d’une fonctionnalité nécessite l’intervention et la coordination de 3 équipes.\n\n\nA l’inverse on peut imaginer une architecture organisée autour des domaines, avec un bout de UI, un bout de backend et un bout de DB chacun, et sous la responsabilité d’équipes pluridisciplinaires.\n\n\nLes microservices ont de nombreux avantages, et il s’agit de comprendre lesquels on cherche à obtenir en priorité pour orienter notre décomposition de monolithe.\nLa possibilité de scaler différemment des parties du système, et d’obtenir de la robustesse (le système peut continuer à opérer même si une partie est down).\nLa possibilité d’utiliser différentes stacks technologiques et de les faire communiquer ensemble.\nLa possibilité pour plusieurs équipes de travailler sur le système sans se marcher dessus.\n\n\nParmi les désavantages :\nLes problématiques des systèmes distribués : la communication réseau étant significativement plus lente que la communication in-process, et les paquets pouvant se perdre, on doit faire attention à beaucoup plus de choses.\nLes transactions deviennent problématiques.\n\n\nLes microservices arrivent avec leurs technologies spécifiques à maîtriser, qui peuvent causer bien plus de problèmes que les systèmes classiques si elles sont mal utilisées.\n\n\nLa UI ne doit pas être mise de côté dans la décomposition : si on veut pouvoir déployer rapidement des features complètes, il faut la décomposer elle-aussi pour qu’elle corresponde avec les services côté backend.\nL’auteur conseille de ne pas adopter une nouvelle stack technologique pour faire la migration vers les microservices. La migration en elle-même est déjà assez difficile, il vaut mieux garder les outils qu’on connaît dans un premier temps.\nA propos de la taille des microservices :\nC’est un des critères les moins importants, surtout quand on débute avec.\nIl vaut mieux s’intéresser d’abord à la question de savoir combien de microservices on sera capable de gérer dans l’organisation, et comment faire en sorte de ne pas trop les coupler.\nIl cite Chris Richardson qui parle d’avoir des microservices avec de petites interfaces.\nNDLR : c’est par cette idée que Vlad Khononov caractérise principalement les microservices dans Learning Domain Driven Design.\n\n\nL’idée initiale des microservices était de les avoir si petits qu’on pourrait facilement les recoder pour les remplacer (par exemple dans une techno qui permette plus de performance/scalabilité), mais l’auteur sous-entend que ce n’est plus vraiment un critère essentiel, en tout cas qui fait consensus.\n\n\nCôté ownership, l’architecture en microservices favorise le modèle où les équipes tech/produit sont au contact du client, et sont supportées par d’éventuelles équipes transverses.\nCa s’oppose au modèle plus traditionnel où le “business” gère la relation avec les clients, et où les développeurs sont dans un silo à part, sans ownership réel sur un business domain de bout en bout.\n\n\nLe terme monolith désigne ici l’unité de déploiement.\nLe single process monolith : il s’agit d’une app single-process, qu’on peut éventuellement dupliquer pour des raisons d’availability.\nEn général le monolithe va au moins communiquer avec une DB, formant un système distribué très simple.\nÇa représente l’essentiel des projets qui cherchent à migrer vers du microservice, donc l’auteur va se concentrer sur ça.\nIl est possible de réaliser un modular monolith en gardant le single-process, mais en créant des modules de code bien séparés.\nShopify est un bon exemple de modular monolith.\nOn a ceci dit souvent la DB dont le split en modules est négligé.\n\n\n\n\nLe distributed monolith : on a plusieurs services communiquant à travers le réseau, mais le système a besoin d’être déployé en un bloc.\nC’est un système qui a tous les désavantages : absence de modularisation, et système distribué.\n\n\nLes third-party black-box systems : les services externes SASS qu’on utilise, ou open source qu’on installe.\n\n\nLes monoliths ont un certain nombre de désavantages :\nLes diverses parties du code ont tendance à être plus facilement couplées.\nLe travail à plusieurs équipes est plus compliqué en terme de conflit de modification, en terme de confusion d’ownership, et aussi pour savoir quand déployer.\n\n\nConcernant les avantages :\nOn n’a pas tous les problèmes associés aux systèmes distribués.\nLe workflow de développement, le monitoring et le débug est plus simple.\nOn peut réutiliser du code très facilement.\n\n\nA propos du couplage et de la cohésion :\nLe couplage c’est l’idée que changer une chose implique d’en changer aussi une autre. La cohésion c’est le fait de garder ensemble des choses qui ont un rapport entre-elles (et qui d’habitude changent ensemble).\nPour avoir un système facile à transformer, on a envie que le couplage soit faible, et la cohésion élevée.\nPar exemple, si la logique d’une fonctionnalité est présente à travers plusieurs modules, on va devoir les changer tous pour la modifier (couplage élevé), et les éléments de cette fonctionnalité ne sont pas rassemblés (cohésion faible).\n\n\nDans le cas spécifique des microservices, les modules en question qu’il faut considérer en priorité sont les microservices eux-mêmes, puisque modifier leurs limites coûte très cher.\nOn veut donc faire en sorte que chaque changement impacte, et donc oblige le redéploiement, du moins possible de microservices.\nSi dans les microservices on peut se tromper dans les limites de chaque service, dans le monolithe ces limites n’existent pas naturellement, et donc on a tendance à avoir un couplage généralisé où tout dépend de tout.\n\n\nIl y a différents types de couplage :\nImplementation coupling : il s’agit d’un service qui doit changer quand on modifie l’implémentation d’un autre service.\nL’exemple typique c’est le couplage à la DB d’un autre service.\nLa solution c’est soit d’avoir une API pour accéder à la donnée, soit d’avoir une DB publique spécifique pour les consommateurs externes, distincte de la DB interne du microservice.\n\n\nAvoir une interface publique distincte permet aussi de concevoir cette interface pour répondre aux besoins des consommateurs, en mode outside-in, plutôt qu’imaginer ce qu’on veut exposer parmi ce qu’on a déjà.\nL’auteur conseille de toujours faire ça : impliquer les consommateurs dans le design de l’API publique, pour que le service les serve au mieux.\n\n\n\n\nTemporal coupling : il s’agit de communication synchrone dépendante d’autres communications.\nPar exemple, si un service envoie un message à un autre service, qui doit d’abord interroger un 3ème avant de répondre. Si le 3ème est down le 2ème ne pourra pas répondre.\nLa solution peut être pour le 2ème service d’avoir les données du 3ème en cache.\nUne autre solution pourrait être d'utiliser des communications asynchrones : le 3ème service reçoit le message asynchrone et recontacte le 2ème quand il est dispo.\n\n\nPour plus d’infos sur le type de communications, voir le chapitre 4 de Building Microservices.\n\n\nDeployment coupling : à chaque fois qu’on doit redéployer des services quand on en déploie un.\nIdéalement on veut pouvoir déployer le plus petit set de choses pour avoir peu de risques et un feedback rapide (et aller vers une continuous delivery).\nLes release trains sont une mauvaise idée.\n\n\nDomain coupling : il s’agit des interactions indispensables liées aux fonctionnalités elles-mêmes.\nOn ne peut pas les éliminer, mais on peut les agencer de telle sorte qu’elles aient un impact limité en termes de couplage.\nPar exemple, dans le cas de l’entreprise de vente de CD, le microservice de la commande doit communiquer au microservice de l'entrepôt quels CD ont été achetés et où ils doivent être acheminés.\nOn peut réduire au maximum les informations communiquées entre services, par exemple l’entrepôt recevrait seulement les données de packaging et pas l’ensemble des détails de la commande.\nOn peut faire en sorte que la commande inclut les infos nécessaires sur l’utilisateur (dont elle aura de toute façon besoin pour d’autres raisons) dans le message envoyé à l’entrepôt, plutôt qu’avoir l’entrepôt faisant un autre appel pour obtenir les infos de l’utilisateur.\nUne autre possibilité pourrait être que la commande émette un event, et que l'entrepôt le consomme.\n\n\n\n\n\n\n\n\nLe Domain Driven Design permet d’organiser les microservices efficacement autour de business domains.\nLes aggregates :\nOn peut les voir comme des représentations de choses réelles, avec un cycle de vie qu’on peut traiter avec une machine à état.\nPar exemple une commande, une facture, un objet en stock.\n\n\nUn microservice peut contenir un ou plusieurs aggregates.\nSi un autre microservice veut changer le contenu d’un aggregate, il doit soit envoyer un message au microservice qui en a la responsabilité, soit faire en sorte que ce microservice écoute des events que lui émet.\n\n\nIl y a de nombreux moyens d’organiser le système en aggregates, mais il vaut mieux commencer par celui qui colle le mieux au modèle mental des utilisateurs.\nL’event storming est un bon moyen pour ça.\n\n\n\n\nLes bounded contexts :\nIls permettent de cacher l’implémentation aux bounded contexts extérieurs.\nIls contiennent un ou plusieurs aggregates, dont certains peuvent être privés pour l’extérieur.\n\n\nConcernant la relation avec les microservices :\nAu début on cherche de gros microservices, donc les bounded contexts sont de bons candidats.\nA mesure qu’on avance, on va affiner nos microservices, et opter pour un aggregate par service.\nA noter que le groupe de microservices autour d’un bounded context peut cacher qu’il y a en fait plusieurs microservices (ce détail relevant de l’ordre de l’implémentation).\nNDLR : selon Vlad Khononov le microservice est de fait un bounded context, et va bien avec la taille d’un subdomain. Il ne peut pas être plus grand que le plus grand bounded context possible, ni plus petit qu’un aggregate. Mais la taille de l’aggregate marche rarement.","2---planning-a-migration#2 - Planning a Migration":"On peut vouloir adopter les microservices pour diverses raisons, et ces raisons peuvent fortement influencer ce sur quoi on va concentrer nos efforts.\nL’auteur pose en général 3 questions pour aider les entreprises à savoir si elles ont besoin des microservices :\nQu'est-ce que vous espérez accomplir ?\nOn devrait pouvoir trouver des choses qui sont alignées avec les besoins business et des utilisateurs finaux.\n\n\nEst-ce que vous avez considéré des alternatives ?\nComment saurez-vous si la transition fonctionne ?\n\n\nParmi les raisons de choisir les microservices :\nAméliorer l’autonomie des équipes / Scaler le nombre de développeurs.\nIl est notoire que les unités business autonomes sont plus efficaces. Et cette règle s’applique aussi à l’échelle de l’équipe, comme le modèle d’Amazon avec les équipes à deux pizzas.\nAvoir le contrôle exclusif sur des microservices permet aux équipes d’acquérir de l’autonomie, et de travailler en parallèle.\nAutres moyens d’obtenir ça :\nLe monolith modulaire peut répondre à ce point, avec une certaine coordination nécessaire quand même pour le déploiement commun.\nOn peut aussi penser à des approches self-service où on provisionne des machines automatiquement au lieu d’avoir à passer par un ticket manuel auprès d’une autre équipe.\n\n\n\n\nRéduire le time to market.\nLe fait que les microservices permettent de déployer sans besoin de coordination fait qu’on peut amener des changements en production plus vite.\nAutres moyens d’obtenir ça :\nL’auteur recommande de faire l’analyse concrète du chemin et du temps réel de chaque étape entre l’idée obtenue en discovery, et la feature en production.\nOn trouve souvent des bottlenecks qui permettent de gagner un temps conséquent.\n\n\n\n\nScaler efficacement la charge.\nComme les microservices tournent dans des processus différents, on peut les scaler indépendamment, et donc maîtriser les coûts de notre infrastructure.\nAutres moyens d’obtenir ça :\nOn peut essayer de passer sur une plus grosse machine (scaling vertical).\nFaire tourner plusieurs copies du monolithe, derrière un load balancer (scaling horizontal). Le bottleneck risque d’être la DB, mais ça ne coûte pas très cher d’essayer.\n\n\n\n\nAméliorer la robustesse.\nComme on a plusieurs unités indépendantes et tournant sur des machines séparées, on peut concevoir le système de sorte qu’il continue à fonctionner même si certaines parties sont en échec.\nAttention quand même : il y a tout un effort à faire pour obtenir cette robustesse, le fait de distribuer le système ne suffit pas à le rendre robuste.\nAutres moyens d’obtenir ça :\nFaire tourner plusieurs copies du monolithe permet de répondre à cette problématique. Y compris par exemple dans des racks ou datacenters différents.\n\n\n\n\nAdopter de nouvelles technologies.\nLes microservices étant isolés et communiquant par réseau, on peut très bien tester un nouveau langage, une nouvelle DB ou autre sur un seul microservice.\nAutres moyens d’obtenir ça :\nOn peut parfois switcher de langage, par exemple si on utilise la JVM, on peut basculer entre les langages supportés.\nPour les nouvelles DB c’est plus compliqué.\n\n\nOn peut toujours remplacer le monolithe par un nouveau avec une approche incrémentale type strangler fig.\n\n\n\n\nRéutiliser des composants.\nC’est une mauvaise raison.\nEn général on cherche à optimiser autre chose derrière la réutilisation, il vaut mieux se concentrer sur cette vraie raison.\nPar exemple, la réduction du time to market. Or le coût de coordination entre équipes peut impliquer que réécrire le composant serait plus rapide.\n\n\n\n\n\n\nQuand ne pas adopter les microservices :\nUn domaine pas très clair.\nDans le cas où on a un domaine encore jeune et pas très bien compris, la décomposition en microservices peut impliquer de se tromper de limites, et les changer coûte cher.\nEt donc typiquement il faut éviter les microservices dès le début.\n\n\nQuand on est une startup.\nLes microservices sont utiles pour les scale-ups ou les entreprises établies qui ont trouvé leur product market fit. Les startups le cherchent et donc seront amenées à beaucoup changer leur produit.\nOn peut éventuellement séparer ce qui est clairement à part dans un service, et laisser le reste dans le monolithe pour nous donner plus de temps pour le faire maturer.\nIl y a aussi la question de la capacité à gérer les microservices avec les effectifs de la boite : si on a du mal à en gérer 2, en gérer 10 va être vraiment difficile.\n\n\nQuand le logiciel est déployé chez le client.\nLe déploiement de microservices implique une grande complexité au niveau de l’infrastructure. On ne peut pas attendre des clients qu’ils puissent la gérer.\n\n\nQuand on n’a pas de bonne raison.\nMine de rien c’est un des cas les plus courants où les gens adoptent les microservices alors qu’ils ne devraient pas.\n\n\n\n\nOn a souvent plusieurs raisons d’adopter les microservices dans notre organisation. Il faut les prioriser.\nPar exemple, on décide qu’il nous faut des microservices pour gérer une augmentation de trafic. Puis on se dit que ce serait pas mal d’augmenter aussi l’autonomie des équipes, et d’adopter un nouveau langage.\nIl faut bien garder en tête que c’était l’augmentation du trafic qui était la plus importante. Et donc si on trouve un autre moyen plus simple de régler le problème, peut-être que les autres raisons devront attendre.\n\n\nUn bon moyen pour aider aux décisions est de représenter l’ensemble des raisons d’adopter les microservices avec des curseurs de 1 à 5 : si on augmente le curseur pour une raison, on doit le baisser pour une autre.\n\n\nPour réussir à créer un changement organisationnel (pour mettre en place des microservices ou autre chose), l’auteur propose la méthode en 8 étapes de John Kotter, décrite plus en détail dans son livre Leading Change.\nÉtape 1 : Establishing a sense of urgency. Le meilleur moment pour initier le changement c’est juste après une crise dont l’idée qu’on veut mettre en place règlerait le problème sous-jacent, avec l’idée “Il faut le mettre en place maintenant”.\nÉtape 2 : Creating the guiding coalition. On a besoin de convaincre des personnes autour de nous. En fonction de l’impact de notre idée, il faudra avoir des personnes plus ou moins haut placées, et typiquement des personnes du business dans le cas où on introduit des systèmes distribuées qui vont impacter les utilisateurs.\nÉtape 3 : Developing a vision and strategy. La vision définit le “quoi”, elle doit donner envie mais être réaliste. La stratégie définit le “comment”.\nÉtape 4 : Communicating the change vision. Il vaut mieux privilégier la communication en face à face (plutôt que slack ou ce genre de chose) pour pouvoir ajuster le discours en fonction des réactions.\nÉtape 5 : Empowering employees for broad-based action. Souvent les organisations amènent de nouvelles personnes dans l’équipe pour aider au changement en donnant de la bande passante.\nÉtape 6 : Generating short-term wins. Pour éviter que l’engouement retombe, il faut obtenir des quick wins. Ça peut être par l’extraction de microservices “faciles” (à condition qu’ils aient un intérêt quand même).\nÉtape 7 : Consolidating gains and producing more change. On continue avec des changements plus profonds en fonction des succès ou échecs. Ça peut être la décomposition de la DB qu’on ne peut pas mettre de côté éternellement.\nÉtape 8 : Anchoring new approaches in the culture. A force de pratiquer la nouvelle manière de faire, la question de savoir si c’est la bonne approche ou non disparaît. Elle devient habituelle.\n\n\nLa décomposition d’un monolithe étant une chose difficile, il faut qu’elle soit faite de manière incrémentale. On sort un service à la fois, et on obtient du feedback pour s’améliorer sur la suite.\nLe feedback en question est aussi précieux parce que la plupart des problèmes complexes liés aux microservices sont remarqués une fois que c’est déployé en production.\n\n\nUne des raisons de la méthode incrémentale est de rendre les erreurs réversibles.\nMais il y a des décisions qui sont plus impactantes que d’autres, et donc il faut adapter le temps passé à analyser à la facilité à annuler la décision.\nExemple : changer de fournisseur cloud ou changer l’API qu’on fournit publiquement est très impactant, alors d'expérimenter une librairie open source ou un nouveau langage beaucoup moins.\n\n\nCertaines décisions liées aux microservices peuvent être difficiles à défaire, par exemple annuler une migration de DB ou défaire la réécriture d’une API utilisée par de nombreux consumers.\nDans ces cas, l’auteur recommande d’utiliser un tableau blanc pour simuler les divers use-cases et leurs conséquences en terme de communication entre services, pour voir s’il y a des problèmes apparents.\n\n\n\n\nPour ce qui est de savoir où on commence, il nous faut une décomposition en composants business. Et pour ça on utilise le Domain Driven Design.\nLa notion de bounded context et les relations entre les BCs nous permet de représenter un découpage possible en microservices.\nOn n’a pas besoin d’un modèle super détaillé des BCs, mais d’avoir juste assez d’information pour pouvoir commencer à faire des choix. Et comme on procède de manière incrémentale, une erreur est vite rattrapée.\nL’event storming est un outil recommandé par l’auteur pour obtenir une connaissance partagée du modèle, et pouvoir faire des choix pertinents à partir de là.\nPour approfondir il y a Introducing EventStorming, le livre pas encore terminé d’Alberto Brandolini.\n\n\nPour prioriser, on peut se servir du context mapping (le nom n’est pas mentionné par l’auteur).\nUn BC qui a beaucoup de liens avec d’autres BCs ne sera peut être pas le bon premier candidat pour être extrait en microservice parce qu’il impliquera beaucoup de communications réseau.\nA noter que le context map qu’on a à ce stade ne représente pas forcément le vrai découpage. Il va falloir regarder dans le code et vérifier ce que le BC fait dans base de données.\nIl faudra aussi mettre la facilité d’extraction en balance avec l’utilité d’extraire ce BC là.\nPar exemple, si notre objectif c’est d’améliorer le time to market, mais qu’on commence par extraire un BC en microservice alors qu’il n’est presque jamais modifié, on n’aura pas beaucoup d’impact sur ce qu’on voulait faire.\nOn peut placer les BCs sur un graphique à deux axes : en abscisses l’intérêt de la décomposition, et en ordonnée la facilité de la décomposition.\nOn va choisir en priorité les BCs qui se retrouvent en haut à droite.\n\n\n\n\n\n\n\n\nA propos de l’organisation des équipes.\nHistoriquement les équipes étaient organisées par compétences techniques : devs Java ensemble, DBA ensemble, testeurs ensemble etc.\nPour intervenir sur une fonctionnalité il fallait passer par plusieurs équipes.\n\n\nDe nos jours, avec des mouvements comme DevOps, les spécialités sont poussées vers les équipes de delivery, qui sont organisées autour de domaines fonctionnels, en vertical slices.\nLe rôle des équipes centrales qui restent s’est transformé : au lieu de faire eux-mêmes, ils aident les équipes delivery, en y envoyant des spécialistes, organisant des formations, et en créant des outils self-service.\n\n\nPour aller plus loin, l’auteur recommande Team Topologies et The Devops Handbook.\nIl faut faire attention à ne pas chercher à copier tel quel les autres organisations, sans prendre en compte le contexte, la culture d’entreprise etc. On peut en revanche s’en inspirer.\nLe changement prend du temps. Par exemple, on peut intégrer des ops dans des équipes de dev pour former petit à petit chacun aux problématiques de l’autre.\nPour commencer le changement, on peut réunir des personnes de chaque équipe, et faire un mapping des responsabilités liées à la delivery, en fonction de chaque équipe.\nEt ensuite on peut planifier un changement de responsabilités liées aux équipes, et de la structure des équipes, sur 6 mois à un an par exemple.\n\n\nConcernant la montée en compétence nécessaire pour la nouvelle organisation, l’auteur préconise de laisser les développeurs s’auto-évaluer avec une note sur chaque compétence nécessaire, et de les aider ensuite sur celles où ils se sont mis un faible score.\nCes auto-évaluations devraient être privées pour ne pas être faussées.\n\n\n\n\nPour savoir si on va dans la bonne direction :\nIl faut avoir quelques métriques quantitatives et qualitatives liées aux outcomes qu’on recherche avec la transition qu’on a entamé.\nLes métriques quantitatives dépendent des objectifs.\nPar exemple, si c'est le time to market, on peut mesurer le cycle time, le nombre de déploiements et le failure rate.\nSi on cherche la scalabilité, on peut se reporter au dernier test de performance réalisé.\nAttention aux métriques : elles peuvent pousser à des comportements non souhaités pour satisfaire la métrique.\n\n\nPour ce qui est des métriques qualitatives, il s’agit de vérifier si l’équipe est contente ou pas, s’ils sont débordés etc.\n\n\nIl faut organiser des checkpoints réguliers pour voir où on en est.\nOn vérifie que les raisons pour lesquelles on a commencé la transition sont toujours là.\nOn jette un œil aux métriques quantitatives pour voir l’avancée.\nOn demande du feedback qualitatif.\nOn décide d’éventuelles actions.","3---splitting-the-monolith#3 - Splitting the Monolith":"Ce chapitre décrit des patterns pour migrer le code dans des micros de manière incrémentale.\nUn des critères à prendre en compte pour le choix des patterns c’est le fait qu’on ait ou non la possibilité de changer le code du monolithe.\nOn peut avoir de nombreuses raisons pour ne pas le pouvoir :\nSi on n’a plus le code source du monolithe.\nSi le monolithe est écrit dans une technologie pour laquelle on n’a pas les compétences.\nSi on a peur de trop impacter les autres développeurs du monolithe.\n\n\nDans le cas où on peut modifier le code du monolithe, si le code est en trop mauvais état, ça peut aussi parfois être plus rapide de le réécrire dans le microservice plutôt que de l’extraire.\n\n\nUne des grandes difficultés c’est d’isoler le code qu’on veut extraire dans notre microservice, c'est-à-dire modulariser le monolithe.\nEn général le code dans les monolithes est organisé autour de considérations techniques et pas de domaines métier, c’est pourtant ça qu’on veut extraire.\nPour aider à faire ça, l’auteur recommande le concept de seam, qu’on trouve dans Working Effectively with Legacy Code de Michael Feathers.\nUn seam est une délimitation autour d’une zone qu’on veut changer. On travaille ensuite à une nouvelle implémentation de la fonctionnalité, et à la fin on remplace l’ancienne par la nouvelle.\nÇa peut être plus ou moins grand, ici ce qui nous intéresse c’est un bounded context.\n\n\nRéorganiser le code pour obtenir un modular monolith peut être suffisant pour ce qu’on recherche, en fonction de nos objectifs (cf. chapitre précédent).\nEt ça peut aussi être une première étape pour aller vers l’extraction d’éventuels microservices ensuite. C’est en tout cas le conseil de l’auteur.\nPour autant, de nombreuses équipes préfèrent identifier une fonctionnalité, et la recoder dans un microservice sans refactorer le monolithe.\nDans tous les cas, l'auteur recommande une approche incrémentale : si la réécriture du service se compte en jours ou semaines ça peut être OK, si ça se compte en mois, il vaut mieux adopter une approche plus incrémentale.\n\n\n\n\nDans la suite du chapitre, on voit des patterns de migration, qui permettent d’extraire du code sous forme d’un microservice cohabitant avec le monolithe.\nChaque pattern a des avantages et des inconvénients, il faut les comprendre pour pouvoir prendre à chaque fois le plus adapté.\nOn extrait toujours les microservices un par un, en apprenant des erreurs pour le prochain.","pattern-strangler-fig-application#Pattern: Strangler Fig Application":"C’est un des patterns les plus utilisés, et ça se base sur l’image d’un figuier qui s’implante sur un arbre existant, plante ses racines, et petit à petit “étrangle” l’arbre qui finira par mourir sans ressources, laissant le figuier à sa place.\nCette technique permet d’avoir la nouvelle version en parallèle de l’ancienne. On fait grossir petit à petit les fonctionnalités de la nouvelle, puis on fait le switch quand le microservice est prêt à remplacer la fonctionnalité dans le monolithe.\nIl faut faire la différence entre deployment et release : on intègre et déploie régulièrement ce qu’on fait en production, pour éviter les problèmes de merge et dérisquer le plus possible de choses en production, mais on n’active la fonctionnalité que quand elle est prête.\nConcrètement, vu qu’on est en train de sortir un microservice qui va tourner sur un processus à part, le switch se passe au niveau réseau : tant que le microservice n’est pas prêt, les requêtes concernant sa fonctionnalité vont vers le monolithe, et quand on veut le release, on les redirige vers lui.\nSi attendre que le microservice soit fini n’est pas assez incrémental pour nous, on peut aussi commencer à rediriger une partie des requêtes du monolithe vers le microservice, en fonction de ce qui a déjà été implémenté.\nÇa va par contre nous obliger à partager temporairement la même DB entre la fonctionnalité dans le monolithe, et celle dans le microservice.\n\n\n\n\nCette technique a l’avantage de ne pas avoir à toucher au monolithe dans le cas où la portion de fonctionnalité qu’on sort est autonome.\nPour ça il faut qu'elle n'ait pas besoin de faire d’appel vers le monolithe, et que le monolithe n’ait pas besoin de faire d’appel vers elle non plus.\nDans le cas où la fonctionnalité doit faire des appels vers le monolithe, il faudra que le monolithe expose des endpoints, et donc on devra le modifier.\nSi c’est le monolithe qui doit faire des appels vers le microservice, alors on ne peut pas vraiment utiliser cette technique : on ne pourra pas faire le switch de la fonctionnalité au niveau réseau.\nOn pourra à la place utiliser le pattern Branch by Abstraction par exemple.\n\n\n\n\nExemple : HTTP Reverse Proxy : HTTP permet très facilement de faire de la redirection.\nSi notre monolithe reçoit des requêtes HTTP, on va pouvoir mettre en place un proxy pour router les requêtes entre le monolithe et le microservice.\nÉtape 1 : On met en place le proxy, et on le configure pour laisser passer les requêtes comme avant vers le monolithe.\nÇa nous permet de nous assurer que la latence additionnelle d’une étape réseau de plus ne pose pas problème.\nOn peut aussi dès cette étape tester le mécanisme de redirection pour vérifier qu’il n’y aura pas de problème à le faire.\n\n\nÉtape 2 : on implémente progressivement la fonctionnalité dans le microservice, vers lequel il n’y a aucun trafic.\nÉtape 3 : Quand le microservice est prêt, on redirige le trafic vers lui.\nOn peut remettre le trafic vers le monolithe s' il y a un problème.\nPour plus de facilité, la redirection peut être activée avec un feature toggle.\n\n\n\n\nPour ce qui est du proxy lui-même, ça va dépendre du protocole. Si on a du HTTP, on peut partir sur un serveur connu comme NGINX.\nCa peut être par exemple sur le path : rediriger /invoice/ vers le monolithe, et /payroll/ vers le microservice.\nSi on route sur un contenu se trouvant dans le body d’une requête POST (NDLR : comme GraphQL), ça risque d’être un peu plus compliqué.\nEn tout cas, l'auteur déconseille de coder soi-même son proxy si on a besoin de quelque chose de custom.\nLes quelques fois où il a essayé, il a obtenu de très mauvaises performances.\nIl conseille de plutôt partir d’un proxy existant comme NGINX, et de le personnaliser avec du code (du lua pour NGINX).\n\n\n\n\n\n\nDans le cas où on voudrait que notre microservice supporte un autre protocole que celui du monolithe (par exemple gRPC au lieu de SOAP), on pourrait envisager faire la traduction dans le proxy.\nPour l’auteur c’est une mauvaise idée : si on le fait pour plusieurs microservices, on va finir par complexifier ce proxy partagé, alors qu’on voulait que les microservices soient indépendants.\nL’auteur conseille plutôt de faire ce mapping de protocole dans chacun des microservices qui en ont besoin, et éventuellement de faire en sorte qu’ils supportent les deux protocoles.\nOn peut aussi aller vers le service mesh où chaque microservice a son proxy local, qui peut faire les redirections et mapping qu’il veut.\nLes outils les plus connus pour ça sont Linkerd et Istio.\nSquare a mis en place le service mesh et en a fait un article.\n\n\n\n\nExemple : FTP.\nL’entreprise suisse Homegate a utilisé le strangler fig pattern pour extraire des microservices, et en profiter pour changer le protocole utilisé pour uploader des fichiers : de FTP vers HTTP.\nMais ils voulaient qu’il n’y ait pas de changement pour les utilisateurs.\nDonc ils ont mis en place une interception des appels FTP, et le remapping vers du HTTP pour taper dans le microservice responsable de ça.\n\n\nExemple : Message Interception : dans le cas de messages asynchrones à router vers le nouveau microservice.\nUne première possibilité est le content-based routing, où un router va consommer tous les messages du message broker, et les queuer sur deux autres queues : une pour le monolithe, et une pour le microservice extrait.\nCe pattern vient d’Enterprise Integration Patterns. Et de manière générale l’auteur recommande ce livre pour des patterns de communication asynchrone.\nL’avantage c’est qu’on n’a pas à toucher au monolithe.\nL’inconvénient c’est qu’on complexifie là encore le système de communication plutôt que les programmes. Donc l’auteur est plutôt réticent.\n\n\nL’autre possibilité c’est la selective consumption, où le monolithe et le microservice consomment sur la même queue, mais sélectionnent les messages qui leur sont destinés.\nL’avantage c’est qu’il n’y a pas de complexité dans le mécanisme de communication.\nParmi les désavantages :\nLe message broker pourrait ne pas supporter la consommation sélective.\nIl faut déployer les changements dans le monolithe et dans le microservice en même temps pour que la consommation se passe bien.\n\n\n\n\n\n\nDedans le cas où on veut ajouter des fonctionnalités ou fixer des bugs en même temps qu’on implémente le microservice, il faut bien garder en tête que le rollback sera alors plus difficile.\nIl n’y a pas de solution facile : soit on accepte que le rollback sera plus compliqué à faire, soit on freeze les features sur la partie extraite en microservice tant que l’extraction est en cours.","pattern--ui-composition#Pattern : UI Composition":"L’interface utilisateur doit aussi être découpée par considérations business, pour obtenir des slices verticaux avec les microservices.\nExemple : Page Composition.\nL’auteur a travaillé chez The Guardian, où la migration a été réalisée à plusieurs reprises page par page.\nLa 2ème fois en utilisant un CDN pour le routing redirigé progressivement vers les nouvelles pages.\n\n\nREA Group, une entreprise immobilière australienne, avait plusieurs équipes responsables de parties différentes du site, et donc la séparation par pages avait dans ce cas encore plus de sens.\n\n\nExemple : Widget Composition.\nDe nombreuses entreprises utilisent la séparation en widgets pour suivre les microservices.\nC’est le cas par exemple d’Orbits qui avait une UI décomposée en widgets majeurs sous la responsabilité d’équipes différentes.\nQuand ils ont voulu migrer vers des microservices, ils ont pu le faire incrémentalement, en suivant le découpage des widgets côté front.\n\n\nUn des avantages de cette séparation c’est que même quand un des widgets ne fonctionne pas, le reste peut être affiché.\n\n\nCôté applications mobiles (Android / iOS), on est face à des monolithes de fait, puisqu’il faut tout redéployer et faire retélécharger à l’utilisateur à chaque changement.\nDe nombreuses entreprises (comme Spotify cf. video) utilisent des composants affichés qui viennent du backend, pour ne pas avoir à redéployer l’app mobile quand ils y font un changement.\n\n\nExemple : Micro Frontends.\nIl s’agit de faire des composants indépendants dans un frontend de type SPA, avec des bouts de React, Vue, etc. cohabitant et partageant de l’information, mais sans se gêner."}},"/books/reinventing-organizations":{"title":"Reinventing Organizations","data":{"":"","introduction#Introduction":"Il est possible que nous ne voyions pas certaines choses évidentes par aveuglement idéologique.\nExemple : jusqu’à très récemment (années 90), les scientifiques n’avaient pas \"pensé\" à vérifier le fait qu’on ait plusieurs cerveaux. On en a en fait 3 : le cerveau dans la tête, un autre dans les intestins (avec autant de neurones que le cerveau d’un chien), et un autre dans le cœur (avec autant de neurones que le cerveau d’une souris).\nL’aveuglement en question vient sans doute du fait qu’on n’ose pas envisager plusieurs sources de décision.\n\n\nLes organisations modernes ont permis depuis 2 siècles un progrès fulgurant au regard de l’histoire.\nMais ces derniers temps, on voit bien que quelque chose ne va pas.\nIl nous faut un cadre conceptuel pour envisager quelque chose d’autre qui soit concret.\n\n\nOrganisation de ce livre :\nPartie 1 : il s’agit de regarder les changements historiques qui ont pu avoir lieu, en particulier au sujet des formes l’organisation. Comprendre le passé peut permettre d’entrevoir les formes organisations futures.\nPartie 2 : l’auteur a étudié 12 organisations (de plus de 100 personnes et plus de 5 ans d'existence) qu’il considère comme fonctionnant selon le nouveau paradigme.\nCes organisations ne se connaissent pas, et pourtant leurs pratiques convergent.\n\n\nPartie 3 : l’auteur a étudié les conditions de réussite de ces organisations.","1---histoire-et-développement-des-organisations#1 - Histoire et développement des organisations":"","11-dun-paradigme-à-lautre--modèles-dorganisation-dhier-et-daujourdhui#1.1 D’un paradigme à l’autre : modèles d’organisation d’hier et d’aujourd’hui":"Selon l'auteur, l'évolution de l’humanité a par le passé avancé par bonds en avant successifs, et ça dans de nombreux domaines (même si ce livre s’intéresse spécifiquement à l’évolution des organisations).\nIl désigne chaque stade majeur de l’évolution de l’humanité par une couleur, tiré du code couleur proposé par Ken Wilber dans sa \"Théorie intégrale\" (dans une version légèrement remaniée).\nStade Réactif - Paradigme Infrarouge\nStade primitif de l’humanité de -100 000 à -50 000 ans avant JC.\nPetits groupes d’une douzaine de membres.\nPas de division du travail (sauf les femmes qui élèvent les enfants), pas de chefs, pas de hiérarchie, pas d’entreprise.\nViolence omniprésente.\n\n\nStade Magique - Paradigme Magenta\nA partir de -50 000 ans avant JC.\nTribus de quelques centaines de personnes.\nViolence toujours omniprésente.\nLa magie, les chamans etc. sont omniprésents.\nLes anciens ont une certaine autorité, mais la hiérarchie est très faible.\n\n\nStade Impulsif - Paradigme Rouge\nA partir de -10 000 ans avant JC.\nPremières chefferies, et des dizaines de milliers de sujets.\nViolence omniprésente.\nOn a une division des tâches, et le début de l’esclavage à grande échelle.\nLes structures sont encore très instables.\nExemple contemporain :\nOn peut comparer ça aux gangs ou mafias.\n\n\n\n\nStade Conformiste - Paradigme Ambre\nOn parle ici de l'agriculture, des Etats, institutions, administrations.\nOn est sur de l'ethnocentrisme : les individus s’intègrent à leur groupe, mais rejettent ceux qui sont en dehors.\nLa morale est très déontologiste, avec la figure du prêtre qui fait autorité.\nLa société est stratifiée, en castes, classes. On a une forte hiérarchisation. On pense en haut, on exécute en bas.\nLes structures deviennent plus stables, ce qui permet de pouvoir accomplir de grandes choses collectivement par rapport au stade précédent.\nLe management se fonde sur la surveillance.\nLes organisations fonctionnent en silos qui se rejettent la faute, mais qui obéissent quand même aux règles.\nExemple contemporain :\nOn peut comparer ça aux administrations actuelles.\n\n\n\n\nStade de la Réussite - Paradigme Orange\nÇa commence à la Renaissance, et se développe plus avec les Lumières et la Révolution Industrielle.\nNDLR : le libéralisme.\n\n\nÇa favorise la recherche scientifique, l’entreprenariat.\nL’innovation est mise en avant, plutôt que le fait de suivre des processus immuables.\nOn responsabilise les gens, en leur donnant des objectifs, et en les laissant trouver comment les atteindre.\nNDLR : un peu comme les OKR ?\nMalheureusement ça ne marche pas très bien : la direction refuse souvent de décentraliser les décisions, et la fixation des objectifs est une lutte constante.\n\n\nOn optimise le fonctionnement de l’entreprise de manière rationnelle.\nPar contre, il n'est pas question de se poser la question du sens et de la finalité de ce qui est fait.\nOn a des dérives importantes : recherche du profit à tout prix, invention de nouveaux besoins sans intérêt, destruction de l’environnement.\nExemple contemporain :\nOn peut citer les grandes multinationales (Amazon, Nike etc.).\n\n\n\n\nStade Pluraliste - Paradigme Vert\nLa partie de l’universalisme que promet le stade orange sans vraiment le donner, le stade Vert propose de le réaliser : castes, classes sociales, patriarcat, religions, il s’agit de s’attaquer à tout ça.\nAvant le 19ème siècle très peu de gens étaient dans ce paradigme (ex : abolition de l’eslcavage, lutte pour le droit des femmes).\nA partir du 20ème siècle il y en a de plus en plus, en particulier dans le monde universitaire et associatif.\n\n\nPour autant, même s’il arrive à abattre les anciens paradigmes, il n’arrive pas à proposer une alternative réellement crédible et durable : le pouvoir éliminé finit par revenir par la fenêtre.\nLe consensus est mis en avant comme processus de décision, et les chefs sont au service du bas plutôt que l’inverse.\nLes cadres intermédiaires sont choisis et formés pour permettre au bas d’exprimer son plein potentiel et son autonomie.\nLes cadres peuvent même être directement choisis par le bas.\n\n\nLa culture d’entreprise et les valeurs sont primordiales, devant même la vision stratégique.\nElles doivent être réellement \"vécues\".\nLe paradigme Vert utilise la métaphore de la famille ou du village.\n\n\nL’intérêt des actionnaires est mis en balance avec l’intérêt des autres parties prenantes, et la responsabilité sociétale.\nLà où dans le paradigme orange, l’intérêt des actionnaires est le principal mis en avant.\n\n\nExemple contemporain :\nOn peut citer Southwest Airlines, Ben & Jerry’s et Starbucks.\n\n\n\n\nParadigme Opale\nC’est la next step 🙂\n\n\n\n\nLes nouveaux stades apparaissent de plus en plus rapidement au cours de l’histoire, on trouve aujourd’hui de nombreux stades coexistant au sein d’une même ville.\nVu l’accélération apparente, l’auteur imaginerait bien une ou deux évolutions majeures supplémentaires de notre vivant (Opale et au-delà).","12---a-propos-des-stades-de-développement#1.2 - A propos des stades de développement":"Pour éviter de penser que les formes plus avancées sont supérieures aux formes précédentes, l’auteur propose d’aborder la question par :\nLa complexité : les formes avancées permettent d’aborder le monde avec plus de nuance.\nEx : les points de vues antagonistes du Vert vs l’impulsivité du Rouge.\n\n\nLe contexte : en situation de guerre civile, la forme Rouge sera sans doute plus efficace, par contre en temps de paix et pour gérer un ensemble de process mondialisés hyper complexes, ça le sera beaucoup moins.\n\n\nEt pour éviter de catégoriser trop facilement les personnes comme étant à tel ou tel stade, on peut y ajouter que le modèle des stades ne fait que simplifier la réalité :\nChacun peut adopter un fonctionnement d’un stade ou d’un autre en fonction du moment et de son propre entourage.\nIl y a différentes dimensions (cognitive, morale, psychologique, sociale, spirituelle etc.) dans lesquelles on peut être plutôt vers un stade ou un autre.\nOn peut éventuellement parler du stade principal utilisé par une personne.\n\n\nLa catégorisation des entreprises en stade les catégorise d’abord en tant que structure et pas en tant que personnes qui seraient à tel ou tel stade.\nEt d’ailleurs même à l’intérieur de l’entreprise on peut avoir des différences : par exemple le siège social en orange et l’usine en Ambre.\n\n\nExemple du cas de la rémunération pour chaque stade :\nSi le patron décide sur un coup de tête des augmentations, on est dans le Rouge.\nSi on a des salaires déterminés par les diplômes ou le rôle, on est sur de l’Ambre.\nSi on a des primes individuelles sur objectif, on est plutôt sur de l’orange.\nSi on a des primes d’équipe, on est plutôt sur du Vert.\n\n\nLes dirigeants ont une influence capitale sur le fonctionnement et donc le stade d’une entreprise : si eux-mêmes utilisent principalement un stade moins avancé, l’entreprise ne pourra pas arriver à un stade plus avancé.\nL’inverse par contre est possible : exemple : des cadres initialement habitués à l’Ambre très hiérarchique, qui seraient incités par des consignes, des primes etc. à donner de l’autonomie, à faire des évaluations 360° etc. pourraient basculer petit à petit vers le Vert.\n\n\nLe stade Vert étant efficace, ses principes sont copiés : par exemple le fait d’établir des valeurs.\nMais les organisations avec fonctionnement principal orange se distinguent par le fait que seul le résultat compte vraiment, et qu’en cas d’arbitrage, les valeurs cèdent immédiatement. La conséquence c’est que ces valeurs ne sont pas vraiment \"vécues\" (= valeurs bullshit).","13---le-stade-évolutif-opale#1.3 - Le stade évolutif Opale":"Le prochain stade de l’évolution de l’humanité peut être caractérisé comme \"Authentique\", \"Intégral\", \"Opale\" (en anglais \"Teal\").\nOn est au stade que Maslow appelle ‘l’accomplissement de soi’.\nOn peut considérer qu’il y a une coupure entre les stades précédents et le stade Opale.\n\n\nLe stade Opale est celui où on se détache de son égo.\nIl s’agit de faire confiance à ce qu’on trouve juste, au fond de soi, et d’agir de manière authentique. Laisser notre nature profonde nous guider pour qu’elle s’exprime à travers nous.\nSi on considère les stades au regard de la décision :\nRouge : on décide en fonction de ce qui me permet d’obtenir ce que je veux.\nAmbre : on décide en fonction de ce qui respecte les normes sociales.\norange : on décide en fonction de ce qui mène à l’efficacité et au succès.\nVert : on décide en fonction de l’intégration de tous et de l’harmonie.\nOpale : on décide en fonction de ce qu’on trouve juste en son for intérieur, y compris si ça implique de prendre des risques.\n\n\nComme on est authentique et qu’on ne se cache pas derrière un masque, ça nous permet aussi de voir ce que les autres ont à offrir plutôt que leur manque.\nOn a une approche stoïcienne de l’adversité : elle est une occasion pour nous faire grandir petit à petit.\nOn n’est ni sur le pure rationnel de l’orange, ni sur du pur émotionnel / intuitif du Vert, mais sur une synthèse prenant en compte les deux, une approche holistique.\nQuand on se recentre sur son for intérieur, on se rend compte qu’on est en fait une partie d’un tout, partie intégrante de la vie et de la nature.\n\n\nA propos des organisations :\nLe vocabulaire \"équilibre vie professionnelle - vie privée\" est révélateur du fait qu’on n’est pas intégralement soi-même au travail.\nComme les stades plus avancés permettent d’appréhender plus de complexité, les personnes utilisant principalement le paradigme Opale sont plus efficaces que les autres pour résoudre des problèmes.\nIl en est de même pour les entreprises dont le paradigme principal est l’Opale.\n\n\nDe nombreuses pratiques problématiques sont le résultat des peurs de l’égo : processus bureaucratiques, paralysie de l’analyse, culture du secret, silos en lutte, concentration du pouvoir etc.\nA l’inverse, si on laisse les collaborateurs être authentiques par rapport à ce qu’ils sont au fond d’eux, il n’y a pas besoin d’autant de règles, budgets détaillés, d’objectifs fixés etc.","2---structures-modes-de-fonctionnement-et-culture-des-organisations-opales#2 - Structures, modes de fonctionnement et culture des organisations Opales":"","21---trois-avancées-et-une-métaphore#2.1 - Trois avancées et une métaphore":"Là où dans le paradigme orange la métaphore était une mécanique, dans le paradigme Vert elle était une famille, dans le paradigme Opale elle est un organisme vivant.\nMême bienveillant et au service d’eux, un dirigeant d’entreprise qui se présente comme père de famille par rapport à ses subordonnés n’occupe pas forcément le rôle le plus émancipateur possible.\n\n\nLes entreprises Opales ont trois caractéristiques majeures :\nL’autogouvernance : il n’y a pas de hiérarchie, on fonctionne d’égal à égal.\nL’affirmation de soi (wholeness) : il s’agit de venir au travail tel qu’on est dans sa totalité et son authenticité, y compris ses vulnérabilités, ses émotions, etc.\nLa raison d’être évolutive : comme un organisme vivant, les cellules de l’entreprise avancent dans la bonne direction de manière \"organique\".\n\n\nLes 12 entreprises étudiées :\nAES : entreprise d’énergie américaine, 40 000 collaborateurs.\nBSO/origin : entreprise de conseil en informatique aux Pays Bas, 10 000 collaborateurs.\nBuurtzorg : entreprise à but non lucratif de soin aux personnes âgées, aux Pays Bas, 7000 collaborateurs.\nESBZ : collège et lycée en Allemagne, 1500 personnes (élèves inclus).\nFAVI : entreprise de métallurgie française, 500 collaborateurs.\nHeiligenfeld : hôpital psychiatrique en Allemagne, 700 collaborateurs.\nHolacracy : entreprise de formation américaine.\nMorning Star : entreprise de transport de tomates américaine, 400 à 2400 collaborateurs.\nPatagonia : entreprise de fabrication de vêtements américaine, 1350 collaborateurs.\nRHD : entreprise à but non lucratif pour aider les personnes handicapées et sans abri, aux Etats-Unis, 4000 collaborateurs.\nSounds True : entreprise qui diffuse des contenus spirituels aux Etats-Unis, 90 collaborateurs et 20 chiens.\nSun Hydraulics : entreprise multinationale américaine qui fabrique des composants hydrauliques, 900 collaborateurs.\n\n\nCertaines de ces entreprises (comme AES et BSO/origin) étaient Opales et sont revenues à des pratiques plus traditionnelles.\nLa plupart sont Opales sur certains aspects et pas d’autres, Morning Star par exemple a une autogouvernance très élaborée, mais néglige d’autres aspects.\nL’étude de l’ensemble de ces entreprises permet d’avoir une vue d’ensemble des pratiques qui reviennent.","22---autogouvernance-structures#2.2 - Autogouvernance (structures)":"Dans la plupart des entreprises la concentration du pouvoir fait qu’il y a une grande partie des salariés qui sont démotivés, parce qu'ils n’ont aucune prise sur ce qu’ils font, et n’y trouvent donc pas de sens.\nLa forme Verte donne le pouvoir au bas de l’échelle, mais elle implique qu’il y ait en haut des dirigeants qui œuvrent pour favoriser ce fonctionnement.\nLa forme Opale va plus loin en impliquant qu’aucune personne n’ait de pouvoir sur une autre, par construction.\n\n\nExemple de Buurtzorg qui a fait passer tout un secteur d’Orange à Opale :\nLe contexte c’est des infirmières employées par des organisations de plus en plus grosses, avec une division des tâches de plus en plus marquée, et des critères de rentabilité décidés par le haut.\nRésultat : des actes chronométrés, des infirmières et des patients de plus en plus déshumanisés.\n\n\nBuurtzorg arrive là dedans avec :\nDes équipes autonomes : des groupes de 10 à 12 personnes travaillant sur un périmètre restreint, et s'occupant eux-mêmes des décisions d’organisation, planning, partenariats, résolution des problèmes etc.\nRésultat :\nLes infirmières prennent beaucoup plus le temps de s’occuper de leurs patients.\nElles veillent aussi à autonomiser un maximum leurs patients, pour qu’ils n’aient plus besoin d’elles.\nL’entreprise est particulièrement performante, par exemple : les patients guérissent 40% plus vite.\nEn 2013 Buurtzorg employait les 2/3 des infirmières de quartier des Pays Bas.\n\n\nLes équipes d’infirmières n’ont pas de chef : pour que ça marche, chaque recrue est formée aux techniques de prise de décision collective.\nTypiquement dans une réunion où il faut décider, le groupe choisit un facilitateur qui ne peut pas suggérer ni décider, mais est chargé de demander aux autres ce qu’ils proposent et pourquoi.\nPuis on discute et améliore collectivement les propositions.\nEt enfin le groupe décide et choisit une solution à laquelle il n’y a pas d’objection de principe.\n\n\nL’absence de subordination n’enlève pas les rapports de reconnaissance, d’influence et de talent. On remplace la hiérarchie de pouvoir par une \"hiérarchie naturelle\" où chacun trouve une place particulière en fonction de ce qu’il apporte au groupe.\n\n\nIl n’y a pas de middle management : il n’y a que des coachs régionaux, s’occupant chacune de 50 équipes (un nombre élevé à dessin, pour les empêcher de trop s’y impliquer).\nLes coachs n’ont pas de pouvoir ni de responsabilité, elles ne sont là que pour aider dans le cas où les équipes demandent de l’aide.\n\n\nQuelques règles que les équipes s’engagent à respecter :\nLes équipes doivent se scinder au-delà de 12 personnes.\nLes équipes doivent faire attention à se partager les responsabilités, pour éviter qu’une hiérarchie traditionnelle réémerge.\nLes équipes doivent faire certaines réunions de partage de connaissance, s’évaluer chaque année en fonction de critères qu’elles établissent, établir des plans annuels pour mettre en œuvre leurs initiatives.\nElles ont pour objectif de facturer 60 à 65% du temps.\n\n\nLes fonctions de support sont réduites au minimum : en dehors de la dizaine de coachs et du fondateur, le siège compte une trentaine de personnes, pour 600 équipes d’une douzaine d’infirmières (7000 personnes).\nDans les entreprises classiques on a beaucoup de fonctions de support : RH, planning stratégique, juridique, finance, communication, relations internes, contrôle qualité etc.\nLes fonctions de support permettent des économies d’échelle grâce à la spécialisation, mais d’un autre côté elles font perdre encore plus sous forme de démotivation des salariés.\nL’autre raison c’est l’impression de pouvoir que ça donne aux dirigeants : les fonctions support sont comme des manettes utilisées pour contrôler ce qui se passe en bas.\n\n\nChez Buurtzorg la quasi-totalité des choses est faite directement par les équipes, y compris par exemple le recrutement qui se fait en général par cooptation.\nLa spécialisation (sur des questions médicales par exemple) est principalement gérée en encourageant les infirmières qui sont dans les équipes à acquérir une spécialité, et à servir de référente sur cette spécialité pour les autres équipes.\nSi besoin, on engage une personne indépendante (sur du juridique par exemple) sans lui confier de pouvoir, plutôt que de l’embaucher au siège.\n\n\n\n\n\n\n\n\nAutre exemple d’entreprise Opale : FAVI.\nAlors que toutes les entreprises de métallurgie délocalisent en Chine, celle-ci est restée en France.\nElle est passée d’une entreprise traditionnelle de 80 personnes avec une grosse hiérarchie et pas mal de fonctions de support, à 500 personnes avec très peu de hiérarchie suite à un changement de direction.\nIls font une bonne marge et leurs salaires sont au dessus de la moyenne malgré la concurrence chinoise.\n\n\nLe personnel est divisé en équipes de 15 à 50 personnes, en général centrées autour d’un client (Audi, Volkswagen etc.).\nL’encadrement intermédiaire et la plupart des effectifs de fonction de support ont disparu, les équipes décident elles-mêmes.\nLes directeurs de clientèle sont au sein des équipes, ils partagent l’information des commandes passées par les clients chaque semaine, et l’équipe décide immédiatement du planning, en cherchant des solutions si besoin.\nLes directeurs de clientèle n’ont pas d’objectif de chiffre d'affaires.\n\n\nLà où avec les entreprises pyramidales les personnes qui sont en haut passent leur temps à se réunir pour coordonner l’ensemble, dans les entreprises horizontales il n’y a quasiment pas de réunions en haut, on se réunit en bas quand ça apporte de la valeur.\nPour revenir à Buurtzorg, le fondateur et les coachs ne se réunissent que 4 fois par an pour faire le point et voir ce qu’il y a à régler. Ce nombre est très faible à dessein, pour éviter d’interférer avec l’autonomie des équipes.\n\n\nA propos de la coordination chez FAVI :\nPour se répartir temporairement entre équipes en fonction de la charge de travail, une personne de chaque équipe se réunit régulièrement pour faire le point, puis chacun retourne dans son équipe demander qui voudrait aller dans telle autre équipe surchargée pendant quelques jours.\nC’est basé sur le volontariat.\n\n\nL’investissement est décidé par les équipes elles-mêmes. Si une équipe décidait de plaquer en or leur machine, les autres s’en apercevraient vite et exerceraient une \"pression amicale\" pour les en dissuader.\nSi le budget d’investissement total est trop élevé, le DG demande aux équipes de revenir vers lui avec un autre plan.\n\n\nCertaines fonctions support émergent d’elles-mêmes parce qu’il y a un besoin particulier à un moment donné, et doivent disparaître si elles ne suscitent pas ou plus l’intérêt des équipes.\nElles n’ont pas de pouvoir sur les équipes.\n\n\n\n\nLe contrôle laisse la place à la confiance :\nLes salariés savent mieux que la direction ou que les fonctions support quelle cadence est la plus optimale. Une fois les pointeuses et autres outils de contrôle supprimés, la productivité a augmenté.\nL’émulation qui vient avec la responsabilité et la pression des collègues régule bien mieux les comportements que la surveillance.\nLes salariés n’ont pas besoin de la direction pour s’organiser exceptionnellement pour répondre à une demande inhabituelle d’un client, et ça se fait plus efficacement parce qu’ils sont fiers de leur exploit.\n\n\n\n\nExemple de Sun Hydraulics.\nLà aussi il n’y a pas de contrôle qualité, de service achats, ni de pointeuses.\nL’entreprise est largement bénéficiaire et en croissance depuis 50 ans.\nLa gestion de projet classique est éliminée :\nIl n’y a pas d’échéances et de budgets à fixer, ni d’avancement à suivre, et pas non plus de statistiques à surveiller. Pas non plus de coupable à trouver sur qui rejeter la faute au cas où le plan serait dépassé.\nLes salariés doivent occuper leur temps à ce qu’ils estiment le plus productif. C’est la règle du \"20% de temps\" de Google, mais étendue à 100% du temps.\nLes projets qui ne trouvent personne ne sont tout simplement pas assez importants.\n\n\n\n\nExemple d’AES.\nC’est un exemple qui montre que même avec des dizaines de milliers de collaborateurs, il est possible d’avoir des pratiques d’auto-gouvernance.\nIl montre aussi qu’on peut dériver vers un mode traditionnel si on ne fait pas attention.\n\n\nLes effectifs étaient organisés en équipes de 15 à 20 personnes, avec 15 à 20 équipes par site, pour pouvoir mettre un visage sur les collègues.\nSeulement 100 personnes au siège pour 40 000 salariés.\n\n\nLes équipes avaient de nombreuses responsabilités :\nBudget, planning, charge de travail, horaires de travail.\nRecrutement et licenciement.\nFormation, évaluation.\nInvestissement, achat et contrôle qualité, stratégie de long terme.\n\n\nL’idée est que plus on leur donne de responsabilité, plus ils se sentent investis et trouvent le travail amusant.\nVu sa taille impressionnante (et avant de repasser au mode traditionnel), AES avait mis en place la règle du 80-20 : chaque salarié devait passer 80% de son temps à son métier de base, et 20% à des groupes de travail dans l’entreprise.\nCes groupes de travail remplaçaient les fonctions transverses : définition du budget, réalisation des audits etc.\nCes groupes apportaient un authentique sentiment d’appartenance du fait de la responsabilité confiée.\n\n\n\n\nLes responsabilités étant très partagées au sein des entreprises Opales, il n’y a pas d’organigramme, les titres étant considérés comme le \"sucre de l'égo\" (mauvais pour la santé).\nPlutôt que de faire rentrer les salariés dans des rôles prédéfinis, on fait émerger les \"rôles\" naturellement en fonction de l’investissement de chacun.\nEt comme elles ne sont pas figées par un titre, ces activités peuvent varier dans le temps.\n\n\nC’est très marqué chez Buurtzorg où les infirmières font tourner le management.\nCa l’est un peu moins chez FAVI qui a des \"Team leaders\" qui ont l’essentiel de ce rôle. La raison est qu’il est plus difficile de le faire tourner chez les ouvriers qui devraient arrêter leur machine, alors que ce genre de problème ne se pose pas chez les infirmières.\nCeci dit les \"Team leaders\" des équipes de FAVI ont un pouvoir limité, et s’ils se comportent de manière autoritaire, les membres des équipes peuvent tout simplement changer d’équipe librement.\n\n\nOn retrouve parfois une liste de certains rôles occupés actuellement, par exemple dans l’intranet de Buurtzorg, on peut trouver les collègues qui ont une spécialité particulière pour savoir qui contacter en cas de besoin sur un sujet spécifique.\n\n\nL'exemple d’ESBZ permet de représenter le paradigme Opale dans le milieu scolaire.\nL’école est généralement l’une des organisations les plus éloignées de l’auto-gouvernance, avec les enfants qui doivent se conformer à rester assis toute la journée, et capter le savoir du professeur, sans participer eux-mêmes.\nESBZ a adopté une manière de fonctionner très différente : chaque enfant est encouragé à incarner ce qu’il est vraiment, dans sa singularité, et dans le talent qu’il veut développer.\nLe résultat c’est que les enfants ne traînent pas des pieds mais sont heureux d’aller étudier, parce qu’ils sont les moteurs de leur propre apprentissage.\n\n\nConcrètement, la plupart des cours magistraux ont été abandonnés, et remplacés par des fiches qu’on donne aux enfants, en les laissant étudier eux-mêmes à leur propre rythme, seuls et en groupe.\nLes professeurs sont disponibles principalement pour répondre aux sollicitations des élèves par un accompagnement personnalisé, quand les autres élèves n’ont pas pu aider leur camarade.\nChaque élève peut décider de passer plus ou moins de temps sur chaque matière et thématique, avec beaucoup de modules optionnels.\nLes classes sont composés de différents niveaux, les plus grands apprennent aux plus jeunes, ce qui leur permet eux-mêmes de mieux assimiler.\nCa permet aussi aux enfants handicapés d’être intégrés aux mêmes classes, et d’avancer eux aussi à leur rythme.\n\n\nPour quand même faire en sorte que les enfants aillent dans la bonne direction, ils voient un professeur chaque semaine pour faire le point sur ce qu’ils ont fait et parler aussi de ce qui se passe bien ou moins bien.\nIls se fixent aussi des objectifs deux fois par an.\nLes objectifs sont de différents ordres, pas seulement les connaissances, ça peut par exemple aussi être le fait d’être plus à l’aise au niveau de la prise de parole en public.\n\n\nUne partie du temps est réservé à l’apprentissage chacun à son rythme, et l’autre partie est consacrée au travail sur des projets individuels et collectifs qui ont un impact dans la vraie vie, et qui sont issus de sujets que les enfants ont identifiés comme comptant pour eux.\nPar exemple : coordonner la rénovation des bâtiments de l’école, tenter d’obtenir des normes environnementales plus élevées de la part du conseil municipal etc.\nIls ont aussi des sessions dédiées à sortir de leur zone de confort, par exemple un bivouac à plusieurs dans les bois, ou méditer en silence dans un monastère pendant plusieurs semaines, ou encore parcourir l’Allemagne en bicyclette, en sollicitant le gîte et le couvert.\n\n\nCôté professeurs, ils fonctionnent en équipe : deux professeurs par classe, et une \"mini-école\" autonome toutes les 3 classes, collaborant sur une base régulière et capable de répondre aux problèmes, l’équivalent des équipes de FAVI, Buurtzorg ou AES.\nLes parents aussi sont partie prenante : comme l’école ne reçoit pas d’argent en dehors du salaire des professeurs, les parents contribuent en fonction de leurs revenus, mais aussi en donnant 3 heures de leur temps par mois.\nIls sont en auto-gouvernance pour ce temps accordé qui peut servir par exemple à rénover les locaux de l’école.\n\n\nESBZ fonctionne avec le même nombre de professeurs par élève que les autres écoles, et avec un budget inférieur, même en tenant compte de la contribution des parents. Donc ce qu’elle fait est à la portée des autres écoles.","23---autogouvernance-processus#2.3 - Autogouvernance (processus)":"Le chapitre précédent traitait de la structuration des entreprises auto-gouvernées (absence de hiérarchie et fonction support limitées, équipes de taille limitée et sur-responsabilisées), celui-ci traite des processus : comment sont prises les décisions, qui peut dépenser l’argent de l’entreprise, comment décider des primes et augmentations etc.\nLes entreprises Opales ne prennent pas la majorité des décisions au consensus, ce serait trop long.\nElles utilisent plutôt une version de la sollicitation d’avis (advice process) : toute personne peut prendre une décision, mais pour ce faire elle doit : 1) recueillir l’avis des concernés, et 2) recueillir l’avis des experts du sujet.\nLa décision est prise par la personne qui a identifié le problème ou l’opportunité, ou la personne concernée par le sujet.\nPlus la décision est importante, plus elle doit ratisser large pour recueillir les avis.\nLa personne qui prend la décision n’a pas à prendre en compte l’ensemble des avis dans une synthèse consensuelle (comme dans le cas du consensus), elle doit juste les recueillir pour en avoir connaissance au moment de sa prise de décision.\nLes avis contraires, même de la part du DG, n’invalident pas la décision.\nLa prise des avis en soi invite à l’humilité, à l’apprentissage, et à la responsabilisation. La personne qui prend la décision devra en général gérer les conséquences.\n\n\nNe pas solliciter l’avis avant de prendre une décision était une des rares causes de licenciement chez AES et d’autres.\nUn variante a été présentée dans le chapitre 2.2, au moment d’analyser le fonctionnement de Buurtzorg : un facilitateur désigné qui aide à recueillir les propositions, un 2ème tour pour les questionner et les améliorer, et un dernier tour pour que le groupe en choisisse un, tant que personne ne présente \"d'objection de principe\" (non pas juste un désaccord parce qu’il préfèrerait sa solution, mais un désaccord grave qui va à l’encontre de ses valeurs par exemple).\n\n\nLa prise de décision au consensus est typique des entreprises au stade Vert. Elle a plusieurs problèmes :\nN’importe qui peut bloquer la décision au moindre désaccord, et donc on se retrouve souvent à ne pas avancer.\nElle dilue la responsabilité : personne en particulier ne se sentant responsable, et donc personne ne s'enthousiasme à tout faire pour que ça marche, et n’assume les conséquences.\n\n\n\n\nPour ce qui est des décisions en situation de crise (par exemple si la santé financière de l’entreprise nécessite des licenciements ou ce genre de chose qui peut provoquer le chaos) :\nLe DG ne peut pas simplement reprendre le contrôle quand il a envie sans entacher la confiance dans la nature auto-gouvernée de la structure.\nLe pire serait de céder à la peur et faire des actions en cachette.\n\n\nLa solution idéale est de garder la sollicitation d’avis même en période de crise.\nExemple de FAVI : juste après la première guerre du Golf, les commandes avaient significativement baissé, et maintenir les effectifs n'était pas possible. Le DG a demandé à l’ensemble des salariés d’arrêter les machines et de se rassembler pour leur exposer la situation délicate. Un des salariés a proposé de réduire le temps de travail de l’ensemble des salariés de 25% sur les prochains mois. La proposition a été adoptée à l’unanimité.\n\n\nSi ce n’est vraiment pas possible, le DG peut reprendre le contrôle temporairement pour gérer la crise, mais il faut absolument qu’il donne des gages à ses salariés : indiquer la nature et la temporalité du changement du mode de décision, et nommer pour le faire une personne qui ne pourra pas être soupçonnée de vouloir continuer à exercer le pouvoir ensuite.\nExemple d’AES : en 2001, suite à une situation de crise, l’action de l’entreprise avait chuté, et l’accès au marché de capitaux n’était plus possible. Il fallait rapidement décider quelles centrales fermer pour ne pas faire faillite. Le nombre de salariés étant trop grand, et la décision complexe, le DG a indiqué à tout le monde qu’il allait prendre une décision unilatérale sur ce sujet, et a nommé une personne externe pour mener à bien la mission.\n\n\n\n\nA propos des décisions d’achat et d'investissement pour l’entreprise :\nDans les entreprises Opales, il n’y a pas de seuil à ne pas dépasser ni d’autorisation à demander, ni non plus de service qui fait les commandes à la place des salariés. Ce sont les salariés qui négocient et commandent ce dont ils ont besoin.\nLa règle reste qu’il faut demander l’avis des personnes concernées et expertes, et plus le montant est élevé, plus il faudra demander d’avis.\n\n\nPour ce qui de la question des achats groupés pour des raisons d’économie, ou pour avoir le même fournisseur, ça se règle en faisant confiance et en responsabilisant : une personne finira par s’en rendre compte et fera le tour des équipes pour prendre les quantités et passer la commande.\nC’est par exemple ce qui s’est passé chez Morning Star où le matériel d’emballage est utilisé en grande quantité dans les équipes.\nLa coordination émerge d’elle-même dès que le besoin s’en fait sentir.\n\n\n\n\nA propos de la communication des informations en interne : tout est transparent, et surtout les informations les plus sensibles.\nIl y a plusieurs raisons à cette transparence :\nSans hiérarchie, les équipes autonomes doivent avoir accès à toute l’information pour prendre leurs décisions.\nToute information qui serait cachée serait source de suspicion : \"Pourquoi on prend la peine de nous cacher ces infos ?\".\nDes hiérarchies officieuses réapparaîtraient entre ceux qui sont dans la confidence et ceux qui ne le sont pas.\n\n\nParmi les informations transparentes, on trouve par exemple les résultats de l’entreprise, mais aussi les salaires de chacun, et les performances de chaque équipe.\nIl n’y a pas de culture de la peur, et si certaines équipes traversent une mauvaise passe, elles peuvent entrer en contact avec celles qui réussissent mieux pour s’en inspirer.\n\n\nLes informations sont en général mises dans l’intranet de l’entreprise pour pouvoir être consultables facilement par tous, sans être déformées.\nUne autre manière de communiquer les informations importantes c’est en assemblée générale. Elles sont imprévisibles et donc risquées, et c’est pour ça qu’elles permettent de réaffirmer et renforcer la relation de confiance entre collaborateurs.\n\n\nA propos de la résolution de conflits : vu qu’il n’y a pas de hiérarchie, ils sont traités d’égal à égal, selon une méthode de résolution de conflits à laquelle les nouvelles recrues sont formées.\nExemple de Morning Star :\nElle produit 40% de la purée de tomates consommée aux Etats Unis.\nLes effectifs varient entre 400 et 2400 en fonction de la saison, et sont divisés en 23 équipes entièrement autonomes comme dans les autres entreprises qu’on a vues plus haut.\nIls ont deux principes fondateurs : \"personne ne devrait jamais employer la force contre qui que ce soit\", et \"les engagements devraient toujours être tenus\".\nLa méthode de résolution de conflits découle de ce principe, elle est utilisée en fait pour tous types de désaccords, qu’ils soient techniques, interpersonnels ou autres :\nD’abord les deux personnes en désaccord se rencontrent et discutent. Celui qui est à l’initiative de la rencontre doit formuler une requête claire, l’autre répond par oui, non, ou une contre-proposition.\nSi ça n’aboutit pas à un accord, ils trouvent une personne qui a leur confiance pour jouer le rôle de médiateur. Il les aide à trouver une solution mais ne peut pas l’imposer.\nSi ça n’aboutit pas, on réunit un jury de collègues, qui vont jouer le même rôle que le médiateur, et n’auront pas non plus de pouvoir d’imposition. Ils servent d’autorité morale, et en général ça suffit à trouver une solution.\nLa dernière carte c’est d’ajouter le DG au jury pour renforcer encore plus l’aspect d’autorité morale et régler la question.\n\n\nLe conflit doit rester privé, y compris après la résolution.\nEt les protagonistes sont dissuadés d’aller chercher des alliés pour défendre leur cause, et constituer des factions rivales.\n\n\n\n\nL’auteur a constaté que le fonctionnement de résolution de conflits de Morning Star est quasi exactement le même que celui utilisé dans d’autres entreprises étudiées.\nCe mécanisme est plus qu’un mécanisme de résolution des conflits, c’est aussi la manière dont les collaborateurs se rendent des comptes dans les entreprises auto-gouvernées.\nPar exemple, quand un collègue ne fait pas ce qu’on attend de lui, ou ne tient pas ses engagements, les collègues sont tenus d’aller le confronter directement.\nPour que ça marche, il faut que la culture de l’entreprise encourage à aller demander des comptes à ses collègues.\n\n\n\n\n\n\nOn a vu qu’il n’y avait pas de titres, mais il y a bien des rôles occupés par des personnes à un moment donné.\nLes rôles se créent en général naturellement et de manière informelle : quand un besoin se fait sentir, une personne se propose et occupe automatiquement le rôle en question. Quand le besoin n’est plus là, elle n’a plus à l’occuper.\nC’est le cas par exemple chez FAVI, AES, Sun et Buurtzorg.\n\n\nParfois il y a un processus plus officiel comme chez Morning Star.\nChaque année les collègues discutent entre eux pour formaliser mutuellement un document qui indique quels rôles ils s’engagent à occuper dans le processus de transformation des tomates, avec les indicateurs qui permettront de savoir si le travail est bien fait.\nLa raison de ce formalisme chez Morning Star, et qui n’existe pas dans les autres, c’est que le processus de transformation de la tomate est déjà très codifié, parce qu’il s’agit d’un processus en continu, sur une commodité à faible marge.\nComme le partage des rôles se fait entre collègues, il n’y a plus la possibilité de chercher à bien se faire voir pour avoir une promotion, parce qu’il n’y a pas de supérieur hiérarchique auprès de qui il faudrait jouer un rôle.\n\n\nUne autre variante de rôle explicite se trouve chez Holacracy :\nA propos de l’entreprise : initialement il s’agissait d’une entreprise informatique où les fondateurs menaient des expérimentations sur le meilleur fonctionnement possible.\nIls ont fini par fonder une entreprise de conseil en organisation chargée d’aider à appliquer la méthode Holacracy qu’ils avaient créée.\n\n\nDans leur méthode, les rôles sont créés ou modifiés/supprimés en fonction d’un processus particulier :\nUn nouveau rôle peut être proposé pendant une réunion de gouvernance (réunions spécifiquement dédiées aux rôles et au fonctionnement de la collaboration, en général sur une base mensuelle et à la demande).\nLa réunion de gouvernance se déroule comme suit :\nUn facilitateur guide le déroulement et traite chaque question mise à l’ordre du jour par les membres.\n1- La proposition est présentée par l’auteur.\n2- Les autre sposent des questions de clarification pour mieux comprendre, sans réagir à la proposition.\n3- Chaque personne réagit à la proposition, sans qu’il n’y ait de réponse.\n4- L’auteur peut clarifier ou amender sa proposition en fonction de ce qui a été dit.\n5- On fait le tour à nouveau pour récolter les objections sans les discuter. Si il n’y en a pas la proposition sera adoptée.\n6- S’il y a des objections, on les traite une à une, en essayant à chaque fois d’élaborer une proposition qui l’intègre, tout en restant fidèle à l’intention initiale.\n\n\nIl s’agit d’une variante de la sollicitation d’avis, mais dans ce cas c'est le groupe entier qui sollicite et prend la décision.\nEt comme avec la sollicitation d’avis, on ne reste pas bloqué avec de longues discussions.\n\n\n\n\n\n\nMalgré les rôles, dans les entreprises auto-gouvernées chaque collaborateur est en situation de responsabilité totale et a le devoir d'agir s’il voit un problème, quel que soit le périmètre.\nIl est donc inacceptable de dire \"On devrait faire quelque chose pour régler ce problème\" et de passer à autre chose.\nEt à l’inverse personne ne peut dire \"Ça ne te regarde pas\".\n\n\nHolacracy propose de régler les \"tensions\" en réunion de gouvernance ou en réunion tactique (de travail), en fonction du type de tension, avec des processus particuliers pour chacun.\n\n\nLa \"nomination\" aux rôles, y compris pour ceux qui émergent, se fait en général de manière informelle et naturelle : les personnes qui sont volontaires et sont perçues par leurs collègues comme les plus aptes prennent le rôle.\nParfois ça peut être un peu plus explicite, par exemple chez Sun, à chaque fois qu’un poste se libère, il y a un processus en interne pour permettre aux candidats de rencontrer les futurs équipiers.\nChez FAVI, le DG a aussi mis en place un processus de confirmation pour sa propre place : tous les 5 ans il demande aux salariés de voter pour dire s’il doit rester DG. Ca permet de renforcer encore plus l’idée d’auto-gouvernance.\n\n\n\n\nA propos de la gestion de la performance :\nLes études montrent que quiconque poursuit un objectif qui a du sens, et qui a le pouvoir de décision et les ressources nécessaires, n’a pas besoin qu’on lui prodigue des paroles d’encouragement ni qu’on surdimensionne ses objectifs (on peut trouver un bon état de la recherche dans La Vérité sur ce qui nous motive de Daniel Pink).\nIl n’y a donc pas besoin de pression hiérarchique.\n\n\nPar contre, on a besoin de mesure, principalement par équipe, pour savoir si on va dans la bonne direction.\nEt on a besoin du regard des autres pour nous motiver de manière naturelle.\nC’est pour ça que les indicateurs par équipe sont transparents.\nChez Buurtzorg et FAVI les équipes peuvent facilement comparer leurs performances respectives.\nChez Morning Star, vu la plus grande diversité des rôles dans la chaîne de traitement de la tomate, les équipes s’auto-évaluent devant des groupes de collègues.\n\n\nConcernant la performance individuelle, elle est secondaire par rapport à l’évaluation par équipe.\nElle peut être évaluée par les collègues avec qui on travaille, de même qu’on les évalue.\nChez Morning Star, on reçoit un retour de chacun des collègues avec qui on a travaillé à la fin de l’année.\nChez AES, chacun se réunissait une fois par an avec son 1er cercle de collaborateurs au cours d’un dîner, et s’auto-évaluait pendant que les autres commentaient.\nChez Buurtzorg, les équipes sont libres de choisir leur mode d’évaluation annuel. Une des équipes a par exemple mis en place un processus où chaque collègue prépare la perception qu’il a de son travail et celui qu’il a des autres, et ensuite ils comparent ce qu’ils ont mis mutuellement.\n\n\n\n\nSur la question des licenciements :\nIls sont plutôt rares parce que les rôles ne sont pas fixes, et une personne qui n’est pas bonne dans un rôle peut en trouver d’autres où elle est plus douée.\nPour autant on a quand même des cas où une personne soit n’arrive pas à s’intégrer, ne respecte pas les valeurs, soit n’est pas au niveau attendu par ses collègues.\nLe licenciement n’est actionnable que par les collègues.\nLa plupart du temps, la personne va partir d’elle-même, voyant que ses collègues ne lui font plus confiance.\nChez Buurtzorg par exemple, c'est l’équipe qui peut enclencher la procédure dans le cas où un des membres perd la confiance des autres. Ils font appel à un coach régional en tant que facilitateur pour voir si le problème ne peut pas être résolu, et en dernier recours ils demandent au fondateur de licencier la personne.\n\n\n\n\nA propos de la rémunération :\nLe montant du salaire est fixé par la personne et ses collègues.\nChez Holacracy par exemple, une fois par an chaque personne répond à deux questions sur ses collègues, sur une échelle +3 / -3 :\n\"Cette personne apporte à l’entreprise une contribution supérieure à la mienne.\" et \"Cette personne dispose de bons éléments pour m’évaluer.\"\nPuis en un algorithme s’occupe de ranger les salariés par ordre, ceux du bas ayant un salaire plus faible que ceux du haut.\n\n\nChez AES la version était plus radicale : chaque personne devait solliciter des avis, puis fixer elle-même son salaire.\nChez Morning Star il s’agit d’un processus un peu plus formel : chaque personne écrit une lettre à la commission élue des salaires, et y met l’augmentation qu’elle pense juste pour elle, en l’argumentant avec les commentaires reçus de ses collègues pendant son évaluation.\n\n\nLa plupart des entreprises étudiées ici ont abandonné les primes, et n’ont gardé que la participation aux bénéfices.\nEnfin, la plupart des entreprises étudiées ici veillent à réduire l’écart entre les salaires les plus hauts et les plus bas.\nMais cet écart reste élevé, et contraste avec l’absence de hiérarchie dans le travail.\nSelon l’auteur, cette différence n’est pas forcément légitime et pourrait s’amoindrir, voire disparaître à l’avenir.\n\n\n\n\nQuelques malentendus à dissiper :\nTout le monde sur un pied d’égalité :\nMettre tout le monde à égalité c’est l’objectif du paradigme Vert. Le paradigme Opale cherche à donner du pouvoir à tous, dans une mesure suffisante pour qu’ils puissent réaliser leur potentiel.\nLes hiérarchies naturelles enchevêtrées (compétence, talent, reconnaissance etc.) empêchent de toute façon une égalité de pouvoir. On peut voir ça plutôt comme un système vivant où les relations de pouvoir sont constamment remodelées.\n\n\nAucune des entreprises étudiées ici comme Opale n’est la propriété de ses salariés. Les coopératives sont là aussi plutôt recherchées par le paradigme Vert.\nTant que le pouvoir est correctement réparti, la propriété ne semble pas importante.\n\n\n\n\nIl s’agit d’autonomisation :\nSi on parle de dirigeants qui doivent faire un effort pour autonomiser les salariés, c’est que les salariés sont en position de victime, à se battre constamment pour du pouvoir.\nL’entreprise Opales est déjà organisée selon une structuration et un fonctionnement où le dirigeant n’a pas besoin de faire d'efforts pour que le pouvoir revienne aux salariés naturellement.\n\n\nOn en est encore au stade expérimental :\nLes exemples étudiés dans ce livre montrent que non, et il y en a plein d’autres.\nOn peut ajouter à ça que les jeunes qui ont grandi avec internet sont taillés pour vouloir l’auto-gouvernance.","24---en-quête-de-plénitude-pratiques-générales#2.4 - En quête de plénitude (pratiques générales)":"Habituellement on vient au travail en masquant une part de soi. En général, on masque notre vulnérabilité et notre dimension émotionnelle.\nDans l’entreprise Opale on vient sans masque, avec la totalité de ce qu’on est.\nLe fait de ne pas avoir de promotions à aller chercher aide à être plus authentique.\n\n\nChez Sounds True, la fondatrice venait initialement avec son chien. Les autres salariés ont demandé s’ils pouvaient ramener le leur, et finalement les 20 chiens amenés régulièrement contribuent à une ambiance fraternelle, un surcroît d’humanité.\nChez Patagonia il y a un centre de développement de l’enfant sur le lieu de travail. Les salariés y amènent leurs enfants en bas âge, et peuvent les voir à la cafétéria ou au déjeuner.\nPour que nous puissions exprimer pleinement ce que nous sommes, il faut que nous soyons dans un environnement protecteur et bienveillant.\nLa plupart des entreprises ont fini par rédiger des valeurs et des chartes pour instaurer cet environnement protecteur.\nExemple d’RHD, association qui aide les personnes handicapées mentales, mais aussi les sans-abris et les alcooliques.\nIls existent depuis 50 ans, ont une croissance de 30% par an, et offrent des services d’une valeur de 200 Millions de dollars à travers le monde.\nIls fonctionnent avec le même modèle que FAVI ou Buurtzorg, avec des équipes auto-organisées, une hiérarchie existante, et des fonctions support minimales.\nLeurs 3 postulats sont :\nQue tout le monde est d’égale valeur humaine.\nQue, sauf preuve du contraire, tout le monde est fondamentalement bon.\nQu’il n’y a jamais une seule bonne façon de résoudre les problèmes qui se posent dans l’entreprise.\n\n\nLeur charte détaille 5 façons inacceptables d’exprimer son hostilité :\nLes propos et comportements dégradants : le fait de chercher à rabaisser l’autre dans sa dignité, sa valeur en tant que personne.\nLes messages négatifs indirects.\nLa menace d’abandon.\nLa négation de la réalité de l’autre.\nL’intimidation/explosion.\n\n\n\n\n\n\nRédiger des documents ne suffit pas, il faut aussi y consacrer du temps. Parmi les possibilité il y a :\nFaire suivre une formation aux nouvelles recrues pour leur présenter les valeurs et les règles de base.\nLa Journée des valeurs, qui consiste à se poser la question individuellement et collectivement de la manière dont on met en œuvre les valeurs.\nLa Réunion des valeurs, qui consiste à exposer les problèmes qu’on a pu rencontrer sur la mise en œuvre des valeurs, et suggérer d’apporter des modifications à la charte.\nL’Enquête annuelle, qui consiste à diffuser un jeu de questions au sujet des valeurs, et de l’analyser pour en tirer des conséquences.\n\n\nLa plupart des entreprises étudiées ont des pratiques qu’on pourrait appeler \"d’introspection\", où il s’agit de chercher au fond de soi, individuellement et collectivement.\nLe silence est une première manière de le faire.\nBeaucoup de structures ont des lieux destinés au silence (méditation, yoga, etc.).\nCertaines structures ont des temps dédiés à des moments de silence collectifs : dans la journée, la semaine, des jours spécifiques etc.\n\n\nOn a aussi des sessions d’éducation populaire (l’auteur ne cite pas ce terme, et appelle ça \"introspection de groupe\") où un sujet est introduit, et les salariés se séparent en groupes pour y réfléchir collectivement à partir de leurs propres expériences, pour en tirer des choses et renforcer leurs liens.\nC’est le cas de l’hôpital Heiligenfeld qui consacre 1h15 chaque semaine à cette pratique pour l’ensemble des salariés disponibles (en général la moitié des effectifs).\nDes exemples de sujets traités collectivement peuvent être : la résolution de conflit, la gestion de l’échec, la bureaucratie, la communication interpersonnelle, la gestion du risque etc.\nC’est des moments où les salariés peuvent partager leur humanité profonde, dans la beauté de leur force et de leur vulnérabilité.\n\n\nHeiligenfeld a mis en place des ateliers pour travailler les questions organisationnelles et interpersonnelles (conflits par exemple) avec des spécialistes, en moyenne plusieurs fois par mois, à la demande des équipes.\nChez Buurtzorg, ils ont mis en place des sessions basées sur la technique Intervisie, où une des infirmières exprime une problématique qu’elle a avec des clients, et le reste de l’équipe l’écoute et la soutient pour l’aider à trouver une solution qui lui appartiendrait.\n\n\nPour créer des relations profondes et authentiques avec ses collègues, il faut qu’il y ait de la narration (storytelling), c’est-à-dire raconter des histoires sur nous, et écouter celles des autres.\nPar exemple, Center for Courage & Renewal mène des ateliers où ils demandent à ceux qui veulent répondre des choses comme \"Parlez-nous d’un aîné qui a compté dans votre vie\" ou \"Parlez-nous du premier dollar que vous avez gagné\".\nChez ESBZ, chaque vendredi les élèves et professeurs se retrouvent pendant une heure. Ils commencent par chanter ensemble, puis ceux qui le veulent peuvent aller prendre le micro, et remercier ou congratuler quelqu’un pour quelque chose de spécifique. Il s’agit en fait de petites histoires.\nChez Ozvision, petite entreprise japonaise, chaque salarié a chaque année une journée de congé où on lui offre 200$ à dépenser pour faire un cadeau à quelqu’un qu’il connaît, et ensuite de le raconter aux collègues.\n\n\nA propos des réunions :\nPresque toutes les entreprises étudiées ont mis en place des techniques pour aider à maîtriser son égo et interagir avec sincérité.\nPar exemple un tour au début et un tour à la fin de chaque réunion pour indiquer comment on se sent sur l’instant.\nCa peut aussi être de commencer ou ponctuer la réunion par des moments de silence pour s’enraciner et prendre du recul.\nOu encore lire un texte, ou féliciter quelqu’un qu’on estime avoir fait quelque chose de remarquable.\nOn peut aussi inviter un facilitateur à se joindre à la réunion.\n\n\nElles ont aussi en général des procédures spécifiques pour empêcher que les réunions soient accaparées par certains.\n\n\nA propos des conflits :\n1- Il faut favoriser l’émergence des tensions :\nPar exemple, chez ESBZ, chaque classe se réunit une fois par semaine pour parler des tensions au sein du groupe.\n\n\n2- Il faut une procédure défini de résolution des conflits (comme expliqué au chapitre précédent), pour que les personnes puissent plus facilement prendre l’initiative de le traiter.\n3- Certaines structures comme ESBZ ou Sounds True forment leurs salariés à la communication non-violente.\nIl s’agit d’une méthode où on exprime son ressenti, ses besoins, et on écoute ceux des autres.\n\n\n\n\nLa gestion des locaux de travail est importante, il faut qu’elle soit accueillante. Par exemple, avoir une cuisinière pour se sentir comme chez soi, pouvoir personnaliser la décoration, pouvoir être proche de la nature quand on est au travail.\nLes préoccupations sociales et environnementales sont fondamentales, elles nous permettent de nous relier au reste du vivant et d’affirmer notre intégrité.\nIl s’agit avant tout de se demander ce qu’on trouve juste, et de voir seulement ensuite combien ça va coûter.\nLes paris qui ont l’air risqués mais qui vont dans le sens de ce qu’on trouve juste se révèlent souvent gagnants.\nLes initiatives de ce genre dans les entreprises Opales viennent des salariés eux-mêmes.\nExemple : chez AES, une salariée a eu l’initiative de faire planter des arbres pour compenser les émissions carbone de l’entreprise. Et depuis, des millions d’arbres ont été replantés.","25---en-quête-de-plénitude-processus-rh#2.5 - En quête de plénitude (processus RH)":"Recrutement :\nEn général, les candidats comme les entreprises jouent un rôle pendant les entretiens.\nDans les entreprises Opales, les entretiens sont faits par les futurs collègues de la personne qu’on veut embaucher.\nDe cette manière, ils auront tout intérêt à être honnêtes (puisqu’ils devront en assumer les conséquences au quotidien), ce qui incitera le candidat à être honnête et authentique aussi.\n\n\nVu le décalage avec les entreprises habituelles, les entreprises auto-organisées vont consacrer beaucoup d’énergie à expliquer le fonctionnement et les valeurs de l’entreprise pour s’assurer que la personne saura s’y intégrer : prendre les responsabilités, faire confiance aux autres, prendre des initiatives, être elle-même etc.\nZappos.com offre un chèque de 3000$ aux employés nouvellement embauchés qui font le choix de partir pendant la période d’essai : il vaut mieux qu’ils partent rapidement s’ils n’aiment pas la culture.\n\n\n\n\nIntégration :\nLes entreprises Opales investissent beaucoup dans l’intégration, qui ressemble souvent à une sorte de formation.\nOn y aborde les 3 piliers :\nL’autonomie : expliquer ce qu’implique d’avoir d’importantes responsabilités, et de les exercer avec un groupe.\nEn général, ceux qui ont le plus de mal sont les anciens cadres dirigeants. Selon Paul Green Jr. de Morning Star, 50% d’entre eux partent au bout d’un an parce qu’ils \"ont du mal à s’adapter à un système où ils ne peuvent pas jouer à Dieu\".\n\n\nL’intégrité : former à des techniques de communication non violente, à la maîtrise de soi, la gestion de l’échec etc. Le but étant de se montrer tel qu’on est.\nLa raison d’être évolutive : examiner la raison d’être de l’entreprise, et voir en quoi elle peut rejoindre notre propre vocation personnelle.\n\n\nSouvent les nouvelles recrues vont aussi recevoir une formation technique, en étant intégré aux divers rôles de l’entreprise pour nouer des relations.\nC’est le cas chez Sun Hydraulics où les nouveaux passent plusieurs mois dans d’autres rôles avant de rejoindre l’équipe où ils étaient censés aller initialement.\n\n\n\n\nFormation :\nEn général, n'importe quel salarié peut s’inscrire à une formation, en sollicitant des avis d’abord.\nParfois au lieu de la sollicitation d’avis il y a plutôt une limitation du montant.\nC’est le cas chez Buurtzorg, où toutes les infirmières ont 3% de leur chiffre d'affaires à dépenser en formation.\n\n\nLes entreprises Opales n’ont pas de formations pour gravir les échelons en s’améliorant en management puisqu’il n’y a pas d’échelons à gravir.\nElles ont des formations pour créer une culture partagée, des formations de développement personnel, et des formations techniques qui sont dispensées par les collègues.\n\n\n\n\nQuantité de travail :\nDans les entreprises classiques on a les ouvriers bas à qui on demande de respecter des horaires strictes et surveillées, et les cadres en haut à qui on demande de faire passer le travail avant tout et d’être joignables tout le temps.\nLes entreprises Opales proposent d’être dans la transparence, et de prendre en compte les besoins de chacun.\nLes obligations du travail sont importantes, mais les obligations personnelles aussi, elles sont à mettre en balance.\nOn demande régulièrement à chacun combien de temps et d’énergie il souhaite consacrer à son travail dans l’entreprise en ce moment.\nC’est le cas par exemple chez Holacracy et Morning Star.\n\n\nÇa peut varier, parfois selon la saison, parfois selon la période dans la vie (enfant, personne malade dont il faut s’occuper, maison à retaper etc.).\n\n\nLa procédure ne passe pas par des fonctions support RH, mais se fait de pair à pair entre collègues. Chacun essaye d’arranger l’autre, sachant qu’il sera bien content d’être arrangé quand il en aura besoin.\n\n\nPerformance individuelle :\nLa performance individuelle est mesurée par les pairs, mais il faut faire attention à ce qu’elle ne soit pas un moment de jugement et de surveillance.\nPour que ça marche, il faut :\nSe mettre en condition pour aborder la question avec bienveillance, sans réponses toutes faites, mais sans avoir peur de s’exprimer non plus.\nNe pas donner l’impression qu’on fait des remarques objectives, mais s’impliquer en parlant à la première personne, en montrant que c’est bien nous qui sommes enthousiasmés, touchés, intrigués, blessés, déçus etc. par ce qu’a fait la personne.\nAborder la question de l’évaluation non pas comme un instantané mais comme un déroulement : d’où on vient, ce qu’on a fait, où on veut aller.\n\n\n\n\nLicenciement :\nPour ce qui est du licenciement individuel :\nIl faut prendre la question comme une porte qui se ferme et qui ne correspondait pas à la voie de la personne licenciée, pour d’autres qui s’ouvrent et qui lui permettront de s’épanouir.\n\n\nPour ce qui est du licenciement collectif pour problèmes économiques :\nQuand le problème est temporaire, en général les entreprises Opales tiennent le coup parce qu’elles sont résilientes.\nQuand il s’agit de problèmes permanents (par exemple AES qui rachète une centrale en sureffectif), alors il vaut mieux ne pas garder trop de personnes.","26---a-lécoute-du-projet-des-entreprises-opales#2.6 - A l’écoute du projet des entreprises Opales":"Les entreprises classiques fonctionnent sur la peur de ne pas réussir à préserver l’entreprise. Les valeurs et la raison d’être de l'entreprise ne sont pas du tout vécues.\nLes entreprises Opales consacrent une part très importante à leur mission.\nLes entreprises Opales ne se soucient pas de leurs concurrents, puisqu’elles travaillent pour leur raison d’être, elles n’ont pas vraiment de concurrents.\nAu contraire, elles ont plutôt tendance à aider les entreprises qui font la même chose à fonctionner sur le même principe qu’elles.\n\n\n\n\nLa croissance n’est pas un objectif en soi pour les entreprises Opales.\nPar exemple Buurtzorg préfère aider les malades à guérir plus vite, quitte à les avoir pour clients pour moins longtemps, parce que c’est dans leur raison d’être.\nPareil pour Patagonia qui fabrique des vêtements qui durent plus, et qui sont réparables, quitte à vendre moins sur le coup.\n\n\nLe profit est toujours secondaire. Il s’agit de poursuivre sa raison d’être, et créer une activité viable dans ce cadre.\nEn général, le profit arrive bien plus pour les entreprises qui se lancent sincèrement dans ce qu’elles croient être porteur de sens.\n\n\nLes entreprises étant comme des êtres vivants, il s’agit d'écouter où elles veulent aller en tant que structure porteuse de sens.\nChaque membre de l’entreprise participe à cette direction par ses idées, à travers l’intérêt et l’implication qu’elles génèrent auprès des autres.\nLa stratégie est portée par l’ensemble des salariés au quotidien par des initiatives. Elle n’est pas précisée dans un endroit central, mais mise en œuvre par les salariés qui sont comme des cellules.\n\n\nLa raison d’être de Buurtzorg est d’aider les gens à retrouver une existence riche, intéressante et autonome.\nMais même en allant du côté des industriels on peut trouver du sens.\nPar exemple, FAVI a pour raison d’être de donner un travail intéressant aux personnes vivant dans la ville où ils sont, et aimer et se faire aimer de leurs clients. Ils ont décidé de ça tous ensemble.\n\n\nL’auteur pense que les pratiques transrationnelles (spiritualité, méditation etc.) peuvent aider à trouver et faire évoluer la raison d’être de l’entreprise.\nUne des techniques consiste à laisser une chaise vide représentant l’entreprise et sa raison d’être pendant les réunions. Chaque personne peut s'asseoir dessus et se poser la question de savoir si ce qui est fait va dans le sens de cette raison d’être et quelle est sa nature.\nUne autre solution est de réunir tous les salariés de l’entreprise, et d’essayer de faire émerger collectivement la direction qu’elle doit prendre.\nLes dirigeants doivent accepter que leur voix compte autant que celle des autres salariés, sinon ça ne peut pas marcher dans un mode ascendant.\n\n\n\n\nLe marketing :\nLes entreprises classiques font beaucoup de marketing, d’études de marché, et font appel à des techniques pour créer des besoins, en jouant sur les émotions.\nLes entreprises Opales font peu de marketing. Elles sont à l’écoute des besoins, mais surtout à l’écoute de leur raison d’être, pour développer un produit dont ils seront fiers, et qui répondra à un besoin authentique.\nLeur stratégie produit est plus guidée par l’intuition que par l’analyse.\nPar exemple Sounds True vendrait sans doute bien plus de livres s’ils se concentraient sur le marché des livres, mais ils pensent que les autres supports sont une plus grande source de clarté pour leur domaine.\n\n\nPlanning, budgets :\nLes entreprises Opales font très peu de planification. Ils n’essayent pas de prévoir et contrôler, mais sont plutôt dans l’intuition, et le fait d’être dans la réponse à ce qui se passe.\nL’auteur évoque la distinction système compliqué / système complexe pour dire que les systèmes complexes ne peuvent pas être planifiés.\nCes entreprises se contentent de solutions réalistes, et n’attendent pas d’avoir plus de données avant d’agir, en se corrigeant souvent.\nElles utilisent des itérations rapides (dans l’idée du lean manufacturing et du manifeste agile).\nPour les objectifs :\nLa notion d’objectif peut poser problème parce qu’elle peut nous pousser dans une direction qui ne sera pas forcément la bonne, et qui peut empêcher de s’adapter au contexte.\nLes objectifs peuvent être bénéfiques s’ils sont fixés par les personnes elles-mêmes, ou par les équipes elles-mêmes, si ça les aide à se motiver. Mais ça ne doit pas être une obligation.\n\n\nPour les budgets :\nLa plupart des entreprises étudiées ne font pas de budget du tout.\nCertaines en font, mais seulement quand il faut un certain degré de prévision.\nQuand il y en a, on ne perd pas de temps à savoir pourquoi on n’était pas dans les clous.\n\n\n\n\nComme l’entreprise est une structure vivante, la question du changement de process ne se pose pas. Le changement survient naturellement et constamment en fonction des besoins.\nTransparence vis-à-vis de l’extérieur :\nLes entreprises Opales choisissent d’assumer pleinement leur projet et leurs valeurs. Ils vont donc en parler en toute transparence avec leurs fournisseurs, et leurs clients.\nIls vont aussi en général être transparents sur les process internes, vis-à-vis des clients et partenaires.\nCertaines mettent en ligne des enregistrements de processus clés, des compte-rendus de réunion, des indicateurs.\n\n\n\n\nIl est important de se poser la question de son projet personnel, sa vocation, et de le mettre en lien avec le projet, la vocation de l’entreprise.\nL’entreprise n’est pas là que pour faire que les choses se fassent, mais aussi pour aider les salariés à trouver leur voie. Le salarié n’a pas forcément fait ce travail d’introspection.\nQuand les deux sont alignés, on obtient une efficacité exceptionnelle.\nPour les entretiens d’embauche, les entreprises Opales insistent beaucoup sur l’adéquation entre la raison d’être de l’entreprise et le projet individuel de la personne.\nExemple de questions à poser : Comment voyez-vous votre parcours de vie ? Comment le fait de travailler ici s’intègre-t-il à votre perception de ce que vous êtes appelé à être et à faire dans le monde ?\n\n\nChez Heiligenfeld, ils posent deux questions aux entretiens annuels : Est-ce que je mets mon cœur dans mon travail ? Est-ce que j’ai le sentiment d’être à ma place ?\n\n\nAlors que dans le modèle Vert, les dirigeants se mettaient au service de l’entreprise, et veillaient à ce que tout se passe bien dans ce sens, dans le modèle Opale l’entreprise est comme un être vivant, un champ d’énergie indépendant qui évolue par lui-même. Il n’y a donc pas besoin de chef pour en assurer la cohérence.\nL’auteur se demande quel sens il y a à \"posséder\" un champ d’énergie indépendant, et propose d’inventer de nouveaux cadres juridiques pour que les investisseurs aient la place qui leur revient, en laissant l’autonomie à l’entreprise.","27---traits-culturels-communs#2.7 - Traits culturels communs":"La culture d'entreprise est la façon dont les choses se font spontanément, sans qu’on ait besoin d’y penser.\nElle détermine pour une large part si l’entreprise va échouer ou perdurer.\nLes entreprises dans le paradigme Vert vont typiquement considérer que c’est la chose la plus importante, avec la notion de culture familiale.\n\n\nKen Wilber propose un modèle basé sur des quadrants pour obtenir une compréhension intégrale de la réalité.\nD’un côté on a le point de vue intérieur (pensées, sentiments, sensations) et extérieur (tangible, matériel, mesurable).\nDe l’autre on a le point de vue individuel et le point de vue collectif.\nOn obtient le quadrant suivant :\nIntérieur\tExtérieur\tIndividuel\tOpinions et mentalités des personnes\tComportements des personnes\tCollectif\tCulture de l’entreprise\tSystèmes de l’entreprise (structures, processus, et pratiques)\n\nCe modèle permet de se rendre compte que tous ces aspects sont interdépendants.\nLes entreprises Ambres ou Oranges mettent l’accent sur le côté extérieur, et obtiennent des salariés peu motivés.\nLes entreprises Vertes mettent l’accent sur l’immatériel et sont parfois face à des incohérences, comme par exemple avoir des structures hiérarchiques où on demande aux cadres de donner le maximum d’autonomie à leurs subordonnés : résultat c’est une difficulté permanente.\nLes entreprises Opales mettent l’accent sur l’autonomisation et la responsabilisation, et à partir de là la culture émerge naturellement sans faire d’efforts particuliers.\n\n\n\n\nLes entreprises ont chacune leur culture, qui est en partie propre à leur domaine et unique à chacune, et en partie commune avec toutes les autres entreprises du même stade de développement.\nPar exemple, les entreprises Ambres partagent le trait commun de l’obéissance aux ordres.\n\n\nLes entreprises Opales ont les traits culturels communs suivants :\nAutogouvernance.\nConfiance : jusqu’à preuve du contraire, s’engager avec les collègues sur la base de la confiance.\nAccès à toutes les informations de manière transparente.\nPrise de décision collective par sollicitation d’avis.\nResponsabilité de ce qui se passe dans l’entreprise pour tous les salariés.\nResponsabilité de faire en sorte que les collègues respectent leurs engagements.\n\n\nPlénitude.\nTous les salariés ont une valeur égale en tant que personne.\nPour autant chaque salarié a un apport singulier qui doit être reconnu comme tel.\nL’environnement permet la sécurité psychologique pour être dans l’authenticité.\nLa bienveillance, la reconnaissance, la gratitude, la curiosité, l’amusement sont mis en avant.\nOn réconcilie le rationnel et l’intuitif.\nOn reconnait être étroitement liés les uns aux autres et à la nature.\nLes problèmes et les confrontations respectueuses sont sources d’apprentissage.\nOn règle les conflits en face à face, sans propager de rumeurs.\n\n\nRaison d’être évolutive.\nL’entreprise a un projet, une âme, qu’il s’agit d’écouter.\nLe projet individuel doit être questionné pour voir dans quelle mesure il est en résonance avec celui de l’entreprise.\nIl n’y a pas besoin de vouloir maîtriser l’avenir, au lieu de ça on se met à l’écoute et on ajuste.\nOn se concentre sur la mission, le profit viendra de lui-même.\n\n\n\n\nQuand on a une culture qui se dessine, on peut aller dans son sens pour la renforcer par 3 manières (même si l’autonomie devrait de toute façon laisser la culture se développer librement) :\nMettre en place des structures, pratiques et processus qui vont dans le sens de cette culture (quadrant inférieur droit).\nS’assurer que ceux qui détiennent l’autorité morale sont en phase avec la culture dans leurs actes (quadrant supérieur droit).\nInviter chacun à examiner dans quelle mesure il est en résonance avec cette culture (quadrant supérieur gauche).","3---lémergence-des-organisations-opales#3 - L’émergence des organisations Opales":"","31---les-conditions-nécessaires#3.1 - Les conditions nécessaires":"Selon l’auteur, il y a deux conditions décisives pour pouvoir créer une organisation Opale :\n1- Le dirigeant (fondateur) doit avoir intégré les principes de fonctionnement Opale, et y adhérer personnellement.\n2- Les propriétaires (actionnaires) doivent aussi avoir compris le modèle Opale, et y adhérer.\nIl est possible qu’ils fassent confiance au un dirigeant même sans comprendre, mais en cas de turbulences ils vont avoir tendance à vouloir reprendre la main.\n\n\n\n\nParmi les paramètres qui n’ont pas l’air d’avoir d’importance :\nLe secteur peut aussi bien être lucratif que non lucratif, dans la santé, l’industrie, le commerce, les services etc.\nLa taille de l’entreprise : ça marche avec les petites comme les grandes.\nLa géographie et la culture ne comptent pas non plus, cf. l’exemple d’AES.\n\n\nL’auteur déconseille de tenter de transformer une partie de l’entreprise si on n’est pas à sa tête, et si le dirigeant / les propriétaires ne sont pas sur un paradigme Opale.\nL’auteur n’a vu aucun exemple d’une partie de l’entreprise qui devient Opale et le reste alors que les dirigeants ne sont pas sur ce paradigme.\nA la limite le paradigme Vert peut être un peu plus tolérant, mais pas l’Orange qui peut laisser faire un temps avant de tout annuler en voyant de quoi il s’agit.\n\n\nIl déconseille aussi de tenter de convaincre des dirigeants qui ne sont pas sur ce paradigme.\nGravir les échelons des paradigmes est un processus personnel et complexe, et ne peut pas être imposé de l’extérieur.\nTous les exemples de consultants qui, chiffres et arguments Oranges à l’appui, ont essayé de convaincre de l’efficacité des organisations Vertes et Opales, ont échoué.\nLes dirigeants écoutent avec intérêt jusqu’au moment où ils comprennent quelles fonctions sont en cause et quel pouvoir ils vont devoir abandonner.\n\n\nCe qui est possible pour un cadre travaillant dans une entreprise Orange c’est de la faire évoluer vers de l’Orange moins toxique.\nPar exemple, au lieu d’enlever complètement les objectifs (ce qui serait inacceptable), on peut dire aux subalternes qu’ils peuvent fixer leurs objectifs eux-mêmes.\nEt pour aller plus loin, le cadre peut ne même pas participer à la réunion de définition des objectifs, en laissant à son équipe le soin de se les définir.\n\n\nLe dirigeant :\nIl reste l’interlocuteur pour les entreprises extérieures qui demandent à \"parler au un chef\".\nIl n’a plus les rôles classiques de dirigeant : stratégie, promotions, budgets etc.\nIl doit occuper quand même deux rôles :\nCréer et garantir l’espace où se déploient les modes opératoires Opales.\nAu moindre souci, il va y avoir diverses personnes proposant de revenir à plus de règles et de surveillance. Le dirigeant est là pour empêcher qu’on aille dans cette direction.\nExemple : Chez RHD une employée avait fait un abus de bien social en donnant une des voitures de la société à son fils. Elle a été renvoyée mais certains collègues ont proposé qu’on mette en place des règles et un contrôle pour empêcher que ça se reproduise.\nLes cas les plus compliqués sont quand la pression vient de l’extérieur, par exemple apposer un tampon qualité alors qu’on n’a pas de département qualité (fonction support supprimée). Dans ce cas, il faut être créatif…\n\n\nÊtre un modèle exemplaire de comportement Opale (en tant qu’autorité morale).\nIncarner l’auto-gouvernance : le dirigeant doit accepter que son pouvoir soit sévèrement limité par la sollicitation d’avis.\nIncarner l’authenticité : en donnant l’exemple, on incite tous les autres à faire de même.\nPar exemple manifester sa vulnérabilité et son humilité en avouant avoir fait l’erreur de ne pas avoir sollicité d’avis avant de décider, et revenir dessus pour respecter la procédure.\n\n\nIncarner l’écoute de la raison d’être : typiquement poser des questions du genre \"Quelle décision sera le plus en ligne avec notre raison d’être ?\", \"Est-ce que la collaboration avec ce client va dans le sens de notre raison d’être ?\".\n\n\n\n\nIl est un collègue comme les autres.\nEn dehors des deux rôles (garantir l’espace, et être exemplaire), le dirigeant participe aux activités normales comme tous les autres, occupe des rôles où il doit à chaque fois prouver son utilité.\n\n\nIl va en général être à l’origine de décisions plutôt importantes, et donc la plupart du temps impliquer beaucoup de personnes dont il faut prendre les avis.\nPour les petites entreprises ça peut être faire le tour de l’entreprise, comme chez FAVI.\nPour les plus grosses, ça peut être poster la proposition sur un blog interne, et voir les résultats qu’on obtient, comme chez Buurtzorg.\n\n\nL’auteur évoque le modèle proposé par Norman Wolfe dans The Living Organization, pour caractériser l’énergie mise en œuvre en entreprise.\nElle se divise en 3 catégories :\nL’activité : le contenu de ce qui est fait et pourquoi.\nLes relations : la manière dont c’est dit pour entrer en relation.\nLe contexte : le rapport au sens et à la raison d’être.\n\n\nDans les entreprises Oranges l’activité prend tout l’espace des décisions venant des dirigeants, les relations sont au minimum, et le contexte inexistant.\nDans les entreprises Opales, les décisions mobilisent en priorité le contexte et les relations, et ensuite seulement l’activité.\n\n\n\n\nLes propriétaires (et conseil d’administration) :\nParmi les entreprises étudiées, deux sont revenues à des paradigmes plus classiques suite à un changement de dirigeant par les propriétaires :\nBSO/Origin :\nInitialement l’entreprise a atteint 10 000 personnes, avec des équipes autonomes, peu de fonctions support et quasi personne au siège.\n20 ans après sa création elle avait été associée à une autre entreprise liée à Philips, qui a pris la majorité du capital de BSO/Origin en 2 ans.\nLes deux mondes sont entrés en conflit, et l’entreprise liée à Philips a assez rapidement réimposé des pratiques managériales classiques.\n\n\nAES :\nL’entreprise a atteint 40 000 personnes et est entrée en bourse.\nPendant des années les actionnaires soutenaient le modèle, mais ils le faisaient parce que ça rapportait.\nSuite à des problèmes économiques liés à des crises, l’action ayant chuté, ils ont paniqué et petit à petit ont tout fait pour revenir à du management classique, jusqu’à la démission du fondateur.\n\n\n\n\nCes exemples montrent que pour une entreprise Opale il faut faire très attention aux propriétaires qu’on fait entrer dans le capital :\nSoit ne pas faire entrer de personnes extérieures et croître éventuellement moins vite.\nSoit ne faire entrer que des personnes convaincues par le paradigme Opale.\n\n\nSelon le paradigme Vert, le pouvoir des actionnaires doit être sévèrement limité au profit des salariés, clients, voisinage, fournisseurs.\nLe paradigme Opale ne propose pas de limiter leur pouvoir, mais propose à la place qu’ils soient convaincus par la raison d’être de l’entreprise, et le fait qu’elle permettra de rapporter plus.\nHolacracy propose un projet législatif pour faire entrer le fonctionnement Opale dans le type juridique de l’entreprise, et empêcher qu’elle puisse facilement se transformer en entreprise classique.\nAux Etats Unis le modèle juridique B-Corporation permet aux entreprises de prendre en compte les problématiques environnementales et sociales, contrairement aux C-Corporations classiques qui ne doivent légalement prendre en compte que l’intérêt financier des actionnaires.","32---créer-une-entreprise-opale#3.2 - Créer une entreprise Opale":"Pour créer une entreprise Opale, il faut d’abord se poser la question de la raison d’être qu’aurait cette entreprise, comme si on cherchait à déterminer la volonté d’un être vivant.\nPour le choix d’éventuels autres cofondateurs :\nIl faut qu’ils soient sensibles à la raison d’être de l’entreprise.\nIl faut qu’ils soient favorables au paradigme Opale.\n\n\nIl faut réfléchir aux divers aspects assez rapidement, parce qu’à mesure que l’entreprise grandit, elle va se stratifier si on n’y fait pas attention.\nIl est utile d’établir des postulats en équipe, pour pouvoir ensuite s’appuyer dessus.\nPar exemple chez RHD :\nTout le monde est d’égale valeur ; sauf preuve du contraire.\nTout le monde est fondamentalement bon.\nIl n’y a jamais une seule bonne façon de résoudre les problèmes qui se posent dans l’entreprise.\n\n\nIl peut être utile d’expliciter les postulats des entreprises classiques :\nLes ouvriers sont paresseux et ne sont pas dignes de confiance.\nLes échelons supérieurs ont la réponse à toutes les questions.\nLes salariés ne sont pas capables de supporter les mauvaises nouvelles.\netc.\n\n\n\n\nPour les pratiques liées à l’auto-gouvernance :\nOn peut soit appliquer un cadre existant, et dans ce cas l’auteur conseille de regarder du côté de l’Holacracy : il y a de la documentation et des consultants si besoin.\nOu alors on peut personnaliser le fonctionnement, et dans ce cas il faut faire attention à 3 points :\nLa sollicitation d’avis : personne n’est là pour \"approuver\" les décisions.\nUn mécanisme de résolution des conflits : en face à face et éventuellement avec un médiateur, voire un jury.\nUn processus d’évaluation entre pairs : les pairs s’évaluent, y compris au niveau du montant du salaire.\n\n\n\n\nPour les pratiques liées à la plénitude :\nIl faut s’assurer d’instaurer une sécurité psychologique, en rédigeant une charte de comportements admis ou exclus, toujours en équipe.\nOrganiser le lieu de travail pour qu’il permette de bien s’y sentir.\nPréparer le processus d’intégration : accueil, formation initiale etc.\nÉtablir une méthode de réunion : ça peut être des choses simples comme commencer par une minute de silence pour se reconnecter à son âme, ou quelque chose de plus structuré comme les réunions de Holacracy ou Buurtzorg.\n\n\nPour les pratiques liées à la raison d’être :\nDeux outils pour aider à travailler la raison d’être :\nLe recrutement : les candidats peuvent examiner leur vocation personnelle et sa résonance avec la raison d’être de l’entreprise.\nLe rituel de la chaise vide : à la fin (ou à tout moment) de chaque réunion, un membre de l’équipe peut occuper la chaise représentant la raison d’être de l’entreprise, et répondre à une question comme \"Est-ce que cette réunion a été vraiment utile à l’entreprise ?\".","33---transformer-une-entreprise#3.3 - Transformer une entreprise":"3 des entreprises étudiées sont intéressantes pour ce chapitre :\nFAVI est passée d’une méthode classique hiérarchique à Opale.\nAES était Opale au début, mais elle a par la suite racheté une dizaine de centrales existantes, et les a converties à son fonctionnement (c’était des années avant qu’elle-même ne revienne vers un mode classique).\nHolacracyOne aide justement des entreprises existantes à passer au mode Opale.\n\n\nPour commencer il faut les deux conditions indispensables : un DG à qui le mode Opale tient à cœur, et des propriétaires qui comprennent et soutiennent l’idée.\nOn ne peut pas tout bouleverser d’un coup. Il vaut mieux commencer par un des 3 piliers (auto-gouvernance, plénitude, raison d’être évolutive).\nLes trois piliers sont de toute façon liés, les uns déclenchant les autres.\n\n\nEtablir l’auto-gouvernance :\nEn général les salariés qui sont en bas de l’échelle s’habituent vite à l’autonomie et en sont contents. Seules quelques rares personnes quittent l’entreprise.\nSi on a des salariés qui ont intégré le mode de fonctionnement hiérarchique pendant des années, il se peut qu’ils mettent du temps à acquérir le sens de la responsabilité qui va avec la liberté donnée par l’autonomie. On peut les y aider.\nLa raison d’être : elle doit être claire et bien réfléchie pour motiver.\nL’émulation : on peut proposer aux salariés de préparer des objectifs, un plan et un budget, avec dans l’idée de faire voter les équipes entre elles pour élire les meilleurs plans. La partie importante ici est le processus de préparation qui va faire entrer les équipes dans un cercle vertueux.\nLa pression du marché : laisser les équipes au contact des clients (si c’est possible), pour qu’il sachent ce que le client pense de leur travail, s’ils ont un prix compétitif etc.\n\n\nLes cadres et les fonctions support vont être plus problématiques, parce qu’ils vont au mieux perdre du pouvoir, et au pire perdre leur travail.\nIl faut leur faire comprendre qu’il n’y aura pas de retour en arrière, et qu’on peut les recaser dans d’autres rôles, au sein des équipes auto-organisées.\nUne bonne partie reste, et quelques-uns partent, c’est du moins ce qui s’est passé chez FAVI et AES.\n\n\nOn peut établir l’auto-organisation de 3 manières :\nLe chaos récréatif : il s’agit pour le dirigeant de prendre une décision forte de se séparer d’une fonction support (par exemple planning), d’un niveau hiérarchique (par exemple contremaîtres) ou d’un outil stratégique (par exemple pointeuses dans une usine).\nEt espérer que le chaos qui en résultera laissera rapidement place à l’auto-organisation des salariés.\nÇa marche mieux si les salariés se sont déjà appropriés leur travail et font confiance au dirigeant.\n\n\nLa redéfinition par la base : organiser des ateliers avec l’ensemble des salariés, pour définir la nouvelle structure et les nouveaux processus (comme la sollicitation d’avis, la transparence de l’information et l’évaluation par les pairs etc.).\nIl faut que les salariés soient suffisamment impliqués et fassent confiance au dirigeant, et que les fonctions support et les cadres n’aillent pas jusqu’à saboter l’initiative.\nOn peut commencer par faire des présentations du concept d’auto-organisation, acheter ce livre ou d’autres du genre pour l’offrir aux salariés, leur faire visiter des entreprises auto-organisées.\n\n\nLe modèle prêt à l'emploi : utiliser par exemple le modèle de Holacracy.\nOn définit une structure constituée de cercles emboîtés, avec des modes de fonctionnement holacratiques, et on décide d’un jour de bascule où on adoptera la nouvelle organisation.\nLe changement peut être progressifet s’adapter petit à petit à la raison d’être de l’entreprise.\nOn a aussi d’autres sources que Holacracy :\nL’institut de l’autogouvernance créé par Morning Star donne des formations.\nBuurtzorg a publié de nombreux documents sur leur fonctionnement.\n\n\n\n\n\n\n\n\nCréer les conditions de la plénitude :\nIntroduire des processus qui permettent à chacun d’être pleinement soi-même est plus facile qu’introduire l’auto-gouvernance.\nOn n’aura pas les cadres contre soi, et on peut y aller avec un rythme plus tranquille, contrairement à l’auto-gouvernance où il faut mettre en place des pratiques de remplacement rapidement (gestion des conflits, rôles, salaires etc.).\n\n\nOn a deux manière d’introduire les pratiques de la plénitude :\nL’introduction graduelle :\nOn doit d’abord être soi-même dans l’authenticité, et ensuite on peut raconter l’importance que ça a pour nous à travers des histoires personnelles.\nUne autre possibilité est aussi de réfléchir à la raison d’être de l’entreprise, et de son lien avec le fait que chacun soit lui-même, en général on finit par trouver un lien.\nOn peut alors introduire les techniques petit à petit, d’abord en réunion ou à petite échelle, puis en généralisant si ça prend bien.\n\n\nL’introduction globale :\nOn va organiser des événements globaux impliquant l’ensemble des salariés par groupes, pour les faire réfléchir à ce que représente le fait d’être soi-même pour eux.\nOn leur demande de raconter des histoires personnelles de moments où ils étaient eux-mêmes.\nPuis on leur demande de trouver des éléments concrets qui pourraient faire qu’ils pourraient être eux-mêmes au travail, et enfin ce qu’ils seraient prêts à mettre en place.\n\n\n\n\n\n\nCréer les conditions de la réalisation de la raison d’être évolutive :\nIl ne s’agit pas de trouver une raison d’être bullshit du genre \"Nous mettons toute notre énergie à produire les meilleurs bidules du pays, dépassant les attentes de nos clients, offrant des perspectives passionnantes à nos collaborateurs et des dividendes copieux à nos actionnaires\".\nIl faut que la raison d’être soit ce qu’on pense que l’entreprise doit faire d’utile dans le monde de par son existence, c’est pour ça qu’on prend la métaphore de l’être vivant qui a une raison d’être qu’il faut écouter.\nTrouver la raison d’être peut prendre du temps. Ça peut passer par des moments de silence, l’utilisation de méthodes particulières comme la théorie U ou la démarche appréciative.\n\n\nUne fois qu’on a trouvé la raison d’être, il faut s'appuyer dessus pour les décisions importantes, mais aussi pour les décisions ordinaires.\nOn peut aussi s’appuyer dessus pour entamer d’autres transformations : revoir le processus de recrutement, la stratégie marketing etc.","34---les-résultats#3.4 - Les résultats":"De même que ça a été le cas par le passé avec les précédents paradigmes, nous sommes arrivés à un point où nous sommes limités par le paradigme dominant actuel.\nLes entreprises Opales sont plus efficaces, mais elles ne le sont pas pour les mêmes raisons que les entreprises Oranges par exemple. Elles le sont pour poursuivre une raison d’être qui a du sens.\nL'étude faite dans le cadre de ce livre ne permet pas de valider quoi que ce soit d’un point de vue scientifique pour plusieurs raisons : taille de l’échantillon, difficulté à définir et à chiffrer la performance (dans le cas des entreprises Opales ce serait principalement au regard de leur raison d’être) etc.\nPar contre ce qu’on peut faire c’est examiner les exemples :\nBuurtzorg :\nIls sont passés de 10 salariés en 2006 à 7000 en 2013.\nEn 2012, ils ont dégagé un excédent de 7% de leur chiffre d'affaires.\nAu regard de leur raison d’être, qui est la qualité de la prise en charge :\nRéduction de 40% du temps pendant lequel les patients ont besoin de soin par rapport aux concurrents.\nDemande d’admission aux urgences réduites d’un tiers, et séjour en hôpital plus court.\n\n\nErnst & Young estime que les économies pour la sécurité sociale néerlandaise monteraient à 2 milliards d’euros si toutes les entreprises de service à domicile avaient la même performance.\n\n\nFAVI :\nEntre les années 1980 où elle est passée au mode Opale et 2015, elle est passée de 80 salariés à 500 (ce qui est d’ailleurs une de leur raison d’être : donner du travail aux personnes de cette région rurale).\nLes salariés sont payés l’équivalent de 17 ou 18 mois de salaire par rapport au marché, et l'entreprise réalise quand même 5 à 7% de marge.\nElle a aussi montré une grande résilience en situation de crise en maintenant les effectifs malgré une chute de 30% de son chiffre d’affaires en 2008.\nIls sont connus pour la qualité de leur pièces et leur absence de retard de livraison.\n\n\nLes autres organisations étudiées ont plus ou moins le même genre de caractéristiques : croissance, réussite financière, salaires plus élevés, résilience aux crises.\n\n\nPourquoi une telle performance ?\nLa puissance est démultipliée quand tout le monde a du pouvoir.\nLe pouvoir est utilisé à meilleur escient quand chacun est lui-même.\nOn est plus efficace quand tout le monde s’aligne sur la raison d’être de l’entreprise.\nLa fluidité des rôles permet d’être plus flexible.\nOn perd moins de temps à défendre son égo, à jouer des coudes pour une promotion, rejeter la faute sur les autres etc.\nOn perd moins de temps sur des mécanismes de contrôle.\nOn perd moins de temps en réunion puisqu’il n’y a pas besoin de faire remonter et descendre l’info dans une hiérarchie qui n’existe pas.\nLa sollicitation d’avis permet d’intégrer les bonnes personnes au bon niveau de décision.\nLes décisions étant décentralisées, plus de décisions peuvent être prises.","35---entreprises-opales-et-sociétés-opales#3.5 - Entreprises Opales et sociétés Opales":"Ce chapitre tente une analyse prospective plus globale de la société en comparant avec l’émergence des paradigmes passés et poussant plus loin les tendances actuelles.\nLe passage au stade Ambre a fait passer la civilisation au modèle féodal agricole et à la religion organisée, le passage à l’Orange a permis des révolutions scientifiques et industrielles, la république libérale.\nIl est tout à fait probable selon l’auteur que le paradigme Opale permette des changements tout aussi conséquents.\n\n\nLes entreprises Opales d’aujourd’hui sont des pionnières, les analyser c’est comme analyser l’automobile en 1900 : elles ont fini par se multiplier, et mener à goudronner les routes et allonger les distances.\nA quoi pourrait ressembler une société évolutive Opale ?\nCroissance zéro : étant donné les problèmes environnementaux et le manque de ressources, il s’agira de zéro déchet, zéro toxicité et 100% recyclage.\nCroissance dans le domaine relationnel et spirituel : les services sont amenés à remplacer les objets de consommation. La publicité et les centres commerciaux vont disparaître.\nTous les secteurs seront bouleversés : l’agriculture intensive pour des pratiques biologiques, l’éducation laissant la place à une approche plus holistique, la médecine qui serait plus spirituelle, le milieu judiciaire et carcéral basé sur la réparation plutôt que la punition.\nL’apparition de monnaies alternatives : typiquement des monnaies fondantes où il ne s’agirait plus d’être en confiance parce qu’on met de côté, mais parce qu’on a un tissu serré de relations au sein de la communauté.\nLa propriété d’usage : la propriété d’usage (que l’auteur appelle “gérance”) pourrait remplacer le modèle de propriété actuel. On aurait le contrôle sur quelque chose qu’on utilise, et on perdrait ce contrôle si on n’en a plus l’usage.\nCommunautés globales : alors que les problèmes énergétiques pourraient nous obliger à relocaliser, les technologies de télécommunications pourraient permettre l’émergence de communautés virtuelles, en poursuivant la tendance actuelle.\nLa fin du travail contraint : avec la mécanisation et la technologie numérique, la plupart des tâches indispensables seront automatisées, ce qui permettra aux gens de ne plus avoir à travailler au sens courant du mot, mais à “exprimer ce qu’il est de manière créative”.\nLa démocratie évolutive : faire participer les citoyens grâce à la technologie, et peut être se mettre collectivement à l’écoute du monde plutôt que projeter sur le monde ce que veulent les citoyens.\nLe ré-enchantement spirituel : plutôt que les anciennes religions (Ambre et avant) ou le matérialisme athée (Orange), le paradigme Opale irait vers une spiritualité non religieuse.\nEffondrement ou transition progressive ? La transition pourrait passer par une prise de conscience et la multiplication des entreprises Opales, mais nous pourrions aussi être rattrapés par les problématiques écologiques, dont certaines risquent de poser de sérieux problèmes dans quelques décennies.\n\n\nA quoi ressembleraient les entreprises Opales dans une société Opale ?\nSi on part du principe que la propriété d’usage est généralisée, et que la monnaie perd en valeur dans le temps de manière à ne pas pouvoir être mise de côté, alors investir dans une entreprise pour tirer un revenu plus grand n’a juste plus de sens.\nLes notions de lucratif et non lucratif perdent leur spécificité pour ce qui est de caractériser les entreprises : il n’y aurait que des salariés associés contribuant à l’entreprise, et en cas de difficulté de l’un d’entre eux, l’entreprise pourrait l’aider en fonction de ses excédents.\nLes entreprises Opales permettent une plus grande flexibilité : on peut imaginer réduire son temps de travail pour faire autre chose ou s’engager en partie ailleurs. Tout est négocié entre collègues.\nLa raison d’être étant au centre, les entreprises elles-mêmes sont moins importantes, et donc leurs frontières peuvent devenir poreuses. Des personnes ou des équipes pourraient contribuer dans une structure pendant un temps, puis (ou même en parallèle) dans une autre au service de la même raison d’être.\nEn fait, le concept même d’entreprise peut perdre en pertinence au profit de la raison d’être évolutive, autour de laquelle graviteraient des personnes et des groupes de personnes.\n\n\nLe modèle des entreprises Opales ne demande qu’à être généralisé et poussé plus loin. C’est à nous de construire le futur."}},"/books/unit-testing":{"title":"Unit Testing: Principles, Practices, and Patterns","data":{"":"","i---the-bigger-picture#I - The bigger picture":"","1---the-goal-of-unit-testing#1 - The goal of unit testing":"De nos jours, la plupart des entreprises créent des tests pour leurs logiciels.\nEn moyenne le ratio code de test / code de prod est entre 1/1 et 3/1 (en faveur du code de test), et parfois plus.\n\n\nLe but principal des unit tests c’est de permettre une croissance durable du projet. Sans eux, le temps de développement explose au bout d’un moment.\nOn appelle cette explosion la software entropy : la désorganisation progressive du code.\n\n\nLe fait qu’un code soit difficilement testable est un signe de mauvaise conception à cause d’un couplage inapproprié. C’est un bon indicateur négatif. Par contre, si le code est testable, ça ne veut pas dire qu’il est bon, on ne peut pas en faire un indicateur positif.\nLes tests sont un code comme un autre, ils ont un coût de maintenance, et peuvent avoir une valeur nulle ou même négative. Il vaut mieux ne garder que les bons tests.\nLe code coverage est un bon indicateur négatif : si le code coverage est faible c’est que le code est peu testé. Par contre, si le coverage est élevé, on ne peut rien conclure : c’est un mauvais indicateur positif.\nLe problème du coverage c’est :\nQu’on ne peut pas s’assurer qu’on vérifie tout ce qui est fait. Par exemple, si un code renvoie un résultat et assigne ce résultat dans une variable globale, et que le test vérifie seulement l’une de ces choses, on ne pourra pas savoir que l’autre n’est pas testée malgré le coverage de 100%.\nQu’on ne teste pas les chemins permis par nos dépendances. On délègue souvent des responsabilités à des dépendances qui permettent beaucoup de flexibilité, mais sans tester chaque possibilité offerte. Et on ne peut pas vérifier qu’on le fait.\nLe simple fait de faire un parseInt(variable) fera que variable marchera dans des cas précis supportés par la fonction standard parseInt(). Pour autant, on ne peut pas s’assurer de tester chacun de ces chemins et leurs conséquences avec notre code.\n\n\n\n\nSe fixer le coverage comme target crée un incentive pervers qui va à l’encontre de l’objectif du unit testing. Le coverage doit rester un indicateur (négatif).\n\n\nLe branch coverage est une autre forme de coverage qui compte le nombre d’embranchements (if, switch etc.) testés sur le nombre total d’embranchements. C’est un peu mieux que le code coverage, mais ça reste pour autant seulement un indicateur négatif.\nUn bon test c’est un test qui :\nest intégré au cycle de développement, exécuté le plus souvent possible.\nteste les parties les plus importantes de la codebase. En général c’est la logique business.\noffre une grande valeur comparé aux coûts de sa maintenance.","2---what-is-a-unit-test#2 - What is a unit test?":"Il existe deux écoles de unit testing : la classical school (qu’on pratiquait à l’origine), et la London school, qui est née à Londres.\nUn livre canonique pour le style classique est Test-Driven Development: By Example de Kent Beck, et un livre pour le style London est Growing Object -Oriented Software, Guided by Tests de Steve Freeman et Nat Pryce.\n\n\nUn unit test est un test qui vérifie une unité de code, de manière rapide, et isolée.\nLe code testé peut avoir des dépendances.\nLes shared dependencies sont celles qui affectent les tests entre eux parce qu’ils sont changés par le code et ne sont pas réinitialisés entre les tests. Par exemple, une base de données est shared. Elle pourrait ne pas l’être si elle était instanciée à chaque test.\nLes private dependencies sont celles qui ne sont pas partagées.\nLes out-of-process dependencies sont celles qui sont exécutées dans un autre processus. Elles impliquent un temps d’exécution plus important que de rester dans le même processus que le code exécuté. La base de données est out-of-process, même si on l’instancie à chaque fois.\nLes volatile dependencies sont soit non installées sur un environnement par défaut (c’est le cas d’une base de données, mais pas d’un filesystem par exemple), soit on un comportement non déterministe (par exemple new Date()).\nA propos de la gestion de dépendances, l’auteur conseille Dependency Injection: Principles, Practices, Patterns de Steven Deursen et Mark Seemann.\n\n\nOn appellera par la suite\ncollaborators les dépendances qui sont soit shared, soit mutables (un objet utilisé par l’objet qu’on teste, la base de données etc.)\nvalues les dépendances qui sont immutables (par exemple un Value Object, le nombre 5 etc.).\n\n\nLa controverse entre les deux écoles porte sur l’isolation :\nPour la London school l’isolation porte sur le code testé.\nTout collaborator qui n’est pas directement testé doit être remplacé dans les tests par un test double (c’est le terme générique, le terme mock est une forme particulière de test double). On tolère seulement les dépendances immutables (les values).\nLe “unit” c’est l’unité de code (la classe), donc on a un fichier de test par classe.\nAvantages :\nÇa permet de tester du code même très couplé, en remplaçant simplement les dépendances dans les tests par des doubles.\nÇa permet d’être sûr que seul un test ne marchera plus si une fonctionnalité ne marche plus.\n\n\nInconvénients :\nÇa ne force pas à faire du code découplé.\nLes tests cassent facilement au moindre refactoring.\n\n\n\n\nPour la classical school l’isolation porte sur les tests entre eux : il faut pouvoir les jouer en parallèle sans qu'ils s'affectent mutuellement.\nOn n’utilise les doubles que très peu, seulement pour éliminer les shared dependencies.\nLe “unit” c’est le comportement (la fonctionnalité), et celle-ci peut contenir plusieurs classes qui seront toutes instanciées indirectement dans les tests.\nAvantages :\nCa force à faire du code découplé.\nCela permet d’avoir des tests qui collent mieux au cas d’usage business, et qui sont moins fragiles aux refactorings.\n\n\nInconvénients :\nSi une fonctionnalité ne marche pas, plusieurs tests peuvent casser.\nMais si on rejoue tous les tests à chaque changement de code, on peut savoir que c’est lui qui vient de faire passer les tests au rouge.\nEt en plus si un changement casse beaucoup de tests, ça permet de savoir que cette partie du code est très importante.\n\n\n\n\n\n\n\n\nLes deux écoles ont aussi une différence dans leur rapport au TDD :\nLa London school va avoir tendance à faire du outside-in TDD, en construisant d’abord les classes de plus haut niveau utilisant des collaborators sous forme de test doubles. Puis les implémenter petit à petit en allant vers le détail.\nLa classical school va plutôt mener à du inside-out TDD, en partant des classes les plus bas niveau dans le modèle, pour construire par-dessus jusqu’aux couches supérieures.\n\n\nUn test d’intégration est un test qui ne répond pas à une des 3 caractéristiques du test unitaire (tester une unité, de manière rapide et isolée).\nPour la London school, les caractéristiques sont :\nVérifier le comportement d’une seule classe.\nLe faire vite.\nLe faire en isolation vis-à-vis des dépendances de cette classe (grâce aux doubles).\n\n\nPour la classical school :\nVérifier le comportement d’une unité de comportement.\nLe faire vite.\nLe faire avec des tests isolés les uns par rapport aux autres.\n\n\nDu coup pour savoir ce qui est test d’intégration :\nLa plupart des tests unitaires selon la classical school sont des tests d’intégration pour la London school puisqu’ils font intervenir plusieurs classes.\nUn test qui teste plusieurs unités de comportement sera un test d’intégration pour la classical school.\nDans le cas où on a une out-of-process dependency (comme une DB) impliquée, les tests sont lents donc on sera sur des tests d’intégration pour les deux écoles.\nSi on a une shared dependency (comme une DB) impliquée, là encore on aura un test d’intégration pour les deux écoles.\n\n\n\n\nUn test end-to-end est un test d’intégration qui teste toutes les dépendances out-of-process (ou la plupart d’entre elles), là où les autres tests d’intégration n’en testent qu’une ou deux (genre juste la DB, mais pas RabbitMQ ou le provider d’emails).\nDans la suite du livre l’auteur va plutôt adopter l’approche classique, parce que c’est celle qu’il préfère et celle qui est la plus courante.","3---the-anatomy-of-a-unit-test#3 - The anatomy of a unit test":"Les tests unitaires doivent être structurés avec les 3 blocs Arrange, Act, Assert (qu’on appelle aussi Given, When , Then) :\nArrange : Il peut être aussi gros que les deux autres sections réunies. S’il est plus gros, il est conseillé de l’extraire dans une fonction pour augmenter la lisibilité.\nAvoir une méthode unique (constructeur de la classe de tests, ou beforeAll / BeforeEach global) est une moins bonne idée puisqu’on couple les tests ensemble, et que ce que possède chaque test est moins clair.\nL’idéal c’est avoir des fonctions de type factory configurables, qu’on peut réutiliser dans les tests en sachant depuis le test à peu près ce qu’on crée.\n\n\nAct : Il ne devrait faire qu’une ligne vu qu’on est censé vérifier une unité de comportement. S’il fait plus, c’est qu’on a la possibilité de faire une partie de la chose et pas l’autre, ce qui peut vouloir dire qu’on est en état de casser un invariant, et donc qu’on a une mauvaise encapsulation de notre code.\nExemple : si notre act c’est deux lignes qui font :\ncustomer.purchase(item);\nstore.removeFromInventory(item);\nC’est que l’un peut être fait sans l’autre au niveau de l’interface publique (celle-là même qui est testée). C’est un danger qu’on s’impose pour rien. Une seule méthode publique devrait faire les deux.\n\n\nAssert : Vu qu’on vérifie une unité de comportement, il peut y avoir plusieurs outcomes, donc plusieurs asserts.\nAttention quand même, si cette section grossit trop, c’est peut être le signe d’une mauvaise abstraction du code. Par exemple, si on doit comparer toutes les propriétés d’un objet un par un, et qu’on aurait pu implémenter l’opérateur d’égalité sur l’objet en question, et ne faire qu’un assert.\n\n\n\n\nOn écrit en général d’abord la partie arrange si on a déjà écrit le code, et d’abord la partie assert si on fait du TDD.\nQuand on teste plusieurs unités de comportement, on se retrouve par définition avec un test d’intégration. Il vaut mieux revenir sur de l’unitaire si possible.\nIl faut éviter les if dans les tests. Ça complique la compréhension et la maintenance.\nPour repérer facilement l’objet qu’on teste, et ne pas le confondre avec des dépendances, l’auteur conseille d’appeler l’objet testé sut (pour System Under Test) :\n// Arrange\nconst sut = new Calculator();\n// Act\nconst result = sut.sum(3, 2);\n// Assert\nexpect(result).toBe(5);\n\nPour bien séparer les 3 sections AAA l’une de l’autre, l’auteur conseille :\nSoit de laisser une ligne entre chaque section.\nSoit, si certaines sections doivent déjà sauter des lignes parce qu’elles sont longues, de laisser en commentaire //Arrange, //Act et //Assert\n\n\nA propos de la manière de nommer les tests :\nVu que les tests testent un comportement, le nom des tests doit être une phrase, qui a du sens pour les experts métier.\nSauf dans le cas où on teste des fonctions utilitaires, qui n’ont donc pas de sens pour les experts métier.\n\n\nIl faut éviter de mettre le nom du SUT (la fonction testée) dans le nom du test. Ça oblige à changer le nom du test si le nom de la fonction change, et ça n’apporte pas grand chose puisque c’est le comportement qui nous intéresse.\nIl vaut mieux être spécifique dans le nom du test. Par exemple, si on teste qu’une date est invalide si elle est au passé, préciser ça plutôt que de rester vague en parlant simplement de vérifier si la date est valide.\n\n\nOn peut utiliser les tests paramétrisés pour grouper des tests dont seule une valeur d’entrée et la valeur attendue change. Par exemple tester avec la date d'aujourd'hui, avec la date de demain, etc.\nAttention quand même, faire ce genre de regroupement a un coût en lisibilité. Donc à faire que si les tests sont simples.\nIl faut éviter de mettre dans le même test les cas positifs et les cas négatifs.\nEn général, le framework de test fournit la possibilité de paramétriser les tests, en acceptant une liste de paramètres à faire varier en entrée du test.","ii---making-your-tests-work-for-you#II - Making your tests work for you":"","4---the-four-pillars-of-a-good-unit-test#4 - The four pillars of a good unit test":"Dans ce chapitre on pose des critères pour évaluer la qualité d’un test.\nUn bon test a 4 caractéristiques fondamentales :\n1- Protéger des régressions : éviter les bugs.\nPour évaluer ce point on peut prendre en compte :\nLa quantité de code exécutée : plus il y en a plus c’est fiable.\nLa complexité du code exécuté.\nL’importance du code : tester du code du domaine est plus utile que de tester du code utilitaire.\n\n\n\n\n2- Résister aux refactorings : à quel point on peut refactorer sans casser le test.\nPour évaluer ce critère, on peut regarder si le test produit souvent des faux positifs pendant les refactorings.\nL’intérêt de ce critère c’est que si on a trop de faux positifs :\nOn porte de moins en moins attention au résultat des tests puisqu’ils disent souvent n’importe quoi : et on laisse passer de vrais bugs.\nOn n’ose plus refactorer le code, puisqu’on n’a pas confiance dans les tests. Et le code pourrit.\n\n\nCe qui fait casser les tests pendant les refactorings c’est souvent le couplage aux détails d’implémentation, au lieu de porter sur un comportement attendu du point de vue métier.\nPour avoir une idée de ce que veut dire “tester les détails d’implémentation”, l’exemple le plus extrême de ce genre serait un test qui vérifierait simplement que le code source de la fonction testée est bien le code source attendu dans le test. Ce test casserait littéralement à chaque changement.\nSans aller jusqu’à cet extrême, on retrouve souvent des tests qui vérifient la structure interne d’un objet, ou qu'une fonction appelle telle ou telle autre fonction etc. sans que ça n’ait aucun intérêt d’un point de vue métier.\n\n\n\n\n3- Donner un feedback rapide : à quel point on peut exécuter le test vite.\nPlus le test est lent, moins souvent on l’exécutera.\n\n\n4- Être maintenable : il y a deux composantes :\nA quel point c’est difficile de comprendre le test. Ça dépend de la taille du test, de la lisibilité de son code.\nA quel point c’est difficile de lancer le test. Par exemple à cause de la database qui doit être en train de tourner etc.\n\n\n\n\nLes deux premiers piliers caractérisent la précision (accuracy) du test.\nLa protection contre les régressions dépend de la capacité à ne pas avoir de faux négatifs (bugs présents mais ratés par les tests). C’est le fait d’avoir le bon signal.\nLa résistance aux refactorings dépend de la capacité à ne pas avoir de faux positifs (fausses alarmes). C’est l’absence de bruit.\nAu début du projet, les faux positifs (les bugs pas couverts) ont la plus grande importance. Mais à mesure que le projet avance, les faux négatifs deviennent de plus en plus gênants et empêchent de garder le code sain en le refactorant.\nDonc si on est sur un projet moyen ou gros, il faut porter une attention égale aux faux positifs et aux faux négatifs.\n\n\n\n\nOn peut noter un test sur chacun des 4 critères, et lui donner une note finale qui nous aidera à décider si on le garde ou non (pour rappel : garder un test n’est pas gratuit, ça implique de la maintenance).\nOn peut évaluer (subjectivement) la valeur du test à chacun des 4 critères, entre 0 et 1, puis multiplier ces quatre valeurs pour avoir le résultat final.\nCa implique donc qu’un test qui vaut zéro à l’un des critères aura une valeur finale de zéro. On ne peut pas négliger un des critères.\n\n\nOn ne peut malheureusement pas obtenir la note maximale partout, parce que les 3 premiers critères ont un caractère exclusif entre eux : on ne peut en avoir que deux parfaits.\nLes tests end to end, par exemple, maximisent la protection vis-à-vis des régressions parce qu’ils exécutent beaucoup de code, et sont résistants aux refactorings vu qu’ils testent depuis ce que voit l’utilisateur final. Par contre ils sont très lents.\nEt si on a des tests très rapides, en général on n’obtiendra pas à la fois un découplage et donc une résistance aux refactorings, et en même temps une capacité à arrêter tous les bugs.\n\n\nLa règle à retenir c’est que la résistance aux refactorings est non-négociable, pour la raison que ce critère est assez binaire : soit on est bien découplé, soit non. Et si on ne l’est pas, la valeur du test passe à zéro.\nLe choix qui reste c’est donc la possibilité de faire varier le curseur entre la rapidité du test, et sa capacité à empêcher les régressions.\n\n\n\n\nSi on examine notre pyramide de tests (unit, integration, e2e), on maximisera d’abord le critère non-négociable de résistance aux refactorings pour tous, puis :\nLes tests unitaires sont les plus rapides et protègent le moins, puis on a les tests d’intégration qui sont au milieu, et les tests e2e sont très lents et protègent le plus.\nEn général on a peu de tests e2e parce que leur extrême lenteur diminue beaucoup leur valeur. Et ils sont aussi difficiles à maintenir.\nPour les projets classiques on aura une pyramide, et pour les projets très simples (CRUD etc.), on pourra se retrouver avec un rectangle.\n\n\nLe black-box testing consiste à tester sans prendre en compte la structure interne, seulement avec les considérations business. Le white-box testing consiste à faire l’inverse.\nLe white-box testing menant à du code couplé aux détails d’implémentations, il n’est pas résistant aux refactorings, donc il ne faut pas l’utiliser (sauf pour analyser).","5---mocks-and-test-fragility#5 - Mocks and test fragility":"Il y a principalement deux types de test doubles :\n1- Les mocks qui aident à émuler et examiner les interactions sortantes, c’est-à-dire le cas où le SUT interagit pour changer l’état d’une de ses dépendances.\nOn pourrait voir le mock comme la commande du pattern CQS.\nIl existe une petite distinction avec les spies qui sont des mocks écrits à la main, alors que les mocks sont en général générés par une librairie de mock.\n\n\n2- Les stubs qui aident à émuler les interactions entrantes, c’est-à-dire le cas où une des dépendances fournit une valeur utilisée par le SUT.\nOn pourrait voir le stub comme la query du pattern CQS.\nIl existe des sous ensemble de stubs :\nle dummy qui est très simple\nle stub qui est plus sophistiqué, et retourne la bonne valeur en fonction du cas\net le fake qui est un stub utilisé pour remplacer un composant qui n’existe pas encore (typique de l’école de Londres).\n\n\n\n\n\n\nLe mot mock peut vouloir dire plusieurs choses, ici on l’utilise pour sa définition principale de sous ensemble de test double, mais parfois il est utilisé pour désigner tous les tests doubles, et parfois il désigne l’outil (la librairie qui permet de créer des mocks et des stubs).\nVérifier les interactions sur des stubs est un antipattern : les stubs émulent des données entrantes, et donc vérifier que le stub a bien été appelé relève du couplage à des détails d’implémentation.\nLes interactions ne doivent être vérifiées que sur des mocks, c’est-à-dire des interactions sortantes, dans le cas où l’appel qu’on vérifie a du sens d’un point de vue business.\n\n\nLa distinction entre comportement observable et détail d’implémentation :\nIl faut d’abord choisir le client qu’on considère, puis vérifier si notre code lui permet :\nSoit d’exécuter une opération pour faire un calcul ou un side effect pour atteindre ses objectifs.\nSoit d’utiliser un état pour atteindre ses objectifs.\n\n\nSi oui, alors on a un comportement observable, si non alors notre code est un détail d’implémentation.\nLe choix du client considéré est important, on reviendra sur cet aspect dans la suite.\n\n\nSi l’API publique coïncide avec le comportement observable, alors on dira que notre système est bien conçu.\nSinon, on dira qu’il fait fuiter des détails d’implémentation. Parce que des détails d’implémentation pourront alors être accédés de manière publique sans protection (sans encapsulation).\nExemple : Le cas où le renommage de l’utilisateur se faisait en deux temps : renommer, puis appeler la fonction de normalisation qui coupe le nom à 50 caractères max. Ici la fonction de normalisation ne permet d’atteindre aucun objectif du client qui l’appelle (il voulait juste renommer), pourtant elle est publique. On a donc un problème de fuite.\nUn bon moyen de savoir si on fait fuiter des détails d’implémentation, c’est de voir les cas où on a besoin de plus d’une opération pour atteindre un objectif du client (le “act” du test).\n\n\n\n\nL’architecture hexagonale consiste en plusieurs hexagones communiquant entre eux.\nChaque hexagone est constitué de deux couches :\nLe domain layer qui n’a accès qu’à lui-même et qui contient les règles et invariants business de l’application.\nIl est une collection de domain knowledge (how-to).\n\n\nL’application service layer qui orchestre la communication entre le domain layer et le monde externe. Elle instancie des classes importées du domain layer, leur donne les données qu’elle va chercher en base, les sauve à nouveau en base, répond au client etc.\nElle est est une collection de use-cases business (what-to).\n\n\n\n\nLe terme hexagone est une image, chaque face représente une connexion à un autre hexagone, mais le nombre n’a pas besoin d’être 6.\nAu sein de chaque couche, le client est la couche d'au-dessus, et donc ce sont ses objectifs qui sont pris en compte pour savoir si on lui expose des détails d’implémentation ou non.\nLes objectifs du client final sont transcrits en objectifs secondaires dans la couche du dessous, et donc on a une relation fractale qui permet à tous les tests d’avoir toujours un rapport avec un requirement business (Les objectifs de l’application service layer sont des sous-objectifs du client final).\nNDLR : un peu comme les OKR.\n\n\n\n\nExemple :\n// domain layer\nclass User {\nsetName(newName: string) {\n// On normalise et on set la valeur.\n}\n}\n// application service layer\nclass UserController {\nrenameUser(userId: number, newName: string) {\nconst user: User = getUserFromDatabase(userId);\nuser.setName(newName);\nsaveUserToDatabase(user);\n}\n}\n\n\n\nPour savoir quand utiliser les mocks sans abimer la résistance au refactoring, il faut se demander si l’interaction sortante qu’on veut vérifier est interne à notre application (notre hexagone par exemple), ou porte vers des systèmes externes.\nSi l’interaction est interne, alors il ne faut pas mocker, même s'il s’agit d’une dépendance out-of-process comme une base de données. Tant qu’elle n’est visible que depuis notre application, elle est un détail d’implémentation pour nos clients.\nSi l’interaction est externe, et donc visible par nos clients externes, alors il faut vérifier qu’elle se fait correctement par un mock. Par exemple, l'envoi d’un email répond à un besoin client, donc il faut vérifier que l’appel vers le système externe se fait correctement.\nPour parler un peu des écoles : l’école de Londres préconise de mocker toutes les dépendances mutables, ça fait beaucoup trop de mocks. Mais l’école classique préconise de mocker aussi des choses en trop : typiquement la base de données qui est une shared dependency. On peut au lieu de mocker nos interactions avec elle, la remplacer intelligemment par autre choses dans nos tests (cf. les deux prochains chapitres).","6---styles-of-unit-testing#6 - Styles of unit testing":"Il y a 3 “styles” de tests :\nOutput-based : c’est quand il n’y a pas de side effect, et qu’on teste une fonction qui prend des paramètres, et renvoie quelque chose. Il s’agit de fonction pure, donc de programmation fonctionnelle.\nState-based : on fait une opération, et on vérifie l’état d’un objet.\nCommunication-based : on utilise des mocks pour vérifier qu’un appel à une fonction a été fait avec les bons paramètres.\n\n\nA propos des écoles de test :\nL’école classique préfère le state-based plutôt que la communication based.\nL’école de Londres fait le choix inverse.\nEt toutes les deux utilisent l’output-based testing quand c’est possible.\n\n\nOn peut comparer les 3 styles de test vis-à-vis des 4 critères d’un bon test :\nPour la protection contre les régressions et la rapidité de feedback les 3 styles se valent à peu près.\nConcernant la résistance au refactoring :\nL’output-based testing offre la meilleure résistance parce que la fonction se suffit à elle-même.\nLe state-based testing est un peu moins résistant parce que l’API publique exposée est plus importante, et donc les chances de faire fuiter des détails d’implémentation dans la partie publique sont plus grandes.\nLe communication-based testing est le plus fragile, et nécessite une grande rigueur pour ne pas coupler à des détails d’implémentation.\n\n\nConcernant la maintenabilité : c’est à nouveau l’output-based qui est le plus maintenable parce que prenant le moins de place, suivi du state-based, et enfin du communication-based qui prend beaucoup de place avec ses mocks et stubs.\nGlobalement l’output-based testing est le meilleur, mais il nécessite d’avoir du code écrit de manière fonctionnelle.\n\n\nA propos de la programmation fonctionnelle, l’auteur conseille les livres de Scott Wlaschin.\nPour pouvoir faire de l’output-based testing, il faut écrire du code avec des fonctions pures, c’est-à-dire qui renvoient le même résultat à chaque fois qu’on donne les mêmes paramètres, sans qu’il n’y ait d’inputs ou d’outputs cachés.\nParmi ces choses cachées, on a :\nLes side-effects : des outputs cachés, par exemple la modification d’un état d’une classe, l’écriture dans un fichier etc.\nLes exceptions : elles créent un chemin alternatif à celui de la fonction, et peuvent être traitées n’importe où dans la stack d’appel.\nLa référence à un état interne ou externe : un input caché qui va permettre de récupérer une valeur qui n’est pas indiquée dans la signature de la fonction.\n\n\nPour savoir si on a une fonction pure, on peut essayer de remplacer son appel par la valeur qu’elle devrait renvoyer, et vérifier que le programme ne change pas de comportement. Si oui on a une referential transparency.\n\n\nL’architecture fonctionnelle consiste à maximiser la quantité de code écrite de manière fonctionnelle (fonctions pures, avec valeurs immutables), et confiner le code qui fait les side-effects à un endroit bien précis.\n1- Il y a le code qui prend les décisions, qui est sous forme de fonctions pures. C’est le functional core.\n2- Et le code qui agit suite aux décisions, qui prend les inputs et crée les side-effects (UI, DB, message bus etc.). C’est le mutable shell.\nOn va couvrir le functional core par de nombreux tests unitaires output-based, et couvrir le mutable shell qui est la couche d’au-dessus par des tests d’intégration moins nombreux.\nL’architecture fonctionnelle est en fait un cas particulier de l’architecture hexagonale :\nLes deux ont bien deux couches organisées par inversion de dépendance.\nLa différence principale c’est que l’architecture fonctionnelle exclut tout side-effect du functional core vers le mutable shell, alors que l’architecture hexagonale permet les side-effects dans la couche domaine tant que ça n’agit pas au-delà de cette couche (DB par exemple).\n\n\n\n\nExemple d’application peu testable, refactorée vers la functional architecture :\nDescription :\nOn a un système d’audit qui enregistre tous les visiteurs d’une organisation.\nLe nom de chaque visiteur et la date sont ajoutés à un fichier de log.\nQuand le nombre de lignes max du fichier est atteint, on écrit dans un autre fichier.\n\n\nInitialement la classe AuditManager a une méthode addRecord() qui va lire les fichiers existants, les classer pour trouver le dernier. Puis vérifier s’il est plein pour soit écrire dedans, soit écrire dans dans un nouveau.\nLa logique et la lecture/écriture sont dans la même fonction. Donc les tests vont être à la fois lents, et difficiles à paralléliser à cause de la dépendance out-of-process partagée qu’est le filesystem.\npublic class AuditManager {\nconstructor(\npublic maxEntriesPerFile: number,\npublic directoryName: string\n) {}\n\naddRecord(visitorName: string, timeOfVisit: Date) {\n// Get all files in the given directory\nconst fs = require('fs');\nconst files = fs.readdirSync(directoryName);\n// Build the record content\n\n// If no file, create one with our record\nfs.writeFile(...\n\n// Sort by file name to get the last one\n// If file's lines do no exceed max, write inside\n// Otherwise create a new file and write inside\n}\n}\n\n\n\nUne 1ère étape est d’utiliser des mocks pour découpler le filesystem de la logique :\nUne des manières de faire ça c’est d’injecter un objet qui respecte une interface IFileSystem, qui sera soit le vrai filesystem, soit un mock dans les tests.\nLe mock va à la fois servir de stub pour renvoyer le contenu des fichiers, et aussi de mock pour vérifier qu’on appelle bien la bonne fonction avec les bons paramètres pour écrire dans le filesystem. L’usage du mock ici est légitime parce que ces fichiers sont user-facing.\npublic class AuditManager {\nconstructor(\npublic maxEntriesPerFile: number,\npublic directoryName: string,\npublic fileSystem: IFileSystem,\n) {}\n\naddRecord(visitorName: string, timeOfVisit: Date) {\nconst files = fileSystem.readdirSync(directoryName);\n// Build the record content\n\n// If no file, create one with our record\nfileSystem.writeFile(...\n\n// Sort by file name to get the last one\n// If file's lines do no exceed max, write inside\n// Otherwise create a new file and write inside\n}\n}\n\nit(\"creates a new file when the current file overflows\", () => {\nconst fileSystemMock: IFileSystem = {\nreaddirSync: () => [\"audits/audit_1.txt\", \"audits/audit_2.txt\"],\nwriteFile: jest.fn(),\n// ...\n};\nconst sut = AuditManager(3, \"audits\", fileSystemMock);\n\nsut.addRecord(\"Alice\", new Date(\"2019-04-06\"));\n\nexpect(fileSystemMock.writeFile).toHaveBeenCalledTimes(1);\nexpect(fileSystemMock.writeFile).toHaveBeenCalledWith(\n\"audits/audit_3.txt\",\n\"Alice; 2019-04-06\"\n);\n});\n\nOn n’a rien changé à la protection contre les régressions et à la résistance aux refactorings. Par contre on a rendu les tests plus rapides, et on a un peu amélioré la maintenabilité parce qu’on n’a plus à se préoccuper du filesystem. Mais le setup des mocks est verbeux, on peut faire mieux sur la maintenabilité.\n\n\nLa 2ème étape est de refactorer vers la functional architecture :\nAuditManager ne connaît plus du tout l'existence du filesystem : il reçoit des valeurs en entrée (une liste de FileContent à partir duquel il lira le contenu des fichiers), et renvoie des valeurs en sortie : une liste de FileUpdate qui contiendront les contenus à changer).\nclass AuditManager {\nconstructor(public maxEntriesPerFile: number) {}\n\naddRecord(\nfiles: FileContent[],\nvisitorName: string,\ntimeOfVisit: Date\n) {\n// Build the record content\n// If no file, create one with our record\nif (files.length === 0) {\nreturn new FileUpdate(\"audit_1.txt\", newRecord);\n}\n\n// Sort by file name to get the last one\n// If file's lines do no exceed max, write inside\n// Otherwise create a new file and write inside\n}\n}\n\nclass FileContent {\nconstructor(public fileName: string, public lines: string[]) {}\n}\nclass FileUpdate {\nconstructor(public fileName: string, public newContent: string) {}\n}\n\nOn a une classe Persister qui va permettre de lire tous les fichiers, et de renvoyer leurs informations sous forme de FileContent, et une autre méthode pour prendre une liste de FileUpdate, et les appliquer sur le filesystem. Il doit être le plus simple possible pour que le max de logique soit dans AuditManager.\nconst fs = require(\"fs\");\n\nclass Persister {\nreadDirectory(directoryName: string): FileContent[] {\nreturn fs.readdirSync(directoryName).map((file) => {\nreturn new FileContent(file.name, file.lines);\n});\n}\n\napplyUpdate(filePath: string, update: FileUpdate) {\nfs.writeFile(filePath, update);\n}\n}\n\nPour faire fonctionner ensemble le functional core (AuditManager), et le mutable shell (Persister), on a besoin d’une autre classe de type “application service” (pour utiliser la terminologie de l’hexagonal architecture).\nIl va manipuler manipuler Persister pour obtenir les données des fichiers, les donner à une instance d’AuditManager, puis appeler la méthode de calcul sur AuditManager, récupérer les commandes d’écriture en sortie, et les donner à Persister pour mettre à jour le filesystem.\nclass ApplicationService {\nconstructor(public directoryName: string, maxEntriesPerFile: number) {\nthis.auditManager = new AuditManager(maxEntriesPerFIle);\nthis.persister = new Persister();\n}\n\naddRecord(visitorName: string, timeOfVisit: Date) {\nconst files: FileContent[] = this.persister.readDirectory(\nthis.directoryName\n);\nconst update: FileUpdate = this.auditManager.addRecord(\nfiles,\nvisitorName,\ntimeOfVisit\n);\nthis.persister.applyUpdate(this.directoryName, update);\n}\n}\n\n\n\nOn a gardé les précédents avantages, et on a amélioré la maintenabilité en éliminant le setup de mocks verbeux, remplacés par la simple instanciation de valeurs mis dans les objets FileContent et FileUpdate.\nit(\"creates a new file when the current file overflows\", () => {\nconst sut = new AuditManager(3);\nconst files = [\nnew FileContent(\"audits/audit_1.txt\", []),\nnew FileContent(\"audits/audit_1.txt\", [\n\"Peter; 2019-04-06\",\n\"Jane; 2019-04-06\",\n\"Jack; 2019-04-06\",\n]),\n];\n\nconst update = sut.addRecord(files, \"Alice\", new Date(\"2019-04-06\"));\n\nexpect(update.fileName).toBe(\"audit_3.txt\");\nexpect(update.newContent).toBe(\"Alice; 2019-04-06\");\n});\n\nPour rester sur du fonctionnel, on peut renvoyer les erreurs par valeur de retour, et décider de quoi en faire dans l’application service.\n\n\n\n\nLa functional architecture n’est pas toujours applicable.\nElle permet d’avoir des avantages en termes de maintenabilité du code et des tests, mais elle a des désavantages :\nLe code pourra être un peu plus gros pour permettre la séparation entre logique et side effects.\nLe code pourra souffrir de problèmes de performance.\nDans notre cas, ça a marché parce qu’on lisait tous les fichiers avant d’appeler la logique en donnant tous ces contenus et la laissant décider. Si on avait voulu n’en lire que certains en fonction de paramètres décidés par la logique, on n’aurait pas pu la garder comme fonction pure.\nUne autre solution aurait pu être de concéder un peu de centralisation de la logique dans le core en faveur de la performance, en laissant la décision de charger les données ou non à l’application service.\n\n\n\n\nIl faut donc appliquer la functional architecture stratégiquement.\nNe pas sacrifier la performance si elle est importante dans le projet.\nL’appliquer si le projet est censé durer dans le long terme, et que l’investissement initial de séparer en vaut la peine.\n\n\n\n\nEn général (surtout si on fait de l’OOP), on aura une combinaison de tests state-based et output-based, et quelques tests communication-based.\nLe conseil ici c’est de privilégier les tests output-based quand c’est raisonnablement possible.","7---refactoring-toward-valuable-unit-tests#7 - Refactoring toward valuable unit tests":"Les tests et le code sont profondément liés, il est impossible d’obtenir de bons tests avec du mauvais code.\nOn va catégoriser le code en 4 catégories, en fonction de 2 axes :\nL’axe de complexité ou d’importance vis-à-vis du domaine.\nLa complexité cyclomatique est définie par le nombre de branches possibles dans le code : 1 + le nombre de branches.\nLe calcul tient compte du nombre de prédicats dans les conditions : si notre if vérifie 2 prédicats, ça ajoute 2 points.\n\n\nL’importance vis-à-vis du domaine c’est la connexion du code avec le besoin de l’utilisateur final. Du code utilitaire ne rentrera pas là-dedans.\nC’est la complexité ou l’importance domaine. Un code signifiant du point de vue du domaine mais simple rentre dans la description.\n\n\nL’axe du nombre de collaborators impliqués.\nPour rappel un collaborator est une dépendance, qui est soit mutable (autre chose que des valeurs primitives et des value objects), soit out-of-process.\n\n\n\n\nLes 4 catégories de code sont :\nDomain models and algorithms : grande valeur sur l’axe de complexité, faible valeur sur l’axe des collaborators.\nC’est eux qu’il faut le plus tester, à la fois parce qu’ils sont faciles à tester et parce qu’on obtiendra une grande résistance aux régressions. C’est d’eux qu’on obtient le meilleur “retour sur investissement” de nos tests.\n\n\nTrivial code : faible valeur sur les deux axes.\nC’est du code simple, qui ne mérite pas de tests.\n\n\nControllers : faible valeur sur l’axe de complexité, grande valeur sur l’axe des collaborators.\nIl s’agit de code pas complexe mais qui coordonne le code complexe ou important.\nOn peut les tester avec des tests d’intégration qui seront beaucoup moins nombreux que les unit tests des domain models and algorithms.\n\n\nOvercomplicated code : grande valeur sur les deux axes.\nLà on est embêté : c’est à la fois du code qu’on ne peut pas se permettre de ne pas tester, et du code difficile à tester. Par exemple des fat controllers qui font tout eux-mêmes.\nOn va chercher à se débarrasser de ce code en le découpant, pour obtenir du code qui score beaucoup sur l’un des axes mais jamais les deux.\n\n\n\n\nLe Humble Object Pattern va nous permettre de découpler la logique de la partie difficile à tester (par difficile on entend code asynchrone/multi-thread, UI, dépendances out-of-process etc.).\nLe test va tester la partie logique complexe/métier directement.\nLe humble object est une fine couche avec très peu de logique, qui va lier la logique et la dépendance qui pose problème dans les tests.\nIl s’agit de dire qu’un code doit soit avoir une grande complexité (domain layer and algorithms), soit travailler avec beaucoup de dépendances (controllers), mais jamais les deux.\nExemples :\nLa functional et l’hexagonal architecture utilisent le humble object pattern.\nOn peut aussi mettre dans cette catégorie les patterns MVC et MVP qui séparent la logique (le modèle) de la UI (view), avec le humble object (le presenter ou le controller).\nL’aggregate du DDD est aussi un exemple : on groupe les classes dans des clusters (les aggregates) où elles auront une forte connectivité, et les clusters auront une faible connectivité entre eux. Ca permet de faciliter la testabilité en ayant besoin d’instancier essentiellement les collaborators du cluster concerné.\nNDLR : que l’aggregate permette d’améliorer la testabilité ou la maintenabilité OK, mais j’arrive pas à voir le rapport avec le humble object pattern ici. On n’a pas de hard-to-test dependency.\n\n\n\n\n\n\nExemple d’application avec du code overcomplicated, refactorée vers du humble object pattern :\nDescription :\nOn a un CRM qui gère les utilisateurs, et les stocke en DB.\nLa seule fonctionnalité dispo c’est le changement d’email : si le nom de domaine du nouvel email appartient à l’entreprise le user est un employé, sinon il devient un customer.\nEn fonction des emails des users, et donc de leur statut, le nombre d’employés est calculé et mis en base.\nQuand le changement d’email est fait, on doit envoyer un message dans un message bus.\n\n\nLa 1ère implémentation contient une classe User, avec une méthode changeEmail() qui calcule le nouveau statut du user, et sauve son email en base, mais aussi recalcule et sauve le nouveau nombre d’employés dans la table de l’entreprise. Elle envoie aussi le message dans le message bus.\nclass User {\nconstructor(\nprivate userId: number,\nprivate email: Email,\nprivate type: UserType\n) {}\n\nchangeEmail(userId: number, newEmail: Email) {\n// Get user data from database\n// If new email is same as before, return\n// Get company data from database\n// Check whether the email is corporate\n// Set the user type accordingly\n// If the type is different, update company number\n// of employees.\n// Save user info in database\n// Save company info in database\n// Send message to message bus\n}\n}\n\nNotre méthode changeEmail() fait des choses importantes du point de vue domaine, mais en même temps elle a deux collaborators out-of-process (la DB et le message bus), ce qui est un no-go pour du code compliqué ou avec importance domaine.\nOn est en présence du pattern Active Record : la classe domaine se query et se persiste en DB directement. C’est OK pour du code simple, mais pas pour du code qui va croître sur le long terme.\n\n\nPossibilité 1 : rendre explicites les dépendances implicites, en donnant l’objet de DB et message bus en paramètre (ce qui permettra de les mocker dans les tests).\nQue les dépendances soient directes ou via une interface, ça ne change rien au statut du code : il reste overcomplicated.\nOn va devoir mettre en place une mécanique de mocks complexe pour les tests. On peut trouver plus clean que ça.\n\n\nPossibilité 2, étape 1 : introduire un application service (humble object) qu’on appelle UserController pour prendre la responsabilité de la communication avec les dépendances out-of-process.\nLa nouvelle classe va chercher les informations du user et de l’entreprise en DB, crée un objet User avec ces infos. Puis elle appelle user.changeEmail(), et enfin sauve les données du user et de l’entreprise en DB, et envoie l’event d’email changé dans le message bus.\nclass UserController {\nconstructor(\nprivate database: Database,\nprivate messageBus: MessageBus\n) {}\n\nchangeEmail(userId: number, newEmail: Email) {\n// Get user data from database\n// Get company data from database\n\nconst user = new User(userId, email, type);\nconst numberOfEmployees = user.changeEmail(\nnewEmail,\ncompanyDomainName,\nnumberOfEmployees\n);\n\n// Save user info in database\n// Save company info in database\n// Send message to message bus\n}\n}\n\npublic class User {\n// ...\nchangeEmail(\nnewEmail: Email,\ncompanyDomainName: string,\nnumberOfEmployees: number\n) {\n// If new email is same as before, return\n\n// Check whether the email is corporate\n// Set the user type accordingly\nconst newType = ...\n\n// If the type is different, update company number\n// of employees.\nnumberOfEmployees = ...\n\nthis.email = newEmail;\nthis.type = newType;\n\nreturn numberOfEmployees;\n}\n}\n\nProblèmes :\nOn a une logique complexe dans le fait de reconstruire les données à partir de la base de données (le mapping), c’est le travail d’un ORM.\nL’event de changement d’email est envoyé systématiquement, même si l’email n’a pas été changé.\nOn a un petit code smell : la méthode user.changeEmail() prend le nombre d’employés en paramètre, et renvoie le nouveau nombre d’employés. Ça n'a rien à voir avec un user donné.\n\n\nMais au moins la classe User a perdu ses collaborators, elle est donc en l’état purement fonctionnelle. On va pouvoir la tester à fond facilement.\n\n\nÉtape 2 : enlever de la complexité de l’application service.\nPour faire le mapping entre les données de la DB et un objet en mémoire, on va soit utiliser un ORM, soit créer nous-mêmes un objet de type factory qui va renvoyer notre User.\nCette logique a l’air simple avec peu de branches apparentes, mais il faut prendre en compte les branches cachés liés aux dépendances : on fait des conversions de type, on va chercher des objets inconnus dans un tableau un à un etc. beaucoup de choses peuvent mal aller dans ce processus.\n\n\npublic class UserFactory {\ncreate(data: Record&lt;any, any>) {\nPrecondition.requires(data.length >= 3);\n\nconst id = data[0];\nconst email = data[1];\nconst type = data[2];\n\nreturn new User(id, email, type);\n}\n}\n\nOn a ici du code utilitaire complexe.\n\n\nA la fin de l’étape on a bien User qui est dans la case “domain models and algorithms”, et UserController qui est dans la case “Controllers”. Il n’y a plus d’overcomplicated code.\n\n\nÉtape 3 : on introduit une nouvelle classe domaine Company.\nNotre nouvelle classe domaine Company peut récupérer la logique de calcul du nombre d’employés qu’on sort de User.\nOn a donc UserController qui crée les deux objets de domaine à partir des données de la DB, et qui appelle user.changeEmail() en donnant l’instance company en paramètre.\nOn a un principe important d’encapsulation OO ici : tell, don’t ask. Le user va dire (tell) à l’instance de company de mettre à jour elle-même son nombre d’employés, plutôt que lui demander (ask) ses données brutes et faisant l’opération à sa place.\n\n\nuser.changeEmail() n’est plus une fonction pure puisqu’elle a un collaborator (company), mais vu qu’il n’y en a qu’un et qu’il n’est pas out-of-process, c’est raisonnable.\nOn va donc devoir faire du state-based testing, l’output-based étant possible qu’avec des fonctions pures.\n\n\nclass Company {\nconstructor(\nprivate domainName: string,\nprivate numberOfEmployees: number\n) {}\n\nchangeNumberOfEmployees(delta: number) {\nPrecondition.requires(this.nomberOfEmployees + delta >= 0);\nthis.numberOfEmployees += delta;\n}\n\nisEmailCorporate(email: Email) {\n// Get the domain part from the email\n// and return whether it is equal to this.domainName\n}\n}\n\nclass User {\n// ...\nchangeEmail(newEmail: Email, company: Company) {\nif (newEmail === this.email) return;\n\nconst newType = company.isEmailCorporate(newEmail)\n? Usertype.Employee\n: Usertype.Customer;\n\n// If the type is different, update company number\n// of employees.\ncompany.changeNumberOfEmployees(delta);\n\nthis.email = newEmail;\nthis.type = newType;\n}\n}\n\nclass UserController {\nconstructor(\nprivate database: Database,\nprivate messageBus: MessageBus\n) {}\n\nchangeEmail(userId: number, newEmail: Email) {\nconst userData = this.database.getUserById(userId);\nconst user = UserFactory.create(userData);\nconst companyData = this.database.getCompany();\nconst company = CompanyFactory.create(companyData);\n\nuser.changeEmail(newEmail, company);\n\nthis.database.saveCompany(company);\nthis.database.saveUser(user);\nthis.messageBus.sendEmailChangedMessage(userId, newEmail);\n}\n}\n\nA la fin, user et company sont sauvés en DB, et l’event est envoyé dans le message bus par UserController.\n\n\n\n\nComment tester notre exemple refactoré ?\nLe code des classes domaine (User et Company), et le code utilitaire complexe (factory si on n’a pas utilisé d’ORM) peuvent être unit testés à fond.\nExemple : \"changement d'email de corporate à non corporate\", \"changement d'email de non corporate à corporate\", \"changement d'email au même email\" etc.\nit(\"changes email from non corporate to corporate\", () => {\nconst company = new Company(\"mycorp.com\", 1);\nconst sut = new User(\"user@gmail.com\", UserType.Customer);\n\nsut.changeEmail(\"new@mycorp.com\", company);\n\nexpect(company.numberOfEmployees).toBe(2);\nexpect(sut.email).toBe(\"new@mycorp.com\");\nexpect(sut.userType).toEqual(UserType.Employee);\n});\n\n\n\nLes méthodes ultra simples comme le constructeur de User n’ont pas à être testées.\nLe controller doit être testé avec des tests d’intégration moins nombreux. Ce sera l’objet des prochains chapitres.\nLes pré-conditions sont des checks qui permettent de throw une exception tôt si une incohérence est détectée, pour éviter des problèmes plus importants.\nCes pré-conditions doivent être testées seulement si elles ont un lien avec le domaine, sinon c’est pas la peine.\nExemple de pré-condition qu’on teste : la méthode qui permet de mettre à jour le nombre d’employés sur Company throw si le nombre souhaité est inférieur à 0.\nExemple de pré-condition à ne pas tester : notre user factory vérifie que les données venant de la base ont bien 3 éléments avant de reconstruire le user. Cette vérification n’a pas de sens d’un point de vue domaine.\n\n\n\n\nNotre découpage domaine/controller marche bien parce qu’on récupère l’ensemble des données upfront, et les sauve à la fin en base inconditionnellement dans le controller. Mais que faire si on a besoin d’accès à des données seulement dans certains cas dictés par la logique ?\nIl y a des trade-offs à faire en fonction de :\nla testabilité du code du domaine\nla simplicité du code du controller\nla performance\n\n\nOn a 3 possibilités :\nGarder toute la logique dans le domaine, et toute l’interaction avec les deps out-of-process dans le controller.\nDans ce cas on va avoir une moins bonne performance, puisqu’on fera la lecture de la donnée dont on n’aura peut-être pas besoin à l’avance systématiquement. Le controller n’ayant pas la connaissance de si on a besoin ou non, il prend la donnée et la donne tout le temps.\nPar contre on a un code de domaine testable, et un controller simple.\n\n\nInjecter les dépendances out-of-process dans le domaine, et laisser le code business décider quand récupérer ou non les données.\nLe souci ici c’est la maintenabilité des tests du domaine, avec soit des tests lents à travers la DB, soit des mocks compliqués à maintenir.\nPar contre on a un controller simple, et de la performance.\n\n\nDécouper le processus de décision en plusieurs parties.\nLe controller va appeler la 1ère partie, récupérer les données, puis décider lui-même s’il faut faire la deuxième partie. Si oui il récupère les données additionnelles depuis la DB, et exécute la 2ème partie. Et à la fin comme d’habitude sauve le tout en DB.\nUne partie de la logique risque de fuiter du domaine vers le controller et rendre le controller plus compliqué.\nPar contre on a le code du domaine testable, et on garde la performance.\n\n\n\n\nLa plupart du temps, céder sur la performance n’est pas possible.\nIl nous reste donc les 2 dernières possibilités.\nL’auteur conseille de privilégier la séparation du processus de décision plutôt que l’injection des dépendances out-of-process dans le domaine. On peut gérer la fuite de la logique vers le controller et la complexification du code du domaine avec certaines techniques.\n\n\n\n\nUne de ces techniques est le pattern CanExecute / Execute.\nImaginons qu’on veuille mettre à jour l’email du user seulement si son compte n’est pas encore confirmé.\n1ère possibilité : on query les infos upfront, on donne tout à user, et le user décide de changer ou non l’email. Mais on a peut être récupéré les infos de la company pour rien, si le user était déjà confirmé => problème de performance.\n2ème possibilité : le controller vérifie lui-même si le compte du user est confirmé avant de faire éventuellement la query des infos de la company. Ici le controller a récupéré une partie de la logique chez lui.\n3ème possibilité (CanExecute / Execute) : le user expose une méthode canChangeEmail() qui encapsule la logique de prise de décision. Le controller n’a plus qu’à l’appeler pour décider si on passe à l’étape suivante ou non. La décision ne se fait plus vraiment au niveau du controller.\n// controller\nconst canChangeEmail = user.canChangeEmail();\nif (!canChangeEmail) {\nreturn;\n}\nuser.changeEmail(newEmail, company);\n\nPour s’assurer que le controller n’a d’autre choix que d’appeler cette méthode avant d’aller plus loin (et donc lui retirer de la responsabilité), on va mettre une pré-condition dans la méthode user.changeEmail(), où on appelle explicitement canChangeEmail() en vérifiant que la réponse est oui. Et cette pré-condition métier sera testée (contrairement à l’appel à canChangeEmail() dans le controller).\n// user\npublic canChangeEmail() {\nreturn this.isEmailConfirmed ? false: true;\n}\npublic changeEmail(newEmail: Email, company: Company) {\nPrecondition.requires(this.canChangeEmail());\n// [...]\n}\n\n\n\n\n\n\n\nVoici une autre de ces techniques concerne l’envoi de domain events :\nOn parle bien ici des domain events au sens DDD, ces events permettent d’informer les autres composants du système des étapes importantes qui ont lieu dans nos objets domaine.\nSi on revient à notre exemple de CRM, au moment du changement d’email du user, le controller envoie un message dans un message bus. Mais cet envoi est fait dans tous les cas, même si le changement n’a pas eu lieu. On veut l’envoyer seulement si le changement est fait.\nPour enlever la décision d’envoyer ou non l’event du controller, et la mettre dans le domaine, on va créer une liste d‘events qu’on met à l’intérieur de la classe domaine.\nOn a un event :\nclass EmailChanged {\npublic userId: number;\npublic newEmail: Email;\n}\n\nLe User crée l’event si l’envoi est confirmé :\npublic changeEmail(newEmail: Email, company: Company) {\n// [...]\nthis.emailChangedEvents.push(new EmailChanged(userId, newEmail);\n}\n\nEt le Controller va itérer sur les domain events de User pour envoyer les bons messages dans le message bus :\npublic changeEmail(userId: int, newEmail: Email) {\n// [...]\nuser.changeEmail(newEmail, company);\n// [...]\nuser.emailChangedEvents.forEach((event) =>\nthis.messageBus.sendEmailChangedMessage(\nevent.userId,\nevent.newEmail\n);\n);\n}\n\nOn va donc pouvoir unit tester la création de chaque domain event dans chaque cas dans le user, et on fera beaucoup moins d’integration tests pour vérifier que le controller lit bien les events du user et envoie ce qu’il faut.\n\n\nDans des projets plus gros, on pourrait vouloir fusionner les events avant de les dispatcher, cf. Merging domain events before dispatching.\n\n\nPour ce qui est de l’envoi de l’email, c’est un comportement observable de l’extérieur donc il doit être fait que si l’email est changé. Par contre, l'écriture en DB peut être faite inconditionnellement parce qu’elle est privée et que le résultat ne changera pas.\nOn a un petit souci de performance à écrire en DB si l’email n’a pas changé, mais c’est un cas plutôt rare.\nOn peut aussi le mitiger par le fait que la plupart des ORM n’iront pas écrire en DB si l’objet n’a pas changé. Donc on peut faire l’appel sans crainte.\n\n\nLe conseil général de Vladimir est de ne jamais introduire de dépendances out-of-process (même mockées dans les tests) dans le code du domaine. Il conseille plutôt de fragmenter les appels au domaine, et au pire mettre ce code dans le controller et le tester par des tests d’intégration.\nLes cas dans lesquels on va devoir mettre la logique dans le controller peuvent être par exemple :\nVérifier qu’un email est unique (il faut faire un appel out-of-process pour ça).\nGérer les cas d’erreur liés aux appels out-of-process.\n\n\n\n\nA propos de qui est le client de qui et de la notion de détail d‘implémentation :\nAu niveau du controller, le client c’est l’utilisateur final, donc il faut tester ou mocker ce qui lui est visible ou sert directement son but. Les appels qui sont faits vers le domaine sont un détail d'implémentation.\nAu niveau du domaine, le client c’est le contrôler, donc il faut unit tester ce qui sert directement son but. Les appels éventuels vers d’autres classes du domaine sont des détails d’implémentation qu’on n’a pas à mocker.","iii---integration-testing#III - Integration testing":"","8---why-integration-testing#8 - Why integration testing?":"Pour rappel, un test d’intégration est un test qui ne répond pas à au moins un des 3 critères des tests unitaires : vérifier une unité de comportement, le faire vite, le faire en isolation par rapport aux autres tests.\nEn pratique les tests d’intégration vont être ceux qui gèrent la relation avec les dépendances out-of-process.\nOn est donc dans la partie “controllers” en termes de type de code.\n\n\nLes règles de la pyramide des tests sont de :\nCouvrir le maximum de cas par des tests unitaires.\nTester un happy path, ainsi que les edge cases qui ne peuvent pas être couverts par les tests unitaires avec des tests d’intégration.\nQuand la logique est simple, on a moins de tests unitaires, mais les tests d’intégration gardent leur valeur.\n\n\nQuand un edge case amène à un crash immédiat, il n’y a pas besoin de le tester avec un test d’intégration.\nExemple du pattern CanExecute/Execute.\nOn appelle ce principe le Fail Fast principle.\nOn reste dans l’esprit coût/bénéfice pour la maintenance d’un test, dans ce cas le bénéfice n’est pas suffisant parce que ce genre de cas ne mène pas à de la corruption de données, et est rapide à remarquer et à fixer.\n\n\nIl y a 2 manières de tester les dépendances out-of-process : les tester directement ou les remplacer par des mocks.\nOn peut classer ces dépendances en deux catégories :\nLes managed dependencies sont celles que seuls nous utilisons, et que le monde externe ne connaît pas. Exemple typique : la base de données.\nCes dépendances sont considérées comme des détails d’implémentation.\nOn n’a donc pas à se préoccuper de nos interactions avec elles, ce qui compte c’est leur état final, et l’impact que ça aura sur le résultat observable. Donc pas besoin de mock.\n\n\nLes unmanaged dependencies sont celles qui sont observables de l’extérieur. Exemple typique : un serveur SMTP dont les mails seront visibles par les clients finaux, ou encore un message bus dont les messages vont affecter des composants externes à notre système.\nCes dépendances sont considérées comme faisant partie du comportement observable.\nPuisque les unmanaged dependencies sont observables, ils font partie de l’API publique, et donc il faut nous assurer que nos interactions avec elles restent les mêmes : les mocks sont parfaits pour ça.\n\n\nIl peut arriver qu’une dépendance soit à la fois managed et unmanaged : par exemple une base de données dont on choisit de partager certaines tables publiquement avec un composant externe.\nPartager une DB est en général une mauvaise idée parce que ça va nous coupler fortement, il vaut mieux passer par une API synchrone ou un message bus.\nCeci dit, si ça arrive, il faudra différencier les tables partagées des tables privées, et traiter chacune comme ce qu’elle est (managed/unmanaged) : des mocks pour assurer l’ensemble de nos interactions avec les tables partagées, et la vérification de l’état final seulement pour les tables privées.\n\n\nDans le cas où on n’aurait pas la possibilité de tester en intégration une DB privée (base legacy trop grosse, trop coûteuse, raisons de sécurité etc.), l’auteur conseille de ne pas écrire de tests d’intégration du tout pour celles-ci, et de se concentrer sur les unit tests.\nLa raison est que ça compromet la résistance aux refactorings en traitant une dépendance privée comme publique, et ça n’ajoute que très peu de protection contre des régressions en plus des unit tests. Le rapport coût/bénéfice n’est pas suffisant.\n\n\n\n\n\n\nSi on reprend l’exemple du CRM, pour écrire des tests d’intégration pour le UserController :\nOn va d’abord écrire un test pour couvrir le happy path le plus long. Ici ce serait le cas où on change l’email d’un user, qui passe de non corporate à corporate. On va mettre à jour en DB le user, les infos de company, et aussi envoyer le message dans le message bus pour l’email.\nIl n’y a qu’un edge case non couvert par des unit tests : le cas où l’email ne peut pas être changé. Mais dans ce cas on est sur du fail fast : une exception sera lancée et le programme s’arrêtera. Donc pas besoin de test d’intégration pour ça.\nA propos des tests end to end, on peut en faire quelques-uns pour notre projet, et leur faire traverser les scénarios les plus longs pour s’assurer que tout est bien branché. On vérifiera le résultat pour le client final au lieu de regarder dans la DB, et on vérifiera le message envoyé dans le message bus pour la dépendance externe à laquelle on n’a pas accès. Ici pour cette feature on choisit de ne pas en faire.\nConcernant notre test d’intégration de happy path donc, il faut d’abord décider de la manière dont on traite nos dépendances out-of-process : la DB est managed donc doit être testée au niveau de son état pour le user et la company, alors que le message bus est unmanaged donc doit être mocké pour tester les interactions avec lui.\nNotre test va contenir 3 sections :\nD’abord mettre le user et la company en DB et initialiser le mock pour le message bus. (Arrange)\nEnsuite appeler la méthode de notre controller. (Act)\nEt enfin tester le résultat en DB et l’interaction avec notre mock (Assert).\n\n\nit(\"changes email from non corporate to corporate\", () => {\n// Arrange\ndb.createCompany(\"mycorp.com\", 0);\ndb.createUser(1, \"user@gmail.com\", \"customer\");\nconst busMock = { send: jest.fn() };\nconst sut = new UserController(new MessageBus(busMock));\n\n// Act\nsut.changeEmail(1, \"user@mycorp.com\");\n\n// Assert\nconst user = db.getUserById(1);\nexpect(user.email).toBe(\"user@mycorp.com\");\nexpect(user.type).toBe(\"employee\");\n\nconst company = db.getCompany();\nexpect(company.numberOfEmployees).toBe(1);\n\nexpect(busMock.send).toHaveBeenCalledTimes(1);\nexpect(busMock.send).toHaveBeenCalledWith(expect.toInclude(\"1\"));\nexpect(busMock.send).toHaveBeenCalledWith(\nexpect.toInclude(\"user@mycorp.com\")\n);\n});\n\nIl ne faut pas utiliser les mêmes objets entre les sections, de manière à être sûr à chaque fois de lire et écrire depuis la DB.\n\n\n\n\nL’introduction d’interfaces prématurées est une mauvaise idée. Il faut en introduire une quand elle existe déjà mais est implicite, c'est-à-dire quand il y a au moins deux implémentations de celle-ci.\nLe principe fondamental ici c’est YAGNI (you ain’t gonna need it) qui dit que le code supposément utile pour plus tard ne le sera sans doute pas, ou pas sous cette forme.\nPour plus d’info sur le trade off YAGNI vs OCP, l’auteur a fait un article.\nPar conséquent, étant donné qu’un mock est une implémentation de plus, il nous faudra la plupart du temps faire une interface seulement pour les unmanaged dependencies.\n\n\nQuelques bonnes pratiques pour les tests d’intégration :\nCréer une séparation explicite entre domain model et controllers permet de savoir quoi tester unitairement, et quoi tester en intégration.\nLimiter le nombre de couches à seulement 3 : infrastructure layer, domain layer et application service layer.\nDavid J. Wheeler a dit à ce propos : “All problems in computer science can be solved by another layer of abstraction, except for the problem of too many layers of abstraction.”\nOn se retrouve souvent avec 4, 5, 6 layers, ce qui rend l’ajout d’une feature, et même la compréhension d’une feature complexe parce qu’on doit toucher à de nombreux fichiers.\nOn a souvent tendance à tester le layer du dessous depuis le layer du dessus. Et avec de nombreux layers on aboutit à de nombreux tests avec mocks qui apportent chacun peu de valeur.\n\n\nÉliminer les dépendances circulaires : quand deux classes dépendent l’une de l’autre pour fonctionner.\nLes dépendances circulaires créent aussi une difficulté à appréhender le code parce qu’on ne sait pas par où commencer.\nPar exemple, quand une classe en instancie une autre et lui passe une instance d’elle-même. On se retrouve à introduire des interfaces et utiliser des mocks pour les tests.\n\n\nNe pas mettre plusieurs Act dans le même test : parfois on est tenté de mettre en place plusieurs Arrange/Act/Assert à la suite dans le même test. C’est une mauvaise idée parce que le test devient difficile à lire et à modifier, et a tendance à grossir encore.\n\n\nA propos de la question des logs :\nSelon l’auteur, les logs doivent être testé uniquement s’ils sont destinés à être observés par des personnes autres que les développeurs eux-mêmes.\nPar exemple des personnes du business qui en ont besoin pour des insights.\nSteve Freeman et Nat Pryce distinguent deux types de logs dans Growing Object-Oriented Software, Guided by Tests : le support logging qui est destiné au personnel de support et sysadmins, et le diagnostic logging qui est destiné aux développeurs eux-mêmes pour du débug.\n\n\nIl faut bien distinguer le diagnostic logging et le support logging, en n’y appliquant pas la même technique de code.\nLe support logging étant plus important, on pourra utiliser une classe à part inspirée du structured logging : une manière de logger qui sépare les paramètres et le texte principal, de manière à pouvoir reformater ces logs comme on veut.\nExemple de code :\ndomainLogger.userTypeHasChanged(45, \"customer\", \"corporate\");\n\nclass DomainLogger {\npublic userTypeHasChanged(\nuserId: number,\noldType: UserType,\nnewType: UserType\n) {\nthis.logger.info(\n`User ${userId} changed type ``from ${oldType} to ${newType}`\n);\n}\n}\n\nPour le tester il va falloir le traiter comme une dépendance out-of-process unmanaged (puisqu’elle ne nous est pas privée). Et donc on peut faire comme avec le message envoyé dans le message bus :\nSi c’est le controller qui doit faire le log, il peut le faire directement et ce sera testé dans un test d’intégration sous forme de mock.\nSi c’est le code de domaine qui le fait, il faut séparer la logique d’envoi du log de l’envoi du log lui-même : créer un domain event pour l’envoi de ce log dans le domaine, et itérer sur les events de log dans le controller pour logger les logs dans la dépendance out-of-process. Le test pourra être fait sous forme unitaire pour vérifier la création du domain event.\n\n\n\n\nConcernant la quantité de logs :\nPour le support logging la question ne se pose pas : il en faut autant qu’il y a de requirement business.\nPour le diagnostic logging il faut faire attention à ne pas en abuser :\nTrop de logs noient l’information importante.\nMême si on log avec des niveaux différents, on pollue quand même le code avec des lignes de log un peu partout, ce qui rend plus difficile la lecture.\nL’auteur conseille de ne pas utiliser de logs dans le domaine, et dans ne le controller les utiliser que temporairement pour trouver un bug, puis les enlever.\nIdéalement il faudrait que les logs ne servent que pour les exceptions non gérées.\n\n\n\n\nConcernant la manière de passer le logger à nos objets, l’auteur conseille de le passer explicitement dans le constructeur ou dans l’appel à une méthode.","9---mocking-best-practices#9 - Mocking best practices":"Il faut mocker les unmanaged dependencies à l’edge (au bord) de notre système.\nLa raison est d’augmenter la protection contre les régressions en mettant en jeu le plus possible de code. On va donc mocker au plus près de l’appel à la dépendance externe.\nOn améliore aussi la résistance aux refactorings parce que ce qui est mocké est une API publique, et donc peu susceptible de changer contrairement à notre code interne.\n\n\nExemple : Si on a une classe MessageBus qui encapsule et ajoute des fonctionnalités à une classe Bus qui elle-même est un simple wrapper autour de la dépendance externe, il faut mocker Bus et non pas MessageBus.\nclass MessageBus {\nprivate _bus: Bus;\n// [...]\n}\n\nDans les tests on va peut être instancier un peu plus de choses pour que le mock soit en bout de chaîne, mais c’est pas grave :\n// Arrange\nconst busMock = new Mock&lt;IBus>();\nconst messageBus = new MessageBus(busMock);\nconst sut = new UserController(messageBus)\n// [...]\n// Assert\nexpect(busMock).toHaveBeenCalledWith(/* ... */);\n\nPour le mock de notre DomainLogger, on n’est pas obligés d’aller jusqu’à l’edge parce que contrairement à MessageBus où la structure exacte des message est cruciale pour maintenir la compatibilité avec la lib, la structure exacte des messages de log nous importe peu.\n\n\nQuand on veut mocker du code réutilisé dans de nombreux endroits (ce qui est en général le cas du code qui est à l’edge du système), il peut être plus lisible d’implémenter son propre objet de mock, qui est par définition un spy.\nExemple de spy :\nclass busSpy {\npublic send(message: string) {\nthis.sentMessages.push(message);\n}\n\npublic shouldSendNumberOfMessages(num: number) {\nexpect(this.sentMessages.length).toBe(num);\nreturn this;\n}\n\npublic withEmailChangedMessage(userId: number, newEmail: Email) {\nconst message = `Type: user email changed id: ${userId}``email: ${newEMail}`;\nexpect(this.sentMessages).toContain(message);\nreturn this;\n}\n}\n\nUtilisation dans le code :\nbusSpy\n.shouldSendNumberOfMessages(1)\n.withEmailChangedMessage(user.userId, \"new@gmail.com\");\n\n\n\nBonnes pratiques pour les mocks :\nVu que les mocks doivent êtres réservés aux dépendances out-of-process unmanaged, ils doivent être seulement dans les tests d’intégration.\nOn peut utiliser autant de mocks que nécessaire pour gérer toutes les dépendances out-of-process unmanaged qui sont utilisées dans notre controller.\nPour bien s’assurer de la stabilité de l’utilisation de l’API publique constituée par notre dépendance unmanaged, il faut aussi vérifier le nombre d’appels vers la dépendance.\nIl ne faut mocker que les classes qu’on possède. Ca veut dire qu’il faut wrapper toute dépendance unmanaged out-of-process par un adapter qui représente notre utilisation de cette dépendance. C’est ce wrapper qu’on va mocker.\nUn des avantages c’est que si la dépendance change de manière importante dans son interface, elle ne pourra pas impacter le reste de notre code sans qu’on change notre wrapper. Il s‘agit d’une protection.\nA l’inverse, selon l’auteur, créer des wrappers autour de dépendances qui ne sont pas unmanaged ne vaut pas le coup en terme de maintenance. Un exemple en est l’ORM.","10---testing-the-database#10 - Testing the database":"Il est préférable d’avoir tout ce qui concerne la structure de la base de données dans l’outil de versionning, tout comme le code.\nEn plus de la structure, certaines données sont en fait des reference data, et doivent être aussi versionnées avec le code.\nIl s’agit de données qu’il faut générer pour que l’application puisse fonctionner.\nOn peut différencier les reference data du reste en se demandant si l’application peut modifier ces données : si non alors ce sont des reference data.\nExemple : imaginons qu’on reprenne notre exemple CRM, et qu’on veuille mettre le type d’utilisateur en base. Si on veut garantir par la DB elle-même que le type ne sera pas autre chose que les types autorisés, on peut créer une table UserTypes, y mettre les types autorisés, et faire une foreign key depuis la table User vers cette table.\nLes données dans cette table sont là juste pour des raisons techniques, pour faire ce qui est fait ou pourrait l’être dans le code mais avec plus de sécurité. Elles ne sont pas accessibles aux utilisateurs de l’application. Ce sont des reference data.\n\n\n\n\nIl est préférable de permettre à tous les développeurs d’avoir leur base de données (idéalement sur leur machine locale).\nUne DB partagée peut devenir inutilisable, au moins momentanément, et ne permet pas de garantir l’exécution des tests vu que des modifications peuvent être faites par les autres développeurs.\n\n\nIl y a deux types d’approche pour le développement vis-à-vis de la base de données : la delivery state-based et migration-based.\nLa state-based consiste à avoir l’état actuel de la structure de la DB versionnée. On va alors créer une DB modèle à partir de cette structure, puis utiliser un outil de comparaison qui va la comparer avec la DB de production, pour ensuite appliquer les modifications sur la production.\nLa migration-based consiste à écrire des scripts de migration qui vont être versionnées. On ne connaît pas l’état actuel de la DB depuis ces scripts, mais les jouer tous dans l’ordre permet d’en obtenir un exemplaire.\nL’outil de comparaison de DB ne sera pas utilisé ici, sauf pour éventuellement permettre de détecter des anomalies dans la DB de prod.\n\n\nLa state-based est plus utile pour gérer les conflits de merge en ayant l’état explicite, alors que la migration-based est plus utile pour gérer la data motion (le fait de changer la structure de la DB avec des données dedans).\nLa raison est que gérer la transformation de données existantes est difficile à faire automatiquement, il faut y appliquer des règles métier.\nDans la plupart des cas, gérer la data motion est plus important que la gestion de conflits de merge. Donc il vaut mieux préférer l’approche migration-based.\n\n\n\n\nIl ne faut jamais faire de changements directement dans la DB sans passer par l’app, autrement que par des scripts de migration versionnés.\nSi une migration est incorrecte, il vaut mieux faire une migration pour la corriger (sauf si elle n’a pas encore été jouée et que la jouer amènera à de la perte de données).\n\n\nA propos de la gestion des transactions dans nos DB :\nLes transactions sont importantes à la fois dans le code pour garantir la consistance des données, et aussi dans les tests pour s’assurer qu’ils sont fiables.\nDans le code, on a deux notions liées à la DB :\nLes transactions qui décident si les modifications faites doivent être gardées ou non. Elles durent le temps de l’opération entière.\nLes repositories qui prennent une transaction, et agissent sur les données (en lecture ou écriture) dans le cadre de cette transaction.\nExemple dans notre controller du CRM :\npublic UserController {\n\npublic UserController(\nprivate transaction: Transaction\n) {\nthis.userRepository =\nnew UserRepository(transaction);\n}\n\n// [...]\nconst user = this.userRepository.getById(userId);\n// [...]\nthis.userRepository.save(user);\nthis.transaction.commit();\n// [...]\n}\n\n\n\nIl existe aussi un pattern appelé unit of work, qui consiste à retenir les modifications sur les objets qui doivent avoir lieu au cours d’une transaction, et à les soumettre en une seule fois à la DB au moment où la transaction est validée.\nCa permet notamment d’économiser le nombre de connexions à la DB. La plupart des ORM l’implémentent.\n\n\nDans le cas où on travaille avec les document databases comme MongoDB, les transactions sont souvent garanties au sein d’un même document seulement.\nDans ce cas, il faut se débrouiller pour que nos opérations n’affectent qu’un document. Si on utilise le domain model pattern du DDD, on pourra affecter un aggregate par document et suivre la guideline de ne mettre à jour qu’un document à la fois.\n\n\nConcernant les tests d’intégration, il faut que chacun des 3 blocs (Arrange, Act, Assert) ait sa transaction à lui.\nLa raison est de chercher à reproduire au mieux l’environnement de production. Dans le cas contraire on peut par exemple se retrouver à avoir certaines de nos libs (ORM notamment) qui vont mettre en cache certaines données au lieu d’aller lire/écrire en DB explicitement.\nÇa compromet donc l’idée de wrapper chaque test dans une transaction qu’on annule à la fin du test (l’idée est évoquée et balayée pour cette raison).\n\n\n\n\nSelon l’auteur, la parallélisation des tests d’intégration n’en vaut pas le coup, parce que ça nécessite trop d’efforts. Il vaut mieux les jouer séquentiellement, et cleaner les données entre les tests.\nIl suggère de cleaner au début de chaque test. Le faire à la fin peut poser problème à cause de potentiel crash avant la fin.\nConcernant la manière d’effacer les données, il suggère une simple commande SQL de type DELETE FROM dbo.User;\n\n\nIl vaut mieux éviter d’utiliser une DB “in memory” à la place de la vraie DB dans les tests d’intégration. Ça permet de transformer les tests d’intégration en unit tests, mais ça leur enlève aussi de la fiabilité vis-à-vis de l’intégration à cause des différences entre les deux bases de données.\nSelon l’auteur on va finir de toute façon par faire des tests d’intégration à la main si on va sur une BD différente.\n\n\nOn peut utiliser certaines techniques de refactoring pour rendre le code des tests d’intégration plus lisible :\nPour la section Arrange : on peut par exemple utiliser des méthodes de type factory pour que la création des objets en base avec transaction prenne moins de place.\nLe pattern Object mother consiste à avoir une méthode qui crée l’objet, et le renvoie.\nIl conseille de commencer par mettre ces méthodes dans la classe de test, et de ne les déplacer que si besoin de réutilisation.\nOn peut mettre des valeurs par défaut aux arguments, pour n’avoir à spécifier que ceux qui sont nécessaires.\n\n\nPour la section Act : on peut aussi utiliser une fonction helper pour réduire ça à un appel qui créera la transaction et la passera à la méthode testée, comme en production.\nEx :\nconst result = execute(() => user.changeEmail(userId, \"new@gmail.com\"));\n\n\n\nPour la section Assert : là aussi on peut utiliser des fonctions helper :\nOn peut mettre des fonctions qui abstraient le fait d’aller chercher des données en base.\nconst user = queryUser(user.id);\n\nOn peut créer une classe exposant une fluent interface par dessus des instructions assert.\nclass UserExtensions {\npublic shouldExist(user: User) {\nexpect(user).toBeTruthy();\nreturn user;\n}\n\npublic withEmail(user: User, email: Email) {\nexpect(email).toEqual(user.email);\nreturn user;\n}\n}\n\n// In test\nuser.shouldExist().withEmail(\"new@gmail.com\");\n\nTODO : ce code ne fonctionne pas en l’état, il faudrait trouver le moyen de faire de l’extension de méthode en Typescript.\n\n\nAvec les helpers qui créent des objets dans la section Arrange, ou qui lisent des objets dans la section Assert, on crée plus que 3 transactions en tout. Pour autant, ça reste un bon trade off selon l’auteur : on sacrifie un peu de performance du test, contre une amélioration substantielle de maintenabilité du test.\n\n\nFaut-il tester les opérations de lecture ? (comme renvoyer une information au client)\nLe plus important est de tester les opérations d’écriture qui peuvent corrompre les données. Pour celles de lecture il n’y a pas ce genre d’enjeu, donc la barre pour ajouter des tests est plus haute : il ne faut tester que les opérations les plus complexes.\nEn fait, l’intérêt principal du domain model c’est de protéger la consistance des données à travers l’encapsulation. Dans les opérations de lecture il n’y en a pas besoin.\nL’auteur conseille donc de ne tester les opérations qu’avec des tests d’intégration, et seulement pour celles qu’on veut tester.\nIl conseille aussi d’écrire les requêtes pour la lecture directement en SQL, l’ORM n’étant pas utile dans ce cas, et ajoutant des couches d’abstractions inutiles et peu performantes.\n\n\n\n\nFaut-il tester les repositories ?\nNon. Malgré l’intérêt apparent, le rapport bénéfice/coût est défavorable :\nD’un côté les repositories manipulent la DB qui est une dépendance out-of-process, donc si on les testait, ce serait avec des tests d’intégration (et ceux-ci coûtent cher).\nDe l’autre, ils ne fournissent pas tant de protection contre les régressions que ça, et surtout ils sont pour l’essentiel déjà testés par les tests d’intégration des controllers.\nSi on arrive à isoler les factories à part, ça pourrait valoir le coup de les tester à part unitairement, mais quand on utilise un ORM, on ne peut en général pas tester le mapping à part de la DB.\n\n\nIl en est de même pour les event dispatchers par exemple, dont le rapport bénéfice/coût des tests sera défavorable.","iv---unit-testing-anti-patterns#IV - Unit testing anti-patterns":"","11---unit-testing-anti-patterns#11 - Unit testing anti-patterns":"Il ne faut pas rendre publique une méthode privée, juste pour la tester.\nLa 1ère règle est de tester la fonctionnalité privée par l’effet qu’elle a sur l’API publique.\nSi la fonctionnalité privée est trop compliquée pour être testée à travers ce qui est public, c’est le signe d’une abstraction manquante. Il faut alors la matérialiser.\nExemple de code dont on a envie de tester la méthode privée getPrice() sans passer par la méthode publique :\nclass Order {\npublic generateDescription() {\nreturn `Name: ${this.name}, ``total price: ${this.getPrice()}`;\n}\n\nprivate getPrice() {\n// de la logique compliquée ici\n}\n}\n\nOn matérialise l’abstraction manquante et on la teste avec de l’output-based testing :\nclass Order {\npublic generateDescription() {\nconst calculator = new PriceCalculator();\nreturn `Name: ${this.name}, ``total price: ``${calculator.calculate(\nthis.products\n)}`;\n}\n}\n\nclass PriceCalculator {\npublic calculate(products: Products[]) {\n// de la logique compliquée ici\n}\n}\n\n\n\n\n\nIl en est de même avec un attribut privé : le rendre public juste pour le tester est un antipattern.\nIl ne faut pas faire fuiter du domain knowledge du code vers les tests : réutiliser le même algorithme dans le test ne permettra pas de remarquer qu’on s’est trompé.\n_ Un exemple simple peut être un code qui fait une addition :\nreturn a + b, et un test qui teste avec l’addition aussi : expect(result).toBe(3 + 2);\nIl vaut mieux vérifier des valeurs pré-calculées sans réimplémenter l’algo : expect(result).toBe(5);\n_ Si on copie l’algo dans le test, alors on aura tendance à mettre à jour en même temps le code et le test en cas de changement, sans pouvoir se rendre compte que l’algo est faux. * Idéalement il faut pré-calculer le résultat à expect dans le test avec l’aide d’un expert métier (quand on n’est pas expert nous-mêmes comme pour l’addition), et en tout cas il ne faut pas obtenir le calcul à partir du code qui est censé être testé.\nLa code pollution consiste à introduire des choses dans le code, qui ne sont utiles que pour le test. C’est un antipattern.\nPar exemple avoir un if(testEnvironment) ... else ... introduit de la pollution qui posera des problèmes de maintenance plus tard.\nOn peut en général régler le problème avec des interfaces : par exemple s’il s’agit d’éviter certaines opérations de log dans les tests en ne loggant pas si on est en env de test, on peut injecter le logger dans le code avec une interface. Dans le test on donnera une version fake du logger qui ne log pas.\nL’interface est une petite pollution aussi, mais elle crée beaucoup moins de danger que des bouts de code dans des if.\n\n\n\n\nOn est parfois tenté de vouloir stubber/mocker une seule méthode d’une classe qui fait quelque chose de complexe, pour tester ce qui est complexe et éviter qu’elle ne communique avec une dépendance out-of-process. Ceci est un antipattern.\nLa bonne façon de faire est de séparer la logique complexe de la partie qui communique la chose à la dépendance out-of-process (typiquement avec un humble object pattern qui fait le lien entre les deux), et unit tester la logique.\n\n\nConcernant la notion de temps utilisée dans le code (new Date()), l’introduire en tant qu’élément statique est, comme dans le cas du logger, un antipattern qui introduit une dépendance partagée dans les tests, et pollue le code.\nLa bonne manière est d’introduire la dépendance temporelle explicitement dans le constructeur ou la méthode appelée.\nOn peut le faire soit sous forme de service appelable pour obtenir la date, soit en passant la valeur pré-générée. Passer la valeur directement est ce qui présente le moins d’inconvénients, à la fois pour la clarté du code, et pour la testabilité."}},"/":{"title":"Reading notes","data":{"":"These are my detailed notes taken while reading computer science related books. Most of the notes are in french.","complete-notes#Complete notes":"","programming#Programming":"[fr] Refactoring - Martin Fowler - 2018\n[fr] The Design of Web APIs - Arnaud Lauret - 2019\n[fr] Unit Testing: Principles, Practices and Patterns - Vladimir Khorikov - 2020","architecture#Architecture":"[fr] Get Your Hands Dirty on Clean Architecture - Tom Hombergs - 2019\n[fr] Learning Domain-Driven Design - Vlad Khononov - 2021","infrastructure#Infrastructure":"[fr] Designing Data-Intensive Applications - Martin Kleppmann - 2016","product#Product":"[fr] Continuous Discovery Habits - Teresa Torres - 2021","organization#Organization":"[fr] Reinventing Organizations - Frédéric Laloux - 2015","in-progress#In progress":"[fr] Monolith to Microservices - Sam Newman - 2019"}},"/books/get-your-hands-dirty-on-clean-architecture":{"title":"Get Your Hands Dirty on Clean Architecture","data":{"":"","1---whats-wrong-with-layers#1 - What’s Wrong With Layers?":"La layered architecture est tout à fait classique : 3 couches successives (web -> domain -> persistance).\nElle peut même permettre une bonne architecture qui laisse les options ouvertes (par exemple remplacer la persistance en ne touchant que cette couche-là).\n\n\nLe problème de la layered architecture c’est qu’elle se détériore rapidement et encourage les mauvaises habitudes.\nElle promeut le database-driven design : vu que la persistance est à la base, on part toujours depuis la modélisation de la structure de la DB.\nAu lieu de ça on devrait mettre au centre le comportement, c’est-à-dire le code métier, et considérer la persistance comme périphérique.\nUn des éléments qui pousse au database-driven design aussi c’est l’ORM, qui peut être utilisé depuis le layer domain, et introduit des considérations techniques dedans.\n\n\nElle encourage les raccourcis : quand un layer du dessus a besoin d’un élément du dessous, il suffit de le pousser vers le bas et il y aura accès.\nLa couche de persistance finit par grossir et devenir une énorme couche “utilitaire” qui contient la logique et les aspects techniques entremêlés.\n\n\nElle devient de plus en plus difficile à tester :\nLe logique métier a tendance à fuiter vers la couche web, parce que la persistance y est directement utilisée.\nTester la couche web devient de plus en plus difficile parce qu’il faut mocker les deux autres, et parce que la logique y grossit.\n\n\nElle cache les use cases : la logique fuitant vers les autres layers, on ne sait pas où ajouter un nouveau use-case, ni où chercher un existant.\nOn peut ajouter à ça que vu qu’il n’y a pas de limite, la couche domain a des unités (services) qui grossissent au fil du temps, ce qui rend plus difficile de trouver où va chaque fonctionnalité.\n\n\nElle rend le travail en parallèle difficile :\nComme on fait du database-driven design, on doit toujours commencer par la couche de persistance, et on ne peut pas être plusieurs à la toucher.\nSi en plus les services du domaine sont gros, on peut être en difficulté pour être plusieurs à modifier un gros service pour plusieurs raisons.","2---inverting-dependencies#2 - Inverting dependencies":"Le Single Responsibility Principle (SRP) dit qu’un composant doit avoir une seule raison de changer.\nÇa veut dire que si on change le logiciel pour n’importe quelle autre raison, il ne devrait pas changer.\nQuand nos composants dépendent les uns des autres, ça leur donne autant de raisons à chaque fois de changer, si un des composants dont ils dépendent change lui aussi.\n\n\nLa layered architecture fait que la couche web et la couche domain ont des raisons de changer liées à la couche de persistance. On n’a pas envie que la couche domain change pour d’autres raisons qu’elle-même, donc on va inverser les dépendances qu’elle a.\nC’est le Dependency Inversion Principle (DIP).\nOn va copier les entities depuis la persistance vers la couche domain qui en a besoin aussi.\nEt on va créer une interface de persistance dans le domaine, à laquelle va adhérer la couche persistance qui aura donc une dépendance vers le domaine plutôt que l’inverse.\n\n\nSelon Robert Martin la clean architecture doit garder le domaine séparé du reste (frameworks, infrastructure etc.), et les dépendances doivent être tournées vers le code du domaine pour que celui-ci n’en ait pas d’autres que lui-même.\nLes entities du domaine sont au centre.\nIls sont utilisés par les use cases, qui représentent les services, mais impliquent d’avoir une granularité fine.\nCette séparation a un coût, qui est qu’il faut dupliquer les entités entre le domaine et l’infrastructure, notamment pour éviter que les entités du domaine soient polluées par la technique.\n\n\nL’hexagonal architecture est similaire à la clean architecture mais un peu moins “abstraite”, c’est la version d’Alistair Cockburn.\nL’hexagone contient les entities et les use cases, et en dehors on trouve des adapters pour intégrer la communication avec l’extérieur.\nOn a deux types d’adapters :\nLes adapteurs de gauche drivent l’hexagone, parce qu’ils appellent des fonctions exposées par l’hexagone.\nExemple : handlers HTTP.\n\n\nLes adapters de droite sont drivées par l’hexagone, parce que l’hexagone appelle des méthodes sur eux.\nExemple : communication avec la DB.\n\n\n\n\nPour permettre la communication, l’hexagone définit des ports (interfaces), qui doivent être implémentés par les adapters.\nC’est pour ça qu’on parle de Ports & Adapters.\n\n\n\n\nQuel que soit leur nom, tout l’intérêt de ces architectures c’est de permettre d’avoir un domaine isolé, dont on pourra gérer la complexité sans qu’il ait d’autres raisons de changer que lui-même.","3---organizing-code#3 - Organizing Code":"On peut organiser le code par couches : le classique web, domain et persistance, mais avec une inversion de la persistance vers le domain.\n- web\n- AccountController\n- domain\n- Account\n- AccountService\n- AccountRepositoryPort\n- persistance\n- AccountRepositoryImpl\n\nMais cette organisation est sous-optimale pour 3 raisons :\nIl n’y a pas de séparation sous forme de dossiers ou de packages pour les fonctionnalités. Donc elles vont vite s’entre-mêler au sein de chaque couche.\nComme les services sont gros, on peut difficilement repérer la fonctionnalité exacte qu’on cherche tout de suite.\nOn ne voit pas au premier coup d'œil quelle partie de la persistance implémente quel port côté domain. L’architecture ne saute pas aux yeux.\n\n\n\n\nOn peut ensuite organiser le code par features : les limites de dossier/package sont définies par les features qui contiennent un fichier par couche.\n- account\n- Account\n- AccountController\n- AccountRepository\n- AccountRepositoryImpl\n- SendMoneyService\n\nOn a nos features visibles immédiatement (Account -> SendMoneyService), ce qui fait qu’on est dans le cadre d’une screaming architecture.\nPar contre, nos couches techniques sont très peu protégées, et le code du domaine n’est plus protégé du reste par des séparations fortes.\n\n\nOn peut enfin organiser le code dans une architecture expressive, reprenant le meilleur des deux autres :\nUne séparation initiale par features majeures.\nPuis une séparation par couches à l’intérieur de ces features majeures.\nEt enfin la séparation explicite des ports et adapters, en explicitant leur nature entrante ou sortante.\n- account\n- adapter\n- in\n- web\n- AccountController\n- out\n- persistance\n- AccountPersistanceAdapter\n- domain\n- Account\n- application\n- SendMoneyService\n- port\n- in\n- SendMoneyUseCase\n- out\n- LoadAccountPort\n- UpdateAccountStatePort\n\nLe fait que l’architecture soit alignée avec la structure en packages fait que nous avons moins de chances d’en dévier. Elle est incarnée de manière très concrète dans le code.\nLe domaine étant isolé, on peut très bien en faire ce qu’on veut, y compris y appliquer les patterns tactiques du DDD.\nCôté visibilité des packages :\nLes adapters peuvent rester privés, puisqu’ils ne sont appelés qu’à travers les ports.\nLes ports doivent être publics pour être accessibles par les adapters.\nLes objets du domaine doivent être publics pour être accessibles depuis les services et les adapters.\nLes services peuvent rester privés parce qu’ils sont appelés à travers les ports primaires.\n\n\n\n\nConcernant la manière dont fonctionne l’inversion de dépendance ici :\nPour les adpaters entrants il n’y a pas besoin d’inversion puisqu’ils sont déjà entrants vers l’hexagone. On peut, au besoin, quand même protéger l’hexagone derrière des ports quand même.\nPour les adapters sortants par contre il faut inverser la dépendance, en les faisant respecter le port de l’hexagone, puis en les instanciant et les donnant à l’hexagone.\nIl faut donc un composant tiers neutre qui instancie les adapters sortants pour les donner à l’hexagone, et instancie l’hexagone pour le donner aux adapters entrants.\nIl s’agit de l’injection de dépendance.","4---implementing-a-use-case#4 - Implementing a Use Case":"Comme on a une forte séparation hexagone/adapters, on peut implémenter l’hexagone de la manière dont on veut, y compris avec les patterns tactiques du DDD, mais pas forcément.\nDans ce chapitre on implémente un use-case dans l’hexagone de l’exemple buckpal qui est une application de gestion de paiement.\nLa couche domain se trouve dans buckpal -> domain, et contient une entité Account, qui a des méthodes pour ajouter et retirer de l’argent.\nChaque ajout ou retrait se fait en empilant des entités Activity qui contiennent chaque transaction dans un tableau interne à Account.\nLe tableau interne ne contient qu’une fenêtre d’Activity pour des raisons de performance, et une variable permet de connaître la valeur du compte avant ce nombre restreint d’Activity.\n\n\nLes use-cases se trouvent dans la couche applicative, dans buckpal -> application -> service.\nUn use-case va :\nRécupérer l’input (qu’il ne valide pas par lui-même pour laisser cette responsabilité à un autre composant).\nValider les règles business, dont la responsabilité est partagée avec la couche domain.\nManipuler l’état du domaine :\nÇa se fait en instanciant des entités et appelant des méthodes sur elles.\nEt en général en passant les entités à un adapter pour que leur état soit persisté.\nAppeler éventuellement d’autres use-cases.\n\n\nRetourner l’output.\n\n\nLes use-cases vont être petits pour éviter le problème de gros services où on ne sait pas quelle fonctionnalité va où.\n\n\nLa validation des inputs se fait dans la couche applicative pour permettre d’appeler l’hexagone depuis plusieurs controllers sans qu’ils aient à valider ces données, et pour garantir l’intégrité des données dans l’hexagone.\nOn va faire la validation dans une classe de type command. Cette classe valide les données dans son constructeur, et refuse d’être instanciée si les données sont invalides.\nL’auteur déconseille d’utiliser le pattern builder et conseille d’appeler directement le constructeur.\nExemple de builder :\nnew CommandBuilder().setParameterA(value1).setParameterB(value2).build();\n\n\n\nCette classe va se trouver dans buckpal -> application -> port -> in.\nElle constitue une sorte d’anti-corruption layer protégeant l’hexagone.\n\n\nOn pourrait être tenté de réutiliser des classes de validation d’input entre plusieurs use-cases ressemblants, par exemple la création d’un compte et la modification d’un compte. L’auteur le déconseille.\nSi on réutilise, on va se retrouver avec quelques différences (par exemple l’ID du compte) qui vont introduire de potentielles mauvaises données.\nOn va se retrouver à gérer les différences entre les deux modèles dans les use-cases alors qu’on voulait le faire dans un objet à part.\nGlobalement, faire des modèles de validation d’input permet, au même titre que le fait de faire des petits use-cases, de garder l’architecture maintenable dans le temps.\n\n\nLa validation des business rules doit quant à elle être faite dans les use-cases.\nLa différence avec la validation des inputs c’est que pour les inputs il n’y a pas besoin d’accéder à l’état du modèle de données, alors que pour les business rules oui.\nLe use-case peut le faire directement en appelant des fonctions, ou alors le déléguer à des entities.\nDans le cas où l'essentiel de la logique est fait dans les entities et où le use-case orchestre juste des appels et passe les données, on parle d’un rich domain model. Dans le cas où c’est le use-case qui a l’essentiel de la logique et les entities sont maigres, on parlera d’un anemic domain model.\n\n\nLe use-case lève une exception en cas de non-respect des règles business comme pour les règles d’input. Ce sera à l’adapter entrant de décider de ce qu’il fait de ces exceptions.\n\n\nConcernant l’output renvoyé par le use-case, il faut qu’il soit le plus minimal possible : n’inclure que ce dont l’appelant a besoin.\nSi on retourne beaucoup de choses, on risque de voir des use-cases couplés entre eux via l’output (quand on ajoute un champ à l’objet retourné, on a besoin de changer tous ceux qui le retournent).\n\n\nLes use-cases qui font uniquement de la lecture pour renvoyer de la donnée peuvent être distinguées des use-cases qui écrivent.\nPour ça on peut les faire implémenter un autre port entrant que les use-cases d’écriture, par exemple le port GetAccountBalanceQuery.\nOn pourra à partir de là faire du CQS ou du CQRS.","5---implementing-a-web-adapter#5 - Implementing a Web Adapter":"Tous les appels extérieurs passent par des adapters entrants.\nComme le flow d’appel est déjà dirigé de l’adapter vers l’hexagone, on pourrait enlever le port, et laisser l’adapter appeler directement les use-cases.\nLa matérialisation des ports permet d’avoir un endroit où tous les points d’entrée de l’hexagone sont clairement identifiés et spécifiés. C’est utile pour la maintenance à long terme.\nEnlever ces ports fait partie des raccourcis discutés dans le chapitre 11.\nDans le cas où notre adapter est un WebSocket qui va appeler mais aussi qui sera appelé, alors il faut obligatoirement avoir les ports entrants, et surtout sortants, puisque là on a bien une inversion du sens d’appel.\n\n\nLes adapters web vont :\nCréer des objets internes à partir des objets HTTP.\nOn parle aussi de désérialisation.\n\n\nVérifier les autorisations.\nValider l’input, et le faire correspondre à l’input du use-case qui va être appelé.\nVu qu’on valide déjà l’input à l’entrée du use-case, on va ici seulement valider le fait que l’input reçu peut bien être converti dans l’input du use-case.\n\n\nAppeler le use-case.\nRécupérer l’output, et reconstruire un objet HTTP à partir de ça pour le renvoyer.\nOn parle aussi de sérialisation.\n\n\n\n\nL’adapter se trouve dans buckpal -> adapter -> in -> web.\nL’adapter web a la responsabilité de communiquer avec le protocole HTTP. C’est une responsabilité qu’il doit avoir seul, et donc ne pas faire fuiter des détails du HTTP dans le use-case.\nConcernant la taille des controllers, il vaut mieux les avoir les plus petits et précis possibles.\nIl vaut mieux éviter de créer par exemple une classe AccountController qui va avoir plusieurs méthodes associées chacune à un endpoint.\nCe genre de classe peut grossir et devenir difficile à maintenir. De même pour les tests associés.\nLe fait d’être dans une classe fait que ces controllers vont partager des fonctions et objets entre eux. Et donc vont être plus facilement couplés.\n\n\nOn peut nommer les classes de web adapter avec des noms plus précis que UpdateX, CreateX etc. Par exemple RegisterAccount.\nLes petits controllers permettent aussi de travailler plus facilement sur le code en parallèle.","6---implementing-a-persistence-adapter#6 - Implementing a Persistence Adapter":"Avec l’adapter de persistance, on a une vraie inversion des dépendances qui fait que malgré le sens des appels de l’hexagone vers l’adapter, grâce au port c’est l’adapter qui dépend de l’hexagone.\nOn va donc pouvoir faire des modifications dans l’adapter de persistance sans que ça n’affecte la logique business.\n\n\nLes adapters de persistance vont :\nPrendre l’input.\nCa peut être une domain entity ou un objet spécifique pour une opération en base.\n\n\nLe mapper au format base de données et l’envoyer à DB.\nEn général on va mapper vers les entities de l’ORM, mais ça peut aussi être vers des requêtes SQL directement.\n\n\nRécupérer la réponse DB et la mapper au format applicatif.\nEn général une domain entity.\n\n\nRenvoyer la valeur à l’application service.\n\n\nL’adapter se trouve dans buckpal -> adapter -> out -> persistance.\nIl faut que les modèles d’input et d’output vers et depuis le persistance adapter soient tous deux dans l’application core (hexagone).\nPlutôt qu’avoir un gros port qui permet d’accéder à toutes les méthodes de l’adapter correspondant à une entity, il vaut mieux avoir des ports plus fins.\nUne des raisons c’est qu’il est plus difficile de mocker le persistance adapter en entier que de mocker certaines de ses méthodes correspondant aux ports utilisés.\nLa plupart du temps on va se retrouver avec une méthode de l’adapter par port.\n\n\nPour ce qui est de l’adapter lui-même (celui qu’on accède via les ports), on peut l’avoir gros, mais on peut aussi le découper, par exemple pour avoir un adapter par entity, ou un par aggregate (si on utilise les patterns tactiques du DDD).\nOn pourrait aussi créer deux adapters pour une même entity : un pour les méthodes utilisant notre ORM, et un autre pour les méthodes utilisant du SQL directement.\nDans l’adapter on pourra donc utiliser les entities de l’ORM, ou alors des repositories faites à la main, qui contiendront des méthodes exécutant du SQL.\n\n\nSéparer les aggregates dans des adapters distincts permet aussi de faciliter l’extraction d’un bounded context par la suite.\n\n\nOn va donc avoir une duplication des domain entities dans le persistance adapter, en général sous forme d’entities d’ORM.\nUtiliser les entities ORM dans le domaine peut être un choix possible pour éviter des mappings (discuté dans le chapitre 8), mais ça a le désavantage de faire fuiter des contraintes spécifiques à la DB vers le domaine.\n\n\nL’adapter en lui-même :\nPour une lecture, il appelle des méthodes sur les entities ORM ou sur les repositories maison, puis il map le résultat à une entity domaine et la renvoie.\nPour une écriture, il récupère l’entity domaine, la map vers l’entity ORM ou vers la méthode de repository maison, et exécute la méthode sur celle-ci.\n\n\nConcernant les transactions, elles ne peuvent pas être dans l’adapter de persistance. Elles doivent être dans la fonction qui orchestre les appels à la persistance, c'est-à-dire les use-cases applicatifs.\nSi on veut garder nos use-cases purs, on peut recourir à l’aspect-oriented programming, par exemple avec AspectJ.","7---testing-architecture-elements#7 - Testing Architecture Elements":"Il y a 3 types de tests dans la pyramide :\nPlus on monte dans la pyramide, et plus les tests sont lents et fragiles, donc plus on monte et moins nombreux doivent être les tests.\nLes unit tests testent en général une seule classe.\nLes integration tests mettent en jeu le code de plusieurs couches, et vont souvent mocker certaines d’entre-elles.\nLes system tests testent le tout de bout en bout, sans mocks (autre que les composants qu’on ne peut pas instancier dans notre test).\nOn pourra parler de end-to-end tests si on teste depuis le frontend et non depuis l’API du backend.\n\n\n\n\nNDLR : l’auteur adopte une vision proche de l’école de Londres, en considérant qu’un test teste une unité de code (et pas une unité de comportement peu importe la quantité de code), et qu’en testant plusieurs bouts de code ensemble il faut mocker les bouts de codes voisins (c’est comme ça qu’on obtient des tests fragiles aux refactorings).\n\n\nCôté implémentation :\nLes domain entities sont testés avec des unit tests (state-based).\nNDLR : Dans l’exemple on teste Account, mais pour autant on ne mock pas Activity, on en construit une vraie instance. Donc sur cet exemple en tout cas, on n’est pas tout à fait dans l’école de Londre non plus.\n\n\nLes use-cases sont testés avec des unit tests (communication based) vérifiant que le use-case appelle la bonne méthode sur le domain entity et sur l’adapter de persistance.\nNDLR : Là on est bien dans la London school.\nL’auteur fait remarquer que le test utilisant des mocks, il est couplé à la structure du code et pas seulement au comportement. Et donc il conseille de ne pas forcément tout tester, pour éviter que ces tests cassent trop souvent.\n\n\nLes web adapters sont testés avec des tests d’intégration.\nOn parle ici d’intégration parce qu’on est “intégré” avec autre chose que du code pur, en l’occurrence avec la librairie de communication HTTP.\nIl s’agit de tests communication based, envoyant un faux message HTTP sur le bon path, et vérifiant qu’on a fait un appel sur le bon use-case mocké, avec la bonne commande.\n\n\nLes persistance adapters sont testés avec des tests d’intégration.\nIci pas de mocks, on construit des domain entities et on les passe à l’adapter pour qu’il le mette en base, et on vérifie depuis la base avec des méthodes de repository qu’on a écrit la bonne chose.\nSi besoin de données préalables, par exemple pour de la lecture, on exécute d’abord du SQL pour mettre la DB dans un état qui permettra cette lecture.\nLe fait de ne pas mocker est justifié par le fait qu’on perdrait toute confiance dans nos tests puisque chaque type de DB vient avec ses propres spécificités SQL.\n\n\nLes chemins principaux complets sont testés avec des system tests.\nOn utilise de vrais messages HTTP, et on ne mock rien dans nos couches.\n\n\n\n\nOn peut utiliser des fonctions helper pour rendre nos tests plus lisibles en extrayant des bouts de code. Ces fonctions forment un domain specific language qu’il est bon de cultiver.\nConcernant la quantité de tests :\nL’auteur suggère d’aller vers du 100% de coverage du code important. Garder quand même le 100% permet de lutter contre la théorie des vitres cassées.\nPour mesurer la fiabilité des tests, on peut mesurer à quel point on est confiant pour mettre en production notre changement.\nPour être toujours plus confiant, il faut mettre en production souvent.\nPour chaque bug en production, il faut se demander ce qu’on aurait pu faire pour qu’un test trouve le bug, et l’ajouter.\nLe fait de documenter les bugs comme ça permet d’avoir une mesure de la fiabilité des tests dans le temps.","8-mapping-between-boundaries#8. Mapping Between Boundaries":"Les 3 composants principaux (driving adapter, hexagone et driven adapter) doivent avoir un modèle qui leur permet d'appréhender le système et ses entités. On peut choisir différentes stratégies de mapping entre ces modèles en fonction de leur unicité ou de leur différence.\n1- La no-mapping strategy consiste à avoir le même modèle dans l’adapter web, l’hexagone, et l’adapter de persistance.\nOn n’implémente la représentation des entities qu’une fois pour la réutiliser partout.\nChaque couche va avoir besoin de champs ou d’éléments techniques qui lui sont spécifiques, par exemple des annotations liées à HTTP pour l’adapter web, des annotations d’ORM pour l’adapter de persistance. Nos entities auront donc plusieurs raisons de changer.\nTant que toutes les couches ont besoin des informations formatées de la même manière, cette stratégie marche. C’est le cas pour les applications CRUD.\nA partir du moment où on commence à gérer des problèmes spécifiques au web ou à la persistance dans l’hexagone, alors il faut passer à une autre stratégie.\n\n\n2- La two-way mapping strategy consiste à avoir un modèle spécifique pour chacun des composants principaux (adapter web, hexagone et adapter de persistance), et de faire un mapping quand la donnée rentre, et un autre quand elle sort.\nL’avantage c’est que cette séparation des modèles permet d’adapter leur structure pour les besoins de chaque couche : besoins web (ex: sérialisation JSON), besoins du modèle, besoins de la persistance (ex: ORM).\nLe désavantage principal c’est le boilerplate conséquent.\nEt même avec l’utilisation de librairies de mapping, les bugs sont compliqués à trouver parce que le mapping est caché.\n\n\nUn autre désavantage c’est que malgré la séparation, les objets du modèle de l’hexagone sont quand même utilisés par les autres couches externes, ce qui fait qu’ils pourraient avoir besoin de changer pour des nécessités de ces couches.\n\n\n3- La full mapping strategy consiste à utiliser un mapping entre les 3 composants principaux comme pour la two-way mapping strategy, mais cette fois on va établir des modèles d’input et d’output fins spécifiques à chaque use-case.\nOn a encore plus de mapping que si on mappait juste les modèles des 3 composants, mais ce mapping va aussi être plus maintenable parce qu’il sera à chaque fois spécifique au use-case sans avoir besoin d’être adapté pour correspondre à de nouveaux besoins.\nL’auteur conseille ce pattern plutôt entre l’adapter web et l’hexagone qu’entre l’hexagone et l’adapter de persistance (parce qu’il y aurait vraiment trop de mappings).\nOn pourra aussi faire des variantes, par exemple utiliser cette stratégie pour le modèle d’entrée dans l’hexagone depuis l’adapter web, mais renvoyer les objets du modèle de l’hexagone en sortie vers l’adapter web.\n\n\n4 - La one-way mapping strategy consiste à avoir une interface commune aux trois modèles de chacun des trois composants.\nDe cette manière les objets peuvent être passés sans devoir obligatoirement les mapper. Si le mapping est nécessaire, il suffira à la couche qui en a besoin de le faire.\nQuand l’objet est passé de l’hexagone vers l’extérieur ils peuvent l’utiliser tel quel sans risquer de le modifier parce que les setters ne sont pas exposés.\nQuand l’objet passe vers l’hexagone, il devra en général être mappé pour reconstruire le comportement riche du domaine.\nOn peut le faire avec le pattern Factory du DDD.\n\n\nCette stratégie a de l’intérêt quand les modèles sont proches.\nLe modèle web peut facilement ne pas avoir besoin de mapper l’output venant de l’hexagone.\n\n\nLe désavantage c’est que cette stratégie est plus difficile à appréhender étant donné son caractère non systématique.\n\n\nIl faut adapter la stratégie en fonction des cas d’usage et de leur nature, et pas adopter une seule stratégie pour toute la codebase.\nOn peut avoir des différences de stratégie en fonction :\nDes lectures et écritures.\nDe la communication entre adapter web et hexagone, et hexagone et adapter de persistance.\n\n\n\n\nIl ne faut pas avoir peur de changer de stratégie en cours de route.\nLa plupart des applications commencent en étant CRUD, puis soit le restent, soit se complexifient suffisamment pour mériter un changement de stratégie de mapping.\nL’équipe doit se mettre d’accord sur des stratégies à choisir pour chaque partie de la codebase, et surtout noter pourquoi elle fait ce choix pour pouvoir réévaluer plus tard si le choix doit être modifié ou non.\nIl peut être intéressant aussi de noter dans quel cas elle prévoit de changer de stratégie.","9---assembling-the-application#9 - Assembling the Application":"Nous voulons garder l’inversion de dépendance entre les composants externes et l’hexagone.\nDonc nous devons instancier les adapters pour les donner au constructeur des objets de l’hexagone par le mécanisme qui s’appelle l'injection de dépendance.\n\n\nNous devons avoir un composant de configuration qui soit neutre du point de vue notre architecture, et qui ait accès à tous les composants pour les instancier.\nIl va :\nCréer les adapters web et s’assurer que les requêtes HTTP sont câblées aux bons adapters.\nCréer les adapters de persistance et s’assurer qu’elles aient accès à la base de données.\nCréer les use cases, et au moment de leur création, leur donner les adapters web et les adapters de persistance dans leur constructeur, pour qu’elles y aient accès (injection de dépendance).\n\n\nIl va aussi passer certaines valeurs de configuration aux autres composants (serveur de base de données, serveur SMTP etc).\nIl aura toutes les raisons de changer\n\n\nCôté implémentation :\nOn peut créer le composant avec du code sans librairie :\nfunction main() {\nconst accountRepository = new AccountRepository();\nconst activityRepository = new ActivityRepository();\nconst accountPersistanceAdapter = new AccountPersistanceAdapter(\nacountRepository,\nactivityRepository\n);\n\nconst sendMoneyUseCase = new SendMoneyService(accountPersistanceAdapter);\n\nconst sendMoneyController = new SendMoneyController(sendMoneyUseCase);\n\nstartProcessingWebRequests(sendMoneyController);\n}\n\nCette méthode a le désavantage d’amener à écrire beaucoup de code, et d’obliger à ce que les classes de chaque composant soient accessibles publiquement.\n\n\nParmi les techniques impliquant une librairie, en Java on a\nLe classpath scanning où il s’agit d’annoter les classes de chaque composant et demander à Spring de scanner les classes pour trouver celles qu’il faut instancier et injecter.\nElle est plus rapide mais peut mener à des bugs difficiles à trouver parce que le système de scanning est obscur.\n\n\nL’autre méthode c’est d’écrire des classes de configuration qui vont indiquer quelles classes doivent être instanciées et injectées.\nOn va écrire plus de code pour obtenir plus de découplage et de transparence sur ce qui est fait.","10---enforcing-architecture-boundaries#10 - Enforcing Architecture Boundaries":"Ce chapitre traite de la manière d’éviter que l’architecture choisie s’érode au fil du temps, causant un manque de clarté et une lenteur à faire des changements.\nLa principale chose à faire respecter est le sens des dépendances parmi les couches (domaine -> application -> adapters -> configuration).\nOn peut utiliser la visibilité de package, si le langage le permet (package-private modifier en Java), pour rendre le contenu des couches plus cohésives.\nSi on part du principe que notre composant de configuration est une librairie qui passe outre la visibilité de package et qu’elle peut instancier même les classes privées à leur package, alors on n’a que les relations entre couches à se préoccuper :\nLes adapters et les use cases peuvent être privés au package de leur couche.\nLes entities du domaine doivent être publiques pour être accédées par les couches du dessus, de même que les ports qui doivent être accédés par les adapters qui les implémentent.\n\n\n\n\nOn peut aussi utiliser des moyens au runtime pour empêcher que le sens des dépendances s’inverse, par exemple au moment des tests.\nEn Java il y a la librairie ArchUnit qui permet de faire des tests d’architecture, y compris des tests sur le sens des dépendances, en vérifiant qu’un package ne dépend pas d’un autre package (importe rien qui vienne de lui).\nOn peut construire par dessus ce genre de librairie pour obtenir un DSL (Domain Specific Language) dans nos tests, qui vérifie les dépendances pour un bounded context donné :\nHexagonalArchitecture.boundedContext(\"account\")\n.withDomainLayer(\"domain\")\n.withAdaptersLayer(\"adapter\")\n.incoming(\"web\")\n.outgoing(\"persistence\")\n.and()\n.withApplicationLayer(\"application\")\n.services(\"service\")\n.incomingPorts(\"port.in\")\n.outgoingPorts(\"port.out\")\n.and()\n.withConfiguration(\"configuration\")\n.check(new ClassFileImporter().importPackages(\"buckpal\"));\n}\n\nAttention par contre : ce genre de test est vulnérable aux refactorings. Il suffit que le nom des packages change et aucun problème ne sera trouvé sur des packages qui n’existent pas.\n\n\nOn peut enfin profiter de la phase de build de notre application pour créer autant d’artefacts de build que nécessaire, et profiter de l’outil de build pour garantir les limites des composants de notre architecture.\nOn peut découper avec diverses granularités :\nUne première solution c’est de faire 3 groupes de build : la configuration, les adapters, et l’application (hexagone).\nOn peut décider de séparer les types d’adapter pour qu’ils restent autonomes.\nUn cran plus loin encore, on peut isoler les ports dans une unité à part. De cette manière on empêche la no mapping strategy.\nOn peut enfin aussi séparer les types entrants et sortants des ports pour plus de clarté, et séparer le domaine et l’application service dans deux unités pour empêcher le domaine d’accéder aux use cases.\n\n\nUn des avantages c’est que l’outil de build nous empêchera d’avoir des dépendances circulaires entre nos unités de build, améliorant l’aspect single responsibility de nos modules.\nÊtre obligé de maintenir un script de build permet aussi de faire des choix en conscience pour ce qui est du placement des diverses classes.\nD’un autre côté c’est un certain travail de maintenance quand même, donc il vaut mieux que l’architecture soit un minimum stable d’abord.\n\n\nL’idée c’est de combiner les trois méthodes pour avoir une architecture solide dans le temps.\nA noter que plus on découpe finement, plus on devra faire de mappings.","11---taking-shortcuts-consciously#11 - Taking Shortcuts Consciously":"Les raccourcis doivent être connus et compris pour être évités dans le but de garantir l'intégrité de l’architecture, et dans certains cas acceptés en toute conscience.\nLa théorie des vitres cassées vient d’un psychologue (Philip Zimbardo) qui a conduit une expérience en laissant une voiture dans un quartier chaud, et une autre dans un quartier chic.\nLa première a été désossée rapidement puis les passants ont commencé à la dégrader.\nLa 2ème a été laissée intacte pendant une semaine. Puis le psychologue a cassé une de ses vitres, et à partir de là elle a été dégradée aussi vite que la première, par des gens de tout type.\nL’idée est de dire que nous avons une tendance naturelle à en rajouter quand les choses sont déjà en mauvais état ou mal rangées. La même chose s’applique au code et à son architecture.\n\n\nIl est de la responsabilité des développeurs de garder l’architecture la plus clean possible dès le début.\nOn peut cependant prendre des raccourcis choisis en conscience, faire des compromis qu’on assume.\nPour que l’effet des vitres cassées soit limité, il faut documenter les raccourcis choisis par l’équipe dans des ADR (architecture Decision Record).\n\n\nParmi les raccourcis possible dans l’architecture hexagonale on a :\nUtiliser le même modèle pour les inputs (ou les outputs) de deux use cases.\nLa question à se poser est : est-ce que ces deux use cases sont voués à évoluer ensemble ou non ?\nSi oui alors il faut que leurs inputs et leurs outputs aient des modèles séparés. Si non alors on peut (et on doit) les coupler au niveau de leurs inputs et outputs.\nIl ne faut donc pas oublier de régulièrement reconsidérer si deux use cases imaginés comme voués à évoluer ensemble, ne doivent pas être désormais considérés de manière séparée.\n\n\nUtiliser des entities du domaine comme modèles d’entrée ou de sortie de la couche applicative.\nIl s’agit d’avoir les ports gauches (l’entrée et la sortie depuis l’adapter web vers l’hexagone) basés sur des domain entities.\nEn faisant ça on va être tenté d’ajouter des champs à l’entity du domaine à chaque fois qu’on en aura besoin dans notre input ou output de la couche applicative. Donc le domaine va dans une certaine mesure dépendre de l’application..\nC’est OK de le faire si on est sur des use cases simple type create/update parce que dans ce cas c’est bien le contenu exact des domain entities qu’on veut.\nIl ne faut pas oublier de reconsidérer régulièrement les use cases où on l’a fait et qui se sont complexifiés.\n\n\nNe pas utiliser de ports entrants pour l’adapter web.\nLe sens du flow étant le même que le sens des appels pour l’adapter web, on peut très bien lui permettre d’appeler directement les use cases, sans passer par une interface.\nOn perd alors la clarté des points d’entrée qui ne sautent plus aux yeux aussi bien qu’avec des ports explicites.\nOn ne peut plus non plus utiliser des moyens pour forcer l’hexagone à ne pas dépendre de l’adapter web.\nCette technique est donc à réserver aux petites applications, ou celles qui n’ont qu’un adapter web.\n\n\nNe pas créer d’application service.\nParfois l'application service ne fait rien à part appeler une méthode dans l’adapter de persistance et retourner son résultat à l’adapter web. Dans ce cas, on peut laisser l’adapter web appeler directement l’adapter de persistance.\nC’est le cas quand on a des use cases CRUD.\nOn a alors nos domain entities qui sont passés entre adapters gauches et droits.\nLe danger c’est que si la fonctionnalité se complexifie, la logique métier sera ajoutée dans l’adapter de persistance. Il faut dans ce cas absolument créer un application service.\n\n\n\n\nLa plupart des raccourcis sont adaptés aux fonctionnalités simples de type CRUD. On commence en général comme ça, et on réévalue les raccourcis si l’application se complexifie. Si elle reste simple on pourra garder les raccourcis.","12---deciding-on-an-architecture-style#12 - Deciding on an Architecture Style":"La question dans ce chapitre est : quand est-ce qu’il faut utiliser l’architecture hexagonale ?\nLa première chose c’est que cette architecture met le domaine au centre et l’isole du reste, pour pouvoir travailler dessus.\nSi on n’a pas besoin de ça, on n’a sans doute pas besoin d’architecture hexagonale.\nÇa va bien avec le DDD.\n\n\nLa deuxième chose c’est qu’il faut expérimenter cette architecture au moins sur un module de notre application, pour voir ce qu’elle apporte et voir où est-ce qu’on peut l’utiliser.\nEt enfin la 3ème chose c’est que le choix de l’architecture dépend de nombreux critères :\nLe type du logiciel.\nLe rôle du domaine dans le code.\nL’expérience de l’équipe.\netc.\nNDLR : l’auteur ne se mouille pas beaucoup.\nPour info Vlad Khononov conseille dans Learning Domain Driven Design l’heuristique d’adopter l’architecture hexagonale dans le cas où on a un code subdomain et qu’on choisit les patterns tactiques du DDD, et d’adopter une architecture en couches sinon.\nOn peut imaginer qu’étant donné que l’auteur donne des raccourcis pour les use cases CRUD, y compris ceux qui resteront CRUD, il serait pour privilégier l’usage de l’architecture hexagonale la plupart du temps, en modulant avec ces raccourcis."}},"/books/refactoring":{"title":"Refactoring: Improving the Design of Existing Code","data":{"":"","1---refactoring--exemple-introductif#1 - Refactoring : exemple introductif":"Ce chapitre montre un exemple de refactoring selon les techniques qui seront décrites dans le livre.\nDescription :\nLe logiciel sert une entreprise de vente de billets de pièces de théâtre, et permet de créer des factures.\nLes clients pourront avoir des rabais en fonction de divers paramètres (par exemple le type de la pièce).\n\n\nLe programme :\nOn a deux objets/json auxquels notre fonction a accès :\nLes plays qui sont les pièces de théâtre.\nExemple :\n{\n\"hamlet\": { \"name\": \"Hamlet\", \"type\": \"tragedy\" },\n\"othello\": { \"name\": \"Othello\", \"type\": \"tragedy\" }\n}\n\nLes invoices qui sont\n{\n\"customer\": \"BigCo\",\n\"performances\": [\n{\n\"playID\": \"hamlet\",\n\"audience\": 55\n},\n{\n\"playID\": \"othello\",\n\"audience\": 40\n}\n]\n}\n\n\n\nLe code initial est le suivant :\nfunction statement(invoice, plays) {\nlet totalAmount = 0;\nlet volumeCredits = 0;\nlet result = `Statement for ${invoice.customer}\\n`;\nconst format = new Intl.NumberFormat(\"en-US\", {\nstyle: \"currency\",\ncurrency: \"USD\",\nminimumFractionDigits: 2,\n}).format;\n\nfor (let perf of invoice.performances) {\nconst play = plays[perf.playID];\nlet thisAmount = 0;\nswitch (play.type) {\ncase \"tragedy\":\nthisAmount = 40000;\nif (perf.audience > 30) {\nthisAmount += 1000 * (perf.audience - 30);\n}\nbreak;\ncase \"comedy\":\nthisAmount = 30000;\nif (perf.audience > 20) {\nthisAmount += 10000 + 500 * (perf.audience - 20);\n}\nthisAmount += 300 * perf.audience;\nbreak;\ndefault:\nthrow new Error(`unknown type: ${play.type}`);\n}\n// add volume credits\nvolumeCredits += Math.max(perf.audience - 30, 0);\n// add extra credit for every ten comedy attendees\nif (\"comedy\" === play.type)\nvolumeCredits += Math.floor(perf.audience / 5);\n// print line for this order\nresult += ` ${play.name}: ${format(thisAmount / 100)}`;\nresult += ` (${perf.audience} seats)\\n`;\ntotalAmount += thisAmount;\n}\nresult += `Amount owed is ${format(totalAmount / 100)}\\n`;\nresult += `You earned ${volumeCredits} credits\\n`;\nreturn result;\n}\n\nLe programme est court pour les besoins du livre. Pour un si petit programme la question ne se poserait peut-être pas, mais pour un programme de plusieurs centaines de lignes, il faut refactorer.\n\n\nLes utilisateurs veulent deux modifications :\n1- Afficher un relevé de compte en HTML et pas seulement sous format texte comme actuellement.\n2- Ajouter de nouveaux types de pièces de théâtre, avec chacune ses règles particulières pour le système de rabais. On ne sait pas encore quels types on voudra.\n\n\nUne première (mauvaise) solution simple serait de dupliquer la fonction pour faire la version HTML sur l’autre fonction.\nMais il faudrait ensuite mettre à jour les règles des nouveaux types de pièces sur les deux fonctions.\n\n\n\n\nOn va refactorer le programme :\n1 - Les tests\nAvant tout refactoring, la 1ère chose à faire c’est de s’assurer que cette partie du code a des tests solides. Si c’est pas le cas on les écrit.\n\n\n2 - On décompose la fonction en sortant le switch\nOn identifie d’abord à l'œil les parties qui vont ensemble.\nIci c’est d’abord le switch qu’on choisit de traiter comme un bloc.\nOn va utiliser la technique Extract Function pour sortir le bloc dans une autre fonction (qui se trouvera dans la portée de statement).\nOn regarde d’abord les variables qui sont utilisées par la nouvelle fonction extraite :\nDeux variables ne sont que lues (invoices et plays), on va pouvoir les passer en paramètre.\nUne autre variable est assignée (thisAmount), on va pouvoir retourner la valeur pour l’assigner par retour de fonction.\n\n\nUne fois qu’on a déplacé la fonction, on compile (si nécessaire), on joue les tests, et on commit.\nPour éviter de passer du temps à débugger son propre refactoring, il faut faire de petites étapes avec un commit à chaque fois.\n\n\nOn va ensuite renommer la fonction, et ses paramètres :\nfunction amountFor(aPerformance, play) {\n// …\n\nLe fait de mettre For dans le nom de la fonction et a dans le nom du paramètre fait partie du style de Fowler, qu’il a pris à Kent Beck.\n\n\n\n\n3 - On supprime la variable play\nPour chaque variable prise en paramètre de notre nouvelle fonction amountFor, on vérifie sa provenance pour voir si on ne peut pas s’en débarrasser :\naPerformance est différente à chaque tour de boucle, on doit la garder.\nplay est par contre toujours le même : on pourrait le supprimer comme paramètre, et recalculer sa valeur dans notre nouvelle fonction.\n\n\nOn va utiliser la technique Replace Temp with Query pour remplacer dans la boucle :\nconst play = plays[perf.playID];\npar\nconst play = playFor(perf);\nAvec la nouvelle fonction :\nfunction playFor(aPerformance) {\nreturn plays[aPerformance.playID];\n}\n\n\n\nOn compile, teste, commit, puis on utilise Inline Variable :\nOn supprime la variable const play = playFor(perf);\nEt on appelle la fonction au moment d’appeler amountFor :\nlet thisAmount = amountFor(perf, palyFor(perf));\n\nEt on fait le remplacement par l’appel partout où play était utilisé.\n\n\nOn compile, teste, commit, puis on peut utiliser Change Function Declaration pour éliminer le paramètre play dans amountFor :\nOn utilise la nouvelle fonction playFor à l’intérieur d’amountFor, en remplacement du paramètre play.\nOn compile, teste, commit.\nOn supprime le paramètre play qui n’était plus utilisé.\n\n\nRemarques :\nFaire 3 fois l’accès à play est moins performant que de mettre la valeur dans une variable et y accéder les deux autres fois.\nMais cette différence est négligeable.\nEt même si elle était significative, rendre le code plus clair permettra de mieux l’optimiser ensuite.\n\n\nSupprimer les variables locales permet en général de faciliter les extractions, c’est pour ça qu’on le fait.\n\n\nOn utilise encore Inline Variable pour appeler plusieurs fois amountFor dans statement, comme ça on supprime cette variable locale aussi.\n\n\n4 - on extrait les crédits de volume\nOn va extraire le bloc de calcul de crédits de volume. Pour ça on vérifie les variables qui devront être passées :\nperf doit être passé.\nplay a été supprimé par le refactoring d’avant.\nvolumeCredits est un accumulateur. On peut créer une variable locale dans la nouvelle fonction et la retourner.\n\n\nCa donne :\nfunction volumeCreditsFor(perf) {\nlet volumeCredits = 0;\nvolumeCredits += Math.max(perf.audience - 30, 0);\nif (\"comedy\" === playFor(perf).type)\nvolumeCredits += Math.floor(perf.audience / 5);\nreturn volumeCredits;\n}\n\nEt côté appel dans statement :\nvolumeCredits += volumeCreditsFor(perf);\nOn compile, teste, commit.\nOn va renommer les variables dans volumeCreditsFor :\nfunction volumeCreditsFor(aPerformance) {\nlet result = 0;\nresult += Math.max(aPerformance.audience - 30, 0);\nif (\"comedy\" === playFor(aPerformance).type)\nresult += Math.floor(aPerformance.audience / 5);\nreturn result;\n}\n\nOn a ici fait deux changements de variable (aPerformance, et result), il faut compiler, tester et commiter entre chaque changement.\nLa convention result fait partie du style de convention de Martin Fowler, au même titre que aVariable pour les variables en paramètre.\n\n\n\n\n5 - On va supprimer la variable format\nOn continue à supprimer des variables temporaires dans statement pour faciliter l’extraction de blocs.\nIci on s’attaque à format, c’est une variable contenant une fonction. On va la supprimer pour déclarer la fonction en question :\nfunction format(aNumber) {\nreturn new Intl.NumberFormat(\"en-US\", {\nstyle: \"currency\",\ncurrency: \"USD\",\nminimumFractionDigits: 2,\n}).format(aNumber);\n}\n\nCette technique de transformer une fonction dans une variable en fonction déclarée n’est pas assez importante pour figurer dans les techniques du livre.\nOn va ensuite utiliser Change Function Declaration pour renommer la fonction en quelque chose qui indique mieux ce qu’elle fait.\nVu qu’elle est utilisée dans une petite portée et est peu importante, Fowler privilégie un nom plus court que formatAsUSD. Il choisit juste usd pour mettre en avant l’aspect monétaire.\n\n\n\n\n6 - On va déplacer le calcul du volume de crédits\nOn avait précédemment extrait le calcul du volume de crédits pour une pièce. On va maintenant extraire le calcul du volume de crédits pour l’ensemble des pièces hors de statement.\nOn va utiliser Split Loop pour couper la boucle en deux, et avoir le calcul du volume de crédits dans une boucle à part.\nfor (let perf of invoice.performances) {\n// print line for this order\nresult += ` ${play.name}: ${format(thisAmount / 100)}`;\nresult += ` (${perf.audience} seats)\\n`;\ntotalAmount += thisAmount;\n}\nfor (let perf of invoice.performances) {\nvolumeCredits += volumeCreditsFor(perf);\n}\n\nOn compile, teste, commit.\n\n\nOn peut alors utiliser Slide Statements pour déplacer la déclaration de la variable volumeCredits juste au-dessus de la 2ème boucle.\nOn compile, teste, commit.\n\n\nOn va pouvoir utiliser Replace Temp with Query, la première étape pour pouvoir le faire c’est d’utiliser Extract Function :\nfunction totalVolumeCredits() {\nlet volumeCredits = 0;\nfor (let perf of invoice.performances) {\nvolumeCredits += volumeCreditsFor(perf);\n}\nreturn volumeCredits;\n}\n\nOn peut donc appeler la nouvelle fonction dans statement :\nvolumeCredits = totalVolumeCredits();\n\nOn compile, teste, commit.\n\n\nOn peut alors utiliser Inline Variable pour supprimer la variable locale volumeCredits, et appeler totalVolumeCredits là où elle était utilisée.\nOn compile, teste, commit.\n\n\nRemarques :\nPour le côté performance, la plupart du temps créer des boucles supplémentaires ne créera pas de ralentissement significatif.\nMême dans les rares cas où c’est le cas, on pourra toujours mieux optimiser après avoir rendu le code plus clair par du refactoring.\n\n\nLes étapes présentées avant chaque compilation/test/commit sont très courtes. Il arrive à Martin Fowler de ne pas faire à chaque fois des étapes aussi courtes, mais il fait quand même des étapes relativement courtes.\nEt surtout, dès que les tests échouent, il annule ce qu’il a fait depuis le dernier commit et reprend avec des étapes plus courtes (comme celles-là).\nLe but c’est de ne pas perdre de temps à débugger pendant un refactoring.\n\n\n\n\nOn va répéter la même séquence pour extraire complètement totalAmount :\nSplit Loop pour extraire l’instruction qui nous intéresse dans une boucle à part.\nSlide Statements pour déplacer la variable locale près de la nouvelle boucle.\nExtract Function pour extraire la boucle dans une nouvelle fonction.\nLe meilleur nom pour cette fonction est déjà pris par la variable totalAmount, donc on lui met un nom au hasard pour garder un code qui marche et commiter.\n\n\nInline Variable nous permet d’éliminer la variable locale, et de renommer la nouvelle fonction totalAmount.\nOn en profite aussi pour renommer la variable locale dans la nouvelle fonction en result pour respecter notre convention de nommage.\n\n\nCa donne :\nfunction totalAmount() {\nlet result = 0;\nfor (let perf of invoice.performances) {\nresult += amountFor(perf);\n}\nreturn result;\n}\n\n\n\n7 - On va fractionner les phases de calcul et de formatage\nJusqu’ici on avait refactoré pour rendre le code plus clair et mieux le comprendre. On va maintenant le mener vers l’objectif qui est de pouvoir créer des factures HTML en plus des factures texte.\nOn va mettre en œuvre Split Phase pour faire la division logique / formatage.\nPour ça on commence par appliquer Extract Function au code qui constituera la 2ème phase : on va déplacer l’ensemble du code de statement et les fonctions imbriquées dans une fonction renderPlainText.\nfunction statement(invoice, plays) {\nreturn renderplainText(invoice, plays);\n}\n\nfunction renderPlainText(invoice, plays) {\nlet result = `Statement for ${invoice.customer}\\n`;\n// ...\nreturn result;\n\nfunction totalAmount() {\n// ...\n}\n// ...\n}\n\nOn compile, teste et commit.\n\n\nOn va créer un objet qui servira de structure de données intermédiaire entre les deux phases :\nfunction statement(invoice, plays) {\nconst statementData = {};\nreturn renderplainText(statementData, invoice, plays);\n}\n\nfunction renderPlainText(data, invoice, plays) {\n//...\n\nLe but c’est de déplacer tout le calcul de logique hors de renderPlainText, et de tout lui passer au travers de l’objet data.\n(On va compiler / tester / commiter entre chaque étape)\nOn va mettre invoice.customer dans data, pour obtenir data.customer de l’autre côté.\nconst statementData = {};\nstatementData.customer = invoice.customer;\n\nOn fait pareil avec invoice.performances.\nOn veut maintenant que les valeurs des performances soient précalculées dans le paramètre data qu’on donne à renderPlainText. On commence par créer une fonction pour enrichir les performances :\nstatementData.performances =\ninvoice.performances.map(enrichPerformance);\n\nfunction enrichPerformance(aPerformance) {\nconst result = { ...aPerformance };\nreturn result;\n}\n\nOn va déplacer les informations des pièces (plays) dans les performances qu’on a mis dans data.\nPour ça on ajoute la valeur dans notre nouvelle fonction.\nfunction enrichPerformance(aPerformance) {\nconst result = { ...aPerformance };\nresult.play = playFor(result);\nreturn result;\n}\n\nEnsuite on utilise Move Function pour déplacer playFor en dessous d’enrichPerformance.\nEt enfin on remplace toutes les utilisations de playFor dans les fonctions de renderPlainText par les valeurs précalculées issues de data. On va y accéder typiquement par perf.play.\n\n\nOn va ensuite appliquer la même chose que pour playFor, mais cette fois pour amountFor dont on élimine les appels de renderPlainText.\nPuis on fait la même chose pour volumeCreditsFor.\nEt encore la même chose pour totalAmount, et totalVolumeCredits.\n\n\nOn en profite pour utiliser Replace Loop with Pipeline sur les boucles de totalAmount et totalVolumeCredits.\nfunction totalAmount(data) {\nreturn data.performances.reduce((total, p) => total + p.amount, 0);\n}\n\nOn va maintenant extraire le code de première phase qu’on vient de créer (la création des data pré-calculées dans statement) dans une fonction à part :\nfunction statement(invoice, plays) {\nreturn renderplainText(\ncreateStatementData(invoice, plays)\n);\n}\n\nfunction renderPlainText(data) {\n//...\n}\n\nfunction createStatementData(invoice, plays) {\nconst result = {};\nresult.customer = invoice.customer;\nresult.performances =\ninvoice.performances.map(enrichPerformance);\nresult.totalAmount = totalAmount(result);\nresult.totalVolumeCredits =\ntotalVolumeCredits(result);\nreturn result;\n\nfunction enrichPerformance(aPerformance) {\n// ...\n}\n// ...\n\nOn peut alors extraire createStatementData et ses fonctions imbriquées (le code de la phase 1 donc) dans un fichier à part qu’on appelle createStatementData.js.\nOn peut maintenant facilement écrire la version HTML de statement dans le fichier statement.js :\nfunction statement(invoice, plays) {\nreturn renderplainText(createStatementData(invoice, plays));\n}\n\nfunction renderPlainText(data) {\nlet result = `Statement for ${data.customer}\\n`;\n// ...\nreturn result;\n}\n\nfunction htmlStatement(invoice, plays) {\nreturn renderHtml(createStatementData(invoice, plays));\n}\n\nfunction renderHtml(data) {\nlet result = `<h1>Statement for ${data.customer}</h1>\\n`;\n// ...\nreturn result;\n}\n\nRemarque : Fowler propose de suivre la règle du camping : il faut toujours laisser le code un peu plus propre que l’état dans lequel on l’a trouvé. Le code ne sera jamais parfait, mais il sera meilleur qu’avant.\n\n\n8 - On va créer un calculateur pour types de performances\nOn s’intéresse ici au fait de faciliter l’ajout de nouveaux types de pièces de théâtre, avec chacun ses conditions et valeurs de calcul pour la facturation et les crédits de volume.\nLa solution qu’on retient c’est de créer un calculateur sous forme de classes, avec des classes filles pour contenir la logique de chaque type de pièce. On va donc mettre en œuvre la technique Replace Conditional with Polymorphism.\nNDLR : dans Clean Code, Uncle Bob disait qu’un code procédural (qui utilise les structures de données pour représenter les objets) est adapté pour ajouter des fonctionnalités supplémentaires (par exemple dans notre cas une fonctionnalité en plus du calcul de facturation et du calcul de volume de crédits). Un code orienté objet par contre est plutôt adapté pour l’ajout de nouveaux types d’objets sans ajout de nouvelles fonctionnalités (par exemple dans notre cas ajouter un nouveau type de pièce de théâtre avec ses propres règles de facturation et volume de crédits).\nL’idée est de minimiser le nombre d’éléments qui seront changés quand on fera notre changement.\n\n\n\n\nOn commence par créer le calculateur sans qu’il ne fasse rien :\nfunction enrichPerformance(aPerformance) {\nconst calculator = new PerformanceCalculator(aPerformance);\n// ...\n}\n\nclass PerformanceCalculator {\nconstructor(aPerformance) {\nthis.performance = aPerformance;\n}\n}\n\nEnsuite, pour plus de clarté, on déplace les informations des pièces :\nfunction enrichPerformance(aPerformance) {\nconst calculator = new PerformanceCalculator(\naPerformance,\nplayFor(aPerformance)\n);\n// ...\n}\n\nclass PerformanceCalculator {\nconstructor(aPerformance, aPlay) {\nthis.performance = aPerformance;\nthis.play = aPlay;\n}\n}\n\n\n\n9 - On va déplacer les fonctions de calcul dans le calculateur\nOn va d’abord déplacer amountFor dans le calculateur.\nPour ça on commence par utiliser Move Function et copier le code dans un getter, et utiliser les variables d’instance this.play et this.performance :\nget amount() {\nswitch (this.play.type) {\ncase \"tragedy\":\nthisAmount = 40000;\nif (this.performance.audience > 30) {\n//...\n}\n\nOn va ensuite instancier le calculateur et utiliser le getter amount dans amountFor, pour tester que jusque là les tests passent.\nfunction amountFor(aPerformance) {\nreturn new PerformanceCalculator(aPerformance, playFor(aPerformance))\n.amount;\n}\n\nEnfin on va utiliser Inline Function pour éliminer amountFor et utiliser calculator.amount à la place.\n\n\nOn fait la même chose pour volumeCreditsFor, pour se retrouver à utiliser calculator.volumeCredits.\n\n\n10 - On va rendre le calculateur polymorphe\nOn va commencer par utiliser Replace Type Code with Subclasses pour ça.\nPour obtenir la bonne classe, on va utiliser Replace Constructor with Factory Function :\nfunction enrichPerformance(aPerformance) {\nconst calculator = createPerformanceCalculator(\naPerformance,\nplayFor(aPerformance)\n);\n//...\n}\n\nfunction createPerformanceCalculator(aPerformance, aPlay) {\nswitch (aPlay.type) {\ncase \"tragedy\":\nreturn TragedyCalculator(aPerformance, aPlay);\ncase \"comedy\":\nreturn ComedyCalculator(aPerformance, aPlay);\ndefault:\nthrow new Error(`Unknown type: ${aPlay.type}`);\n}\n}\n\nclass TragedyCalculator extends PerformanceCalculator {}\n\nclass ComedyCalculator extends PerformanceCalculator {}\n\nOn peut maintenant utiliser Replace Conditional with Polymorphism pour déplacer les fonctions de calcul dans les classes filles.\nOn peut créer un getter du même nom (amount) dans TragedyCalculator par exemple, pour y déplacer le code lié à la facturation des tragédies.\nPuis on le fait pour la facturation des comédies.\nOn peut alors supprimer get amount() de PerformanceCalculator. Ou alors le laisser et y throw une erreur indiquant que la fonctionnalité est déléguée aux classes filles.\nOn peut faire pareil avec les volumes de crédits. Pour celui-là on pourra laisser le cas le plus courant (attribuer des crédits si l’auditoire est supérieur à 30 personnes) dans PerformanceCalculator, et ne surcharger que pour ComedyCalculator.\n\n\n\n\n\n\n\n\nComme souvent, les premières phases du refactoring permettent de comprendre ce que fait le code en le clarifiant. On peut ensuite réinjecter cette compréhension dans la suite des refactorings pour le faire aller dans le sens qu’on veut.\nLes petites étapes sont étonnantes au premier abord, mais cette méthode est vraiment efficace, et permet d’avancer sereinement et rapidement pour faire au final des refactorings importants.","2---principes-du-refactoring#2 - Principes du refactoring":"L’auteur propose une définition plus restreinte du refactoring que ce qu’on entend habituellement : il s’agit pour lui d’une succession de petits changements qui permettent de rendre le code plus facile à comprendre et à changer, sans changer son comportement.\nComme ce sont de petits changements indépendants, on peut arrêter à tout moment en gardant le code fonctionnel.\nIl propose restructuration (restructuring) comme mot plus général pour désigner le fait de réorganiser le code, le refactoring étant une forme particulière de restructuration.\n\n\nLes développeurs ont deux casquettes distinctes qu’ils peuvent porter, une à la fois : celle d’ajout de changements, et celle de refactoring.\nIl est important d’essayer de garder ces deux casquettes distinctes pour être efficace dans ce qu’on fait et avancer sereinement.\nLa casquette de changement mène normalement à l’ajout ou à la modification de tests, alors que la casquette de refactoring ne devrait pas mener à toucher aux tests.\nNDLR : à petite échelle il s’agit aussi des casquettes qu’on adopte en TDD : red-green avec celle du changement, et refactor avec celle du refactoring.\n\n\nPourquoi faire du refactoring ?\nPour conserver et améliorer l’architecture du logiciel qui se délite peu à peu.\nPour rendre le code plus lisible et compréhensible.\nOn met la connaissance qu’on a au moment où on a passé du temps sur le code dans le code lui-même, comme ça on peut nous-mêmes l’oublier sans souci.\n\n\nPour révéler les bugs qui se cachaient dans du code fouilli.\nPour programmer plus rapidement.\nSi on met en place du code bien structuré et compréhensible, on pourra s’appuyer sur le code existant pour coder plus vite de nouvelles features.\nCette hypothèse est basée sur l’expérience de Fowler et celle de centaines de programmeurs qu’il connaît.\n\n\n\n\nQuand faire du refactoring ?\nLa règle de trois : on fait quelque chose une fois, la 2ème fois qu’on le fait on laisse passer, la troisième fois on fait un refactoring.\nLe meilleur moment est le refactoring préparatoire : juste avant de faire une modification, on remanie le code pour rendre cette modification plus facile.\nÇa peut être un refactoring de compréhension, ou un refactoring de ramassage d’ordures quand on se rend compte que le code est mal structuré pour ce qu’on veut en faire.\n\n\nLa plupart des refactorings doivent être opportunistes, c’est-à-dire s’intégrer au flux habituel d’ajout de fonctionnalités ou de correction de bugs.\nOn peut parfois aussi faire des refactorings planifiés si on a vraiment négligé le code ou qu’on tombe sur quelque chose de spécifique qui le nécessite.\n\n\nLe refactoring est aussi nécessaire pour le code qui est déjà de bonne qualité, parce qu’il ne s’agit d’adapter en permanence le code à notre compréhension actuelle du système, et celle-ci varie tout le temps.\nL’idée de séparer les features et les refactorings dans des features séparées n’a pas que des avantages, Fowler est plutôt réticent..\nElle permet de faire des reviews plus ciblées, mais d’un autre côté le refactoring est souvent lié au contexte du changement qu’il accompagne, et on risque d’avoir plus de mal à le comprendre et à le justifier isolément.\nNDLR : Kent Beck a récemment conseillé de faire des PRs séparées avec de petites granularités pour pouvoir choisir le moment où on fait des refactorings et fournir des fonctionnalités régulières aux personnes qui attendent.\n\n\nOn a parfois besoin de gros refactorings, par exemple pour remplacer une librairie. Dans ce cas, l'auteur conseille de procéder petit à petit quand même.\nPour la librairie, on peut mettre en place une abstraction devant la librairie actuelle, et remplacer les fonctionnalités derrière l’abstraction petit à petit. On appelle ça Branch By Abstraction.\n\n\nLe refactoring est aussi utile pendant les code reviews : on retravaille le code pour comprendre plus en profondeur ce que la personne a fait, et pour avoir des idées d’amélioration qu’on pourra mettre en place immédiatement.\nCa implique de faire la code review en présence de la personne qui a fait la PR. Faire des Code reviews sans la personne ne fonctionne de toute façon pas très bien selon Fowler.\nLa conclusion logique de la pratique est le pair programming.\n\n\nQuand ne pas faire de refactoring :\nQuand on tombe sur du code qui part dans tous les sens mais qu’on n’a pas besoin de le modifier.\nQuand il est préférable de réécrire le code plutôt que de le remanier. Ce genre de décision se fait avec l’expérience.\n\n\n\n\nQue dire aux managers pour le refactoring ?\nLes managers qui ont une bonne compréhension de la technique vont de toute façon encourager le refactoring parce qu’ils sauront que ça permet de rester productif.\nPour ceux qui n’ont pas de bonne compétences techniques, le conseil controversé de Fowler est de ne pas leur dire qu’on en fait.\nNous sommes les professionnels du développement, nous sommes payés pour notre expertise à coder vite, et le refactoring nous permet justement de coder vite.\n\n\n\n\nDe manière générale, et y compris auprès des développeurs, il ne faut pas justifier le refactoring par “la beauté du code” ou “les bonnes pratiques”, mais par le critère économique qui met tout le monde d’accord : ça permet d’aller plus vite.\nIl vaut mieux éviter des granularités trop fines pour ce qui est de la propriété du code, notamment au sein d’une équipe : chaque membre de l’équipe devrait pouvoir modifier toute la codebase pour pouvoir faire des refactorings qui toucheraient éventuellement jusque ces endroits-là.\nOn peut étendre ça entre les équipes où n’importe qui de l’entreprise pourrait faire un PR chez la codebase d’une autre équipe.\n\n\nL’utilisation de feature branches est problématique par rapport aux conflits de mege, et en particulier par rapport aux refactorings qui vont provoquer beaucoup de conflits.\nLa pratique du refactoring va de pair avec la Continuous Integration (CI), aussi appelée trunk-based development, qui consiste à intégrer au moins une fois par jour son travail sur le branche principale.\nLes deux pratiques font partie de l’Extreme Programming.\nA noter que la CI est prouvée comme plus efficace que les autres pratiques d’intégration (cf. le livre Accelerate).\n\n\n\n\nPour faire des refactorings, il faut soit des tests qui assurent que le comportement n’est pas changé, soit utiliser des outils qui font des refactorings automatiquement (par exemple renommer une variable ou extraire une fonction).\nDans Working Effectively with Legacy Code, Michael Feathers décrit comment ajouter des tests à du code legacy : il faut trouver des points d’entrée où insérer des tests, et pour ça il faut prendre des risques en faisant du refactoring.\n\n\nPour ce qui est du refactoring de bases de données, il faut aussi y aller par petits pas, en créant de petites migrations successives.\nIl y a un livre à ce sujet : Refactoring Databases.\nUne bonne pratique est le changement parallèle (aussi appelé expand-contract) où on va d’abord créer la nouvelle structure, puis l’alimenter avec l’ancienne, puis migrer tout petit à petit pour utiliser la nouvelle. Et finalement supprimer l’ancienne.\n\n\nLe refactoring implique d’adopter une approche incrémentale de l’architecture.\nOn appelle ça aussi YAGNI (you aren’t going to need it) : il s’agit de ne pas rendre le code inutilement flexible pour plus tard “au cas où”.\nPar exemple, ne pas ajouter des paramètres non utilisés à une fonction, au cas où on en aurait besoin plus tard. On les ajoutera avec du refactoring quand on en aura effectivement besoin.\nSouvent le besoin imaginé ne se réalise pas, ou pas comme on l’avait imaginé.\nÇa a bien sûr des limites : parfois un refactoring sera beaucoup plus coûteux plus tard. Dans ce cas, on peut l’envisager tout de suite. Mais ce n'est pas si courant.\n\n\nCette approche de l’architecture est aussi étudiée sous le nom d’evolutionary architecture.\n\n\nLe refactoring fait partie d’un ensemble de techniques interdépendantes et cohérentes qui permet l’agilité. Elles sont regroupées au sein de l’Extreme Programming.\nParmi ces techniques il y a notamment : le refactoring, l’intégration continue (CI), la livraison continue (CD), YAGNI, et les tests automatisés.\n\n\nIl y a 3 manières d’aborder la question de la performance :\nLa 1ère est la budgétisation du temps : au moment de la conception on attribue un budget temps qui ne doit pas être dépassé à chaque composant.\nC’est utile pour les systèmes temps réel comme les simulateurs cardiaques, mais inadapté à des systèmes web classiques.\n\n\nLa 2ème est l’attention constante : on essaye de faire attention à la performance sur tout le code qu’on écrit.\nLe problème c’est qu’en général l’essentiel du temps d’exécution des programmes se concentre sur très peu de code. Et on passe 90% du temps à optimiser des choses qui n’ont aucun impact.\nUn autre problème c’est qu’on a en général une mauvaise idée de la manière dont se comporte le compilateur, le runtime, le matériel etc. et on optimise des choses à tort.\nCette solution mène à perdre beaucoup de temps à faire des optimisations inutiles, et à obtenir du code peu maintenable.\n\n\nLa 3ème méthode consiste à séparer l’optimisation de performance dans une phase à part : on code sans prendre en compte la performance, puis on passe du temps dédié à l’améliorer.\nOn utilise un profiler pour repérer les endroits du code qui consomment le plus (de temps, de mémoire etc.), et on se concentre sur ça seulement.\nOn procède là aussi itérativement par petites touches, en annulant ce qu’on a fait si ça n’améliore pas.\nLe fait d’avoir du code bien refactoré permet de plus facilement comprendre ce qui se passe, et aide donc à optimiser.\nCette approche est la plus efficace.\n\n\n\n\nQuelques livres intéressants sur le refactoring :\nRefactoring Workbook de Bill Wake : un livre avec des exercices pour mettre en application le refactoring.\nRefactoring to Patterns de Josh Kerievsky : comment appliquer le refactoring en utilisant les design patterns du gang of four.\nRefactoring Databases de Scott Ambler et Pramod Sadalage et Refactoring HTML de Elliotte Rusty : des livres appliquant le refactoring à des domaines spécifiques.\nWorking Effectively with Legacy Code de Michael Feathers : comment faire du refactoring sur du code avec peu ou pas de tests.","3---quand-le-code-sent-mauvais#3 - Quand le code sent mauvais":"On ne peut pas savoir quand un refactoring est nécessaire mieux que l’intuition d’un programmeur expérimenté, mais ce chapitre contient une liste de 24 code smells qui devraient au moins nous alerter quand on les croise.\n1 - Mysterious Name : quand on ne comprend pas un nom de variable ou fonction au premier coup d'œil, il faut la renommer.\nSi on n’arrive pas à trouver un bon nom, c’est sans doute qu’on a d’autres problèmes plus profonds avec le code qu’on essaye de nommer.\nParmi les techniques, il y a Change Function Declaration, Rename Function et Rename Field.\n\n\n2 - Duplicated Code : quand on a du code dupliqué, il faut essayer de le factoriser pour avoir moins d’endroits à maintenir à jour à chaque modification.\nEn général on va utiliser Extract Function.\nSi le code dupliqué n’est pas tout à fait identique, on peut d’abord utiliser Slide Statements pour obtenir un morceau de code identique à factoriser.\nSi le code dupliqué se trouve dans des classes filles d’une même hiérarchie, on peut la remonter dans la mère avec Pull Up Method.\n\n\n3 - Long Function : les fonctions courtes sont plus efficaces pour la compréhension du code.\nL’idée c’est de nommer les fonctions avec l’intention de leur code plutôt que par ce qu’il fait. A chaque fois qu’on veut commenter, on peut remplacer ça par une fonction qui encapsule le bout de code.\nC’est tout à fait OK de faire des fonctions qui ne contiennent qu’une ligne, pour peu que le nommage apporte une meilleure information sur l’intention.\nEn général, on va utiliser Extract Function.\nLes conditions peuvent être divisées avec Decompose Conditional.\nUn grand switch devrait avoir ses clauses transformées en un seul appel de fonction avec Extract Function.\nS’il y a plus d’un switch sur la même condition, alors il faut appliquer Replace Conditional with Polymorphism.\n\n\nLes boucles peuvent être extraites dans leur propre fonction.\nSi on a du mal à nommer la fonction, alors on peut appliquer d’abord Split Loop.\n\n\n\n\n4 - Long Parameter List : trop de paramètres porte à confusion, il faut essayer de les éliminer.\nSi on peut obtenir un paramètre à partir d’un autre, alors on peut appliquer Replace Temp with Query pour l’éliminer.\nSi plusieurs paramètres sont toujours ensemble, on peut les combiner avec Introduce Parameter Object.\nSi un argument est utilisé pour choisir une logique dans la fonction, on peut diviser la logique en plusieurs fonctions avec Remove Flag Argument.\nOn peut aussi regrouper les fonctions qui ont des paramètres communs en classes avec Combine Functions into Class, pour remplacer les paramètres par des champs.\n\n\n5 - Global Data : le problème des données globales c’est qu’on peut les modifier de n’importe où, et donc c’est très difficile de suivre ce qui se passe.\nPour traiter le problème, il faut les encapsuler avec Encapsulate Variable.\n\n\n6 - Mutable Data : le fait que les structures soient mutables fait qu’on peut changer une structure quelque part, et provoquer un bug ailleurs sans s’en rendre compte.\nLa recherche de l'immutabilité vient de la programmation fonctionnelle.\nOn peut utiliser Encapsulate Variable pour s’assurer qu’on modifie la structure à partir de petites fonctions.\nSi une variable est mise à jour pour stocker plusieurs choses, on peut utiliser Split Variable pour rendre ces updates moins risquées.\nIl faut essayer de garder la logique qui n’a pas de side effects et le code qui modifie la structure séparés, avec Slide Statements et Extract Function. Et dans les APIs, on peut utiliser Separate Query from Modifier pour que l’appelant fasse des queries sans danger.\nDès que c’est possible, il faut utiliser Remove Setting Method pour enlever les setters.\nLes données mutables qui sont calculées ailleurs sont sources de bugs, il faut les remplacer par Replace Derived Variable with Query.\nIl faut essayer de limiter le scope du code qui a accès aux variables mutables. Par exemple avec Combine Functions into Class, ou Combine Functions into Transform.\nSi une variable contient déjà une structure avec d’autres données, il vaut mieux remplacer la structure entière d’un coup, plutôt que de modifier la variable, avec Change Reference to Value.\n\n\n7 - Divergent Change : quand on a un module qui doit être modifié pour plusieurs raisons, on est face à des changements divergents.\nPar exemple si on se dit “Je devrai modifier ces trois fonctions si j’ajoute une nouvelle base de données, et ces quatre fonctions si j’ajoute un nouvel instrument financier” : les bases de données et les instruments financiers sont deux contextes différents qu’il vaut mieux traiter séparément.\nSi les deux contextes forment deux phases (par exemple il faut obtenir les infos de la base de données, puis appliquer un instrument financier), alors on peut utiliser Split Phase pour séparer les deux avec une structure de données.\nSinon on peut utiliser Extract Function pour les séparer dans plusieurs fonctions.\nEt si c’est des classes : Extract Class.\n\n\n8 - Shotgun Surgery : c’est l’inverse du Divergent Change, on a une fonctionnalité qui est dispersée à plusieurs endroits qu’il faut à chaque fois aller modifier.\nOn peut utiliser Move Function et Move Field pour replacer le code au même endroit.\nSi on a des fonctions qui opèrent sur les mêmes données, on peut les associer avec Combine Functions into Class.\nOn peut aussi combiner le code éparpillé dans une grande fonction ou classe (avec Inline Function et Inline Class) avant de séparer ça en plus petites fonctions.\n\n\n9 - Feature Envy : on essaye en général d’avoir des modules à l’intérieur desquels il y a beaucoup de communication, et entre lesquels il y en a peu. On parle de feature envy quand un module communique plus avec du code ‘un module voisin qu’avec le module où il est.\nEn général on va utiliser Move Function, parfois précédé d’Extract Function si seule une partie de la fonction a besoin de changer d’endroit.\n\n\n10 - Data Clumps : quand on a un groupe de données qui se retrouvent toujours ensemble, c’est qu’elles doivent peut-être rejoindre une même structure.\nOn va d’abord chercher où ces données apparaissent sous forme de champs pour les extraire dans une nouvelle classe avec Extract Class.\nOn parle bien d’extraire dans une classe et pas dans une simple structure, parce que ça va permettre ensuite d’y ajouter du comportement propre à ces données, typiquement quand on a des Feature Envies.\n\n\nAu niveau des paramètres des fonctions on va alors pouvoir utiliser Introduce Parameter Object et Preserve Whole Object.\n\n\n11 - Primitive Obsession : il s’agit d’utiliser des Value Objects à la place des types primitifs comme number ou string.\nExemple : un numéro de téléphone doit être validé, et correctement affiché.\nLa règle typique c’est Replace Primitive with Object.\nSi le type primitif est impliqué dans une structure conditionnelle, on peut encapsuler les conditions dans une hiérarchie de classes avec Replace Type Code with Subclasses puis Replace Conditional with Polymorphism.\n\n\n12 - Repeated Switches : on repère les switchs portant sur la même condition, et on les remplace par des classes.\nIl s’agit d’utiliser Replace Conditional with Polymorphism.\n\n\n13 - Loops : les fonctions issues de la programmation fonctionnelle (map, filter, reduce) permettent de voir plus rapidement les éléments qui sont inclus et ce qui est fait avec eux, par rapport à des boucles.\nOn peut remplacer les boucles par des pipelines avec Replace Loop with Pipeline.\n\n\n14 - Lazy Element : parfois certaines classes ou fonctions sont inutiles.\nPar exemple une fonction dont le corps se lit de la même manière que son nom, ou une classe qui n’a qu’une méthode et qui pourrait être une fonction.\nOn peut les éliminer avec Inline Function ou Inline Class.\nDans le cas où on veut réduire une hiérarchie de classe, on peut utiliser Collapse Hierarchy.\n\n\n15 - Speculative Generality : quand on ajoute des mécanismes de flexibilité pour plus tard, au cas où il y en aurait besoin. Il faut s’en débarrasser parce que YAGNI.\nOn peut se débarrasser de classes qui ne font pas grand chose avec Collapse Hierarchy.\nLes délégations inutiles peuvent être éliminées avec Inline Function et Inline Class.\nLes paramètres inutilisés par les fonctions peuvent être enlevés avec Change Function Declaration.\nSi les seuls utilisateurs d’une fonction sont des tests, il faut les supprimer, puis appliquer Remove Dead Code.\n\n\n16 - Temporary Field : quand une classe contient un champ utilisé seulement dans certains cas, ça rend le code plus difficile à comprendre.\nOn peut utiliser Extract Class puis Move Function pour déplacer le code qui utilise le champ qui est à part.\nIl se peut aussi qu’on puisse réduire le problème au fait de traiter le cas où les variables ne sont pas valides en utilisant Introduce Special Case.\n\n\n17 - Message Chains : il s’agit de longues chaînes d’appels d’objet en objet pour obtenir quelque chose au bout du compte. Si une des méthodes d’un des objets de la chaîne change, notre appelant doit changer aussi.\nOn peut utiliser Hide Delegate sur les objets intermédiaires.\nUne autre solution est de voir si on peut utiliser Extract Function suivi de Move Function pour déplacer l’utilisation de la chaîne d’appels plus bas dans la chaîne.\n\n\n18 - Middle Man : il est normal d’encapsuler et de déléguer des choses, mais si une classe délègue la moitié de ses méthodes à une autre classe, c’est qu’il est peut être temps de s’interfacer directement avec la classe qui sait ce qui se passe.\nLa technique à utiliser est Remove Middle Man.\nOn peut aussi utiliser Replace Superclass with Delegate ou Replace Subclass with Delegate pour fondre le middle man dans la classe cible.\n\n\n19 - Insider Trading : il s’agit de code de modules différents qui communique trop entre eux, et donc un couplage trop important entre modules.\nOn peut utiliser Move Function et Move Field pour séparer le code qui ne devrait pas être trop couplé.\nDans le cas où les modules ont des choses en commun, on peut aussi en extraire une classe commune, ou utiliser Hide Delegate pour utiliser un module comme intermédiaire.\n\n\n20 - Large Class : une classe avec trop de champs doit être divisée en plusieurs classes.\nOn peut typiquement repérer les noms de variable qui partagent un préfixe ou suffixe commun, et utiliser Extract Class, ou encore Extract Superclass ou Replace Type Code with Subclasses.\nSi la classe a trop de code, on va probablement avoir des duplications. Le mieux est alors de la refactorer en petites fonctions.\nPar exemple pour une classe de 500 lignes, on peut refactorer en méthodes de 5 à 10 lignes, avec une dizaine de méthodes de 2 lignes extraites dans une classe à part.\n\n\nUn bon indicateur pour découper une classe c’est en regardant le code qui l’utilise, souvent on peut repérer des parties dans la classe.\n\n\n21 - Alternative Classes with Different Interfaces : ça peut être intéressant de pouvoir substituer une classe par une autre. Pour ça il faut faire correspondre leur prototype en les faisant adhérer à une même interface.\nPour faire correspondre les deux classes, on peut utiliser Change Function Declaration et Move Function.\n\n\n22 - Data Class : les classes qui ont des getters/setters mais peu ou pas de logique sont un signe que la logique n’est pas au bon endroit.\nLeurs champs publics doivent être encapsulés avec Encapsulate Record.\nLes setters pour les méthodes qui ne doivent pas être changés doivent être enlevés avec Remove Setting Method.\nEnsuite on peut chercher où ces getters et setters sont utilisés, et appliquer Move Function (et au besoin Extract Function) pour déplacer la logique dans la classe qu’on cherche à enrichir.\nIl y a des exceptions : certaines classes peuvent être légitimes en tant que structure de données, et dans ce cas il n’y a pas besoin de getters et setters. Leurs champs seront immutables et donc n’auront pas besoin d’être encapsulés.\nExemple : la structure de données qui permet de communiquer entre les deux phases quand on utilise Split Phase.\n\n\n\n\n23 - Refused Bequest : parfois certaines classes filles refusent certaines implémentations venant du parent.\nIl ne s’agit pas d’une forte odeur, donc on peut parfois le tolérer.\nSi on veut régler le problème, on peut utiliser une classe soeur et pousser le code qui ne devrait pas être partagé vers elle avec Push Down Method et Push Down Field.\nParfois, ce n'est pas l’implémentation d’une méthode, mais l’interface que la classe fille ne veut pas. Dans ce cas l’odeur est beaucoup plus forte et il faut éliminer l’héritage pour le remplacer par de la délégation avec Replace Subclass with Delegate ou Replace Superclass with Delegate.\n\n\n24 - Comments : la plupart des commentaires cachent des code smells, et sont inutiles si on les refactore.\nQuand on en rencontre, il faut essayer de voir si on ne peut mieux expliquer ce que fait un bloc de code avec Extract Function et Change Function Declaration. Ou encore déclarer des règles sur l’état du système avec Introduce Assertion.\nSi malgré ça on a toujours besoin du commentaire, alors c’est qu’il est légitime. Il peut servir à décrire ce qui se passe, indiquer les endroits où on n’est pas sûr, ou encore expliquer pourquoi on a fait quelque chose.","4---création-de-tests#4 - Création de tests":"Les tests sont utiles pour le refactoring, mais aussi plus généralement ils permettent d’économiser un temps conséquent à débugger.\nEcrire le test avant le code permet :\nDe mieux se concentrer sur le fonctionnalité qu’on veut faire, en s’intéressant d’abord à l’interface plutôt qu’à l’implémentation.\nDe savoir quand la fonctionnalité est terminée : quand le test passe.\n\n\nExemple de programme à tester :\nOn a un plan de production qui doit montrer la production des producteurs classés par province.\nLe code est organisé avec deux classes : Producer qui contient les données du producteur, et Province qui prend des données JSON, et calcule les éléments liés à la production.\nExemple de test pour le déficit de production :\ndescribe(\"province\", () => {\nit(\"shortfall\", () => {\nconst asia = new Province(sampleProvinceData());\nexpect(asia.shortfall).toBe(5);\n});\n});\n\n\n\nConcernant la manière de nommer les tests, Fowler ne prend pas position mais se contente de dire que certains développeurs préfèrent faire des phrases, et d’autres écrire peu de mots, et que lui écrit suffisamment de mots pour reconnaître les tests quand ils sont en erreur.\nQuand on ajoute des tests à du code non testé pour le refactorer, on ne peut pas faire la séquence “red” puis “green”. On peut donc à la place jouer le test qui marche, puis introduire une erreur dans le code et vérifier que le test échoue bien comme on l’aurait pensé.\nL’auteur conseille d'exécuter les tests souvent : si on ne fait pas de TDD, les tests du module sur lequel on travaille au moins toutes les 10 minutes, et l’ensemble des tests au moins une fois par jour.\nIl ne faut tester que le code qui en vaut la peine. Typiquement, les getters/setters n’ont pas à être testés parce que le risque d’erreur est faible.\nIl faut factoriser les répétitions dans les tests comme on le fait pour le code de prod.\nComme pour le code de prod, il faut à tout prix éviter les variables globales (et y compris si elles sont assignées dans un beforeAll), et utiliser soit des variables locales aux tests, soit des beforeEach.\nFowler aime bien avoir un dispositif de test standard pour chaque groupe de tests, avec lequel il faut se familiariser avant de les regarder.\nbeforeEach peut être une convention pour ce dispositif.\nLe describe permet de créer des groupes de tests où il y aura un beforeEach. Si notre test ne rentre pas dans le contexte du dispositif, on pourra créer un autre bloc avec éventuellement un autre dispositif.\n\n\nIl conseille d’essayer de se limiter à une assertion par test, sauf si les éléments testés sont étroitement liés.\nL’argument est que si le test échoue pour un des premiers asserts, on ne saura pas si les autres échouaient ou non.\nNDLR : C’est en contradiction avec le conseil de Vladimir Khorikov sur le fait qu’il faille un seul “act”, et qu'au niveau du “assert” on teste toujours toutes les conséquences.\n\n\nIl faut aussi tester les cas aux limites :\nSi on a une collection, on peut voir ce qui se passe si elle est vide.\nSi on a un nombre, on peut voir ce qui se passe avec 0 ou un nombre négatif.\nSi on a une chaîne, on peut voir ce qui se passe avec la chaîne vide.\nÇa nous oblige à chaque fois à nous poser la question de savoir si ça a un sens. Par exemple, si le nombre négatif n’aurait pas de sens, on peut s’attendre à une erreur.\nIl est possible que l’entrée vienne d’un module déjà testé, dans ce cas il n’est pas nécessaire de tester notre module avec des cas qui ne devraient pas se produire.\nSi un cas d’erreur conduit à une potentielle corruption de données, on peut utiliser Introduce Assertion pour éviter que ça arrive. Il n’y a pas besoin de tester ce genre de cas.\nNDLR : on est sur du fail fast.\n\n\n\n\nQuand on a un bug, l’auteur conseille d’abord d’écrire un test qui reproduit le bug, et ensuite de le faire passer au vert.\nLa bonne quantité de tests c’est quand on est suffisamment confiant pour faire du refactoring et savoir que si on introduit un bug, il sera révélé par les tests.","5---présentation-du-catalogue#5 - Présentation du catalogue":"Les chapitres suivants présentent un catalogue de techniques de refactoring suffisamment importants pour être nommés et décrits.\nMartin Fowler utilise lui-même le catalogue pour se refamiliariser avec des techniques de refactorings qu’il n’a pas faites depuis longtemps.","6---premier-ensemble-de-refactorings#6 - Premier ensemble de refactorings":"","extract-function#Extract Function":"Exemple :\nAvant :\nfunction printOwig(invoices) {\nprintBanner();\nlet outstanding = calculateOutstanding();\n// display details;\nconsole.log(`name: ${invoice.customer}`);\nconsole.log(`amount: ${outstanding}`);\n}\n\nAprès :\nfunction printOwing(invoice) {\nprintBanner();\nlet outstanding = calculateOutstanding();\nprintDetails(outstanding);\n\nfunction printDetails(outstanding) {\nconsole.log(`name: ${invoice.customer}`);\nconsole.log(`amount: ${outstanding}`);\n}\n}\n\n\n\nÉtapes :\n\nOn crée une nouvelle fonction pour accueillir le code à extraire, on la nomme correctement, et on copie le code à extraire dedans.\n\n\nSi le langage le supporte (comme JavaScript), on peut extraire la fonction dans la portée de la première fonction, ne serait-ce que pour visualiser si l’extraction a du sens sans que ce soit coûteux. On pourra la déplacer éventuellement plus tard.\n\n\n\nDans le cas où la fonction est exportée hors de la portée de la fonction source, on lui passe toutes les variables dont elle a besoin en paramètre.\n\n\nSi une variable n’est utilisée que dans le code extrait, on la déplace dedans.\nSi une variable utilisée en dehors est assignée à l’intérieur du code extrait, alors il faut faire en sorte que la fonction extraite la retourne.\nSi on a trop de variables assignées dans le code extrait, on abandonne l’extraction au profit d’abord de Split Variable ou de Replace Temp with Query.\n\n\n\nOn Compile.\n\n\n\nOn remplace le code initialement extrait par un appel à la nouvelle fonction.\n\n\n\nOn teste.\n\n\n\nOn recherche d’autres bouts de code similaires au code extrait pour les remplacer par un appel à la nouvelle fonction avec Replace Inline Code with Function Call.\n\n\n\n\nThéorie :\nLe bon argument sur pourquoi extraire une fonction est de séparer l’intention de l’implémentation, pour que l’intention saute aux yeux à la première lecture.\nL’auteur a tendance à écrire des fonctions courtes, une fonction dépassant une demi-douzaine de lignes commence à sentir mauvais, et les fonctions d’une ligne ne sont pas rares pour peu qu’elles expriment mieux l’intention.\nLa qualité du nommage est fondamentale avec les petites fonctions.\nSi on n’arrive pas à trouver un nom pour la fonction extraite, c’est peut être un signe que l’extraction est inutile.\nIl est normal d’extraire, de manipuler le code, puis de se rendre compte que l’extraction était inutile. Tant qu’on a appris quelque chose, on n’a pas perdu son temps.\n\n\nTypiquement si on a un commentaire qui marque la séparation d’un bloc de code, c’est une bonne heuristique pour extraire.","inline-function#Inline Function":"Exemple :\nAvant :\nfunction getRating(driver) {\nreturn moreThanFiveLateDeliveries(driver) ? 2 : 1;\n}\n\nfunction moreThanFiveLateDeliveries(driver) {\nreturn driver.numberOfLateDeliveries > 5;\n}\n\nAprès :\nfunction getRating(driver) {\nreturn driver.numberOfLateDeliveries > 5 ? 2 : 1;\n}\n\n\n\nÉtapes :\n\nOn vérifie que la fonction n’est pas une méthode polymorphe.\n\n\n\nOn trouve les endroits où la fonction est appelée et on remplace par le corps de la fonction.\n\n\n\nOn teste après chaque remplacement.\n\n\n\nOn peut supprimer la fonction qui n’est plus appelée.\n\n\n\n\nThéorie :\nLes fonctions courtes sont plus lisibles, mais parfois on peut avoir des fonctions qui n’apportent rien, parce que leur contenu est déjà clair. Dans ce cas on les enlève.\nUne autre raison d’enlever les fonctions c’est pour d’abord regrouper le code dans un grand bloc avant de mieux le découper.\nDans le cas où on rencontre des difficultés importantes pour faire l’incorporation, l’auteur conseille de ne pas faire ce refactoring.","extract-variable#Extract Variable":"Exemple :\nAvant :\nreturn (\norder.quantity * order.itemPrice -\nMath.max(0, order.quantity * 500) * order.itemPrice -\n0.05 +\nMath.min(order.quantity * order.itemPrice * 0.1, 100)\n);\n\nAprès :\nconst basePrice = order.quantity * order.itemPrice;\nconst quantityDiscount = Math.max(0, order.quantity - 500) *\norder.itemPrice * 0.05;\nconst shipping = Math.min(basePrice * 0.1, 100);\nreturn basePrice - quantityDiscount + shipping;\n```\n\n\n\nÉtapes :\n\nOn vérifie que l’expression qu’on veut extraire n’a pas de side effects.\n\n\n\nOn crée une nouvelle variable immutable avec la valeur de l’expression.\n\n\n\nOn remplace l’expression d’origine par la nouvelle variable.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nCes variables permettent de décomposer le code pour le rendre plus lisible.\nUne fois qu’on a choisi le nom, on réfléchit au contexte : si c’est un contexte local une variable est très bien, si c’est plus global il vaut mieux une fonction qu’on réutilisera dans plusieurs endroits avec le même nom.\nTypiquement si on est dans une classe, il y a des chances pour que le concept qu’on extrait puisse devenir un membre de la classe (par exemple avec get monConcept()).","inline-variable#Inline Variable":"Exemple :\nAvant :\nlet basePrice = anOrder.basePrice;\nreturn basePrice > 1000;\n\nAprès :\nreturn anOrder.basePrice > 1000;\n\n\n\nÉtapes :\n\nOn vérifie que l’expression qu’on veut réintégrer n’a pas de side effects.\n\n\n\nSi elle ne l’est pas, on rend la variable immutable pour éviter qu’elle ne soit réassignée plus bas.\n\n\n\nOn remplace l’utilisation de la variable par son expression, occurrence par occurrence, en testant à chaque fois.\n\n\n\nOn supprime la variable et on teste.\n\n\n\n\nThéorie :\nQuand un nom de variable devient inutile, on peut la supprimer.","change-function-declaration#Change Function Declaration":"Exemple :\nAvant :\nfunction circum(radius) {...}\n\nAprès :\nfunction circumference(radius) {...}\n\n\n\nÉtapes (version simple) :\n\nSi on supprime des paramètres, on s’assure qu’ils ne sont plus référencés.\n\n\n\nOn remplace la déclaration de la fonction par la nouvelle déclaration.\n\n\n\nOn trouve tous les endroits où l’ancienne déclaration était appelée, et on les met à jour.\n\n\n\nOn teste.\n\n\n\n\nÉtapes (version progressive) :\n\nSi besoin, on refactore le corps de la fonction pour faciliter l’extraction.\n\n\n\nOn extrait l’ensemble du corps de la fonction dans une nouvelle fonction avec Extract Function.\n\n\nSi elle va avoir le même nom que l’ancienne, on la nomme avec un nom temporaire.\n\n\n\nSi on a des changements de paramètres, on peut utiliser la version simple pour les changer sur la nouvelle fonction.\n\n\nL’ancienne fonction devra donner des valeurs par défaut pour les nouveaux paramètres pour continuer à marcher.\n\n\n\nOn teste.\n\n\n\nOn applique Inline Function à l’ancienne fonction pour la faire disparaître.\n\n\n\nSi besoin de modifier le nom parce qu’on avait un nom temporaire, on le fait.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nLes noms des fonctions sont difficiles à choisir, et on doit souvent s’y prendre à plusieurs fois. Dès qu’on a un nom meilleur pour une fonction, il faut l’utiliser.\nUne bonne manière de trouver un bon nom est d’écrire en commentaire ce que fait la fonction, puis de transformer ça en nom.\nLa même difficulté se retrouve avec les paramètres, on doit donc régulièrement refactorer pour améliorer petit à petit..\nPar exemple, il n'y a pas de bonne réponse au fait de savoir s’il faut passer un objet entier à une fonction ou seulement l’une de ses propriétés qui est actuellement utilisée.\n\n\nLa version simple sert pour les cas simples, alors que la version progressive est à utiliser dans le cas où on a de nombreuses occurrences à remplacer, ou d’autres difficultés comme une méthode polymorphe.\nIl est préférable de faire la modification de nom et de paramètres en deux étapes pour la version simple.\nDe manière générale, si la méthode simple ne marche pas du premier coup, il faut annuler les changements et recommencer avec la méthode progressive.\nSi on refactore une API, il faut appliquer la méthode progressive et faire une pause dès qu’on a la nouvelle fonction, sans supprimer l’ancienne. On la supprimera quand on sera sûr que les clients ont migré.\n\n\n\n\nExemple détaillé :\nOn a une fonction qui détermine si un client est basé en Nouvelle-Angleterre ou pas. On veut qu’elle prenne plutôt la valeur indiquant le lieu et non plus l’objet client entier.\nfunction inNewEngland(aCustomer) {\nreturn [\"MA\", \"CT\", \"ME\", \"VT\", \"NH\", \"RI\"].includes(\naCustomer.address.state\n);\n}\n\nconst newEnglanders = someCustomers.filter((c) => inNewEngland(c));\n\nOn va utiliser la version progressive :\nD’abord on utilise Extract Variable pour faciliter l’extraction de fonction.\nfunction inNewEngland(aCustomer) {\nconst stateCode = aCustomer.address.state;\nreturn [\"MA\", \"CT\", \"ME\", \"VT\", \"NH\", \"RI\"].includes(stateCode);\n}\n\nEnsuite on applique Extract Function.\nfunction inNewEngland(aCustomer) {\nconst stateCode = aCustomer.address.state;\nreturn xxNEWinNewEngland(stateCode);\n}\n\nfunction xxNEWinNewEngland(stateCode) {\nreturn [\"MA\", \"CT\", \"ME\", \"VT\", \"NH\", \"RI\"].includes(stateCode);\n}\n\nPuis on applique Inline Variable à la fonction initiale.\nfunction inNewEngland(aCustomer) {\nreturn xxNEWinNewEngland(aCustomer.address.state);\n}\n\nPuis on utilise Inline Function pour remplacer les appels à l’ancienne fonction par des appels à la nouvelle fonction, petit à petit. A la fin on supprime l’ancienne fonction.\nconst newEnglanders = someCustomers.filter((c) =>\nxxNEWinNewEngland(c.address.state)\n);\n\nOn réutilise Change Function Declaration, cette fois avec la version simple, pour changer le nom de la nouvelle fonction. On obtient alors la version finale.\nfunction inNewEngland(stateCode) {\nreturn [\"MA\", \"CT\", \"ME\", \"VT\", \"NH\", \"RI\"].includes(stateCode);\n}\n\nconst newEnglanders = someCustomers.filter((c) =>\ninNewEngland(c.address.state)\n);","encapsulate-variable#Encapsulate Variable":"Exemple :\nAvant :\nlet defaultOwner = { firstName: \"Martin\", lastName: \"Fowler\" };\n\nAprès :\nlet defaultOwnerData = { firstName: \"Martin\", lastName: \"Fowler\" };\n\nexport function defaultOwner() {\nreturn defaultOwnerData;\n}\nexport function setDefaultOwner(arg) {\ndefaultOwnerData = arg;\n}\n\n\n\nÉtapes :\n\nOn crée des fonctions pour lire et écrire dans la variable qu’on veut encapsuler.\n\n\n\nOn remplace chaque référence à la variable par un appel à ces fonctions, en testant à chaque fois.\n\n\n\nOn limite la visibilité de la variable.\n\n\nSi le langage ne le permet pas, on la renomme pour voir si ça casse quelque chose.\n\n\n\nOn teste.\n\n\n\n\n\n\nSi la variable est un record, on peut envisager Encapsulate Record.\n\n\nThéorie :\nAlors qu’il est facile de déplacer une fonction, ça l’est beaucoup moins pour des données, par exemple des variables globales. L’encapsulation peut donc être une première étape pour ce déplacement.\nL’autre avantage c’est de pouvoir ajouter un traitement systématique à chaque lecture ou écriture de la donnée.\nDans le cas où la donnée est immutable, ces problèmes se posent moins : on peut les copier facilement au lieu de les déplacer, et on n’a pas besoin d’ajouter un traitement à la lecture ou à l’écriture.\nL’auteur encapsule toutes les données mutables qui dépassent la portée d’une seule fonction.\nPour y arriver, il saisit chaque occasion d’accéder à nouveau à une donnée mutable qui est hors de la fonction pour l’encapsuler.\n\n\nEn JavaScript on peut implémenter cette technique avec l’utilisation de ES modules : on exporte la fonction getter et setter mais pas la variable.\nPour ce qui est de la convention de nommage, l’auteur déconseille le préfixe get, mais préfère laisser le préfixe set parce qu’il n’aime pas la pratique de l’overloaded getter setter qui consiste à leur donner le même nom.\nDans le cas où on veut contrôler ce qui arrive au contenu de notre variable (si elle est une référence) :\nOn peut renvoyer une copie au moment de la lecture.\nOu on peut utiliser Encapsulate Record :\nlet defaultOwnerData = { firstName: \"Martin\", lastName: \"Fowler\" };\nexport function defaultOwner() {\nreturn new Person(defaultOwnerData);\n}\nexport function setDefaultOwner(arg) {\ndefaultOwnerData = arg;\n}\n\nclass Person {\nconstructor(data) {\nthis._lastName = data.lastName;\nthis._firstName = data.firstName;\n}\nget lastName() {\nreturn this._lastName;\n}\nget firstName() {\nreturn this._firstName;\n}\n}","rename-variable#Rename Variable":"Exemple :\nAvant :\nlet a = height * width;\n\nAprès :\nlet area = height * width;\n\n\n\nÉtapes :\n\nDans le cas où la variable est utilisée au-delà de la fonction, on peut envisager Encapsulate Variable.\n\n\n\nOn recherche toutes les références à la variable, et on les modifie.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nLes noms de variable sont moins importants quand leur portée est petite, et requièrent une plus grande attention quand elle est plus grande.\nUn nom peut évoluer pour diverses raisons, parmi lesquelles une meilleure compréhension du problème, ou l’évolution de la solution.\nDans le cas où le changement de nom est difficile, il vaut mieux passer par encapsuler la variable, et en général on va vouloir garder cette encapsulation.\nSi la variable est immutable, on peut passer par des remplacements progressifs en testant au fur et à mesure, sans avoir besoin d’encapsuler.","introduce-parameter-object#Introduce Parameter Object":"Exemple :\nAvant :\nfunction amountInvoiced(startDate, endDate) {...}\nfunction amountReceived(startDate, endDate) {...}\nfunction amountOverdue(startDate, endDate) {...}\n\nAprès :\nfunction amountInvoiced(aDateRange) {...}\nfunction amountReceived(aDateRange) {...}\nfunction amountOverdue(aDateRange) {...}\n\n\n\nÉtapes :\n\nSi on n’a pas encore de structure pour regrouper les paramètres visés, on la crée.\n\n\nL’auteur préfère créer une classe pour y mettre de la logique, dans l’idée d’avoir un Value Object.\n\n\n\nOn teste.\n\n\n\nOn utilise Change Function Declaration pour ajouter une instance de notre nouvelle structure en paramètre aux fonctions qui prennent les paramètres qu’on veut grouper.\n\n\n\nOn teste.\n\n\n\nOn modifie chaque appelant pour qu’il ajoute l’instance de la nouvelle structure, et on teste à chaque fois.\n\n\n\nOn remplace l’utilisation de chacun des anciens paramètres par les éléments de la nouvelle structure déjà passée en paramètre. Et on supprime à chaque fois l’ancien paramètre concerné.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nGrouper les paramètres qui voyagent souvent ensemble dans le même objet permet :\nd’avoir moins de paramètres à passer aux fonctions.\nde créer des groupements cohérents, permettant de mieux comprendre le domaine.\nde rapatrier de la logique spécifique à l’objet en question dans une classe qu’on crée pour contenir les paramètres qui vont ensemble.\n\n\n\n\nExemple détaillé :\nOn a plusieurs fonctions qui prennent des paramètres min et max.\nfunction readingsOutsideRange(station, min, max) {\nreturn station.readings.filter((r) => r.temp < min || r.temp > max);\n}\n\nalerts = readingsOutsideRange(\nstation,\noperatingPlan.temperatureFloor,\noperatingPlan.temperatureCeiling\n);\n\nOn va créer une classe pour contenir min et max, et ajouter un paramètre supplémentaire à notre fonction.\nclass NumberRange {\nconstructor(min, max) {\nthis._data = {min: min, max: max};\n}\nget min() {\nreturn this._data.min;\n}\nget max() {\nreturn this._data.max;\n}\n}\n\nfunction readingsOutsideRange(station, min, max, range) {\n// ...\n\nOn modifie les appelants pour qu’ils fournissent le nouveau paramètre range.\nconst range = new NumberRange(\noperatingPlan.temperatureFloor,\noperatingPlan.temperatureCeiling\n);\nalerts = readingsOutsideRange(\nstation,\noperatingPlan.temperatureFloor,\noperatingPlan.temperatureCeiling,\nrange\n);\n\nOn supprime les anciens paramètres un par un, min puis max.\nfunction readingsOutsideRange(station, range) {\nreturn station.readings.filter(\n(r) => r.temp < range.min || r.temp > range.max\n);\n}\n\nalerts = readingsOutsideRange(station, range);\n\nEt enfin pour aller un peu plus loin que ce refactoring, on va tirer parti du fait qu’on a un Value Object pour y placer de la logique.\nclass NumberRange {\n// ...\ncontains(arg) {\nreturn arg >= this.min && arg <= this.max;\n}\n// ...\n}\n\nfunction readingsOutsideRange(station, range) {\nreturn station.readings.filter((r) => !range.contains(r.temp));\n}","combine-functions-into-class#Combine Functions into Class":"Exemple :\nAvant :\nfunction base(aReading) {...}\nfunction taxableCharge(aReading) {...}\nfunction calculateBaseCharge(aReading) {...}\n\nAprès :\nclass Reading {\nbase() {...}\ntaxableCharge() {...}\ncalculateBaseCharge() {...}\n}\n\n\n\nÉtapes :\n\nOn applique Encapsulate Record aux paramètres communs entre les fonctions.\n\n\nSi ces paramètres ne sont pas déjà au sein d'une même structure, on applique d’abord Introduce Parameter Object pour les regrouper.\n\n\n\nOn utiliser Move Function pour déplacer chaque fonction visée dans la nouvelle classe qu’on vient de créer.\n\n\nOn peut supprimer les arguments de ces fonctions qui sont déjà membres de la classe.\n\n\n\nOn va ensuite à la recherche de la logique restante qui manipule la donnée qu’on a encapsulée, pour l’extraire sous forme de fonctions avec Extract Function et la rapatrier dans la nouvelle classe.\n\n\n\n\nThéorie :\nUn des usages des classes c’est de grouper des fonctions qui prennent des paramètres communs, pour donner ces paramètres au constructeur et éviter d’avoir à les donner à chaque appel de fonction.\nEn fonction du contexte, si on n’a pas besoin de modifier les instances indépendamment mais plutôt grouper deux fonctions à appeler ensemble, on voudra peut-être plutôt utiliser Combine Functions into Transform.\nOn peut aussi utiliser les fonctions imbriquées à la place des classes pour mutualiser les paramètres, mais dans ce cas on ne pourra accéder qu’à la fonction de plus haut niveau. La classe est plus flexible.\nIl ne faut pas hésiter à créer des méthodes sous forme de getter pour aller dans le sens du Uniform Access Principle.\nExemple : l’appelant accède à baseCharge comme si c’était une valeur, et ne sait pas si on a une fonction qui est appelée derrière ou pas.\nclass Reading {\n// ...\nget baseCharge() {\nreturn this.baseRate(this.month, this.year) * this.quantity;\n}\n}\n\nconst aReading = new Reading(rawReading);\nconst basicChargeAmount = aReading.baseCharge;","combine-functions-into-transform#Combine Functions into Transform":"Exemple :\nAvant :\nfunction base(aReading) {...}\nfunction taxableCharge(aReading) {...}\n\nAprès :\nfunction enrichReading(argReading) {\nconst aReading = _.cloneDeep(argReading);\naReading.baseCharge = base(aReading);\naReading.taxableCharge = taxableCharge(aReading);\nreturn aReading;\n}\n\n\n\nÉtapes :\n\nOn crée une fonction qui prend le record à transformer et en retourne une deep copy.\n\n\nOn aura sans doute besoin d’un test pour vérifier la nature de la copie.\n\n\n\nOn prend une partie de la logique qu’on veut utiliser pour la transformation et on la déplace dans la fonction de transformation.\n\n\n\nOn teste.\n\n\n\nOn déplace les autres parties de logique, et on teste à chaque fois.\n\n\n\n\nThéorie :\nOn aurait pu simplement utiliser Extract Function pour extraire la logique commune, mais l’intérêt de combiner des fonctions avec les données sur lesquelles elles opèrent (avec Combine Functions into Class et Combine Functions into Transform) c’est qu’on les retrouve plus facilement qu’une fonction qui se balade.\nD’un point de vue convention de nommage, l’auteur aime bien le préfixe enrich quand la fonction renvoie la même donnée modifiée, et transform quand c’est une donnée qu’il estime être autre chose.\nPour la deep copy, on peut utiliser lodash.","split-phase#Split Phase":"Exemple :\nAvant :\nconst orderData = orderString.split(/\\s+/);\nconst productPrice = priceList[orderData[0].split(\"-\")[1]];\nconst orderPrice = parseInt(orderData[1]) * productPrice;\n\nAprès :\nconst orderRecord = parseOrder(order);\nconst orderPrice = price(orderRecord, priceList);\n\nfunction parseOrder(aString) {\nconst values = aString.split(/\\s+/);\nreturn {\nproductID: values[0].split(\"-\")[1],\nquantity: parseInt(values[1]),\n};\n}\n\nfunction price(order, priceList) {\nreturn order.quantity * priceList[order.productID];\n}\n\n\n\nÉtapes :\n\nOn extrait le code de la 2ème phase dans sa fonction avec Extract Function.\n\n\n\nOn teste.\n\n\n\nOn introduit une structure de données en paramètre de la nouvelle fonction.\n\n\n\nOn teste.\n\n\n\nOn examine chaque paramètre pris par la fonction de la 2ème phase :\n\n\nSi elle est aussi utilisée dans la 1ère phase, on la déplace dans la structure de données.\nSi elle n’est pas utilisée par la 1ère phase (la fonction principale prend le paramètre et le transfert à la fonction de la 2ème phase), on la laisse en paramètre.\nOn teste après chaque déplacement.\n\n\n\nOn extrait le code de la 1ère phase dans une fonction à part retournant la structure de données avec Extract Function.\n\n\n\n\nThéorie :\nQuand on a un code qui fait deux choses différentes, on peut le séparer en deux phases distinctes, pour que la prochaine fois qu’on le modifie, on puisse éventuellement intervenir sur une seule des deux phases indépendamment de l’autre.\nSouvent on va séparer une phase de mise en forme d’une phase de calcul, mais ça peut aussi être par exemple deux phases de calcul indépendantes.","7---encapsulation#7 - Encapsulation":"","encapsulate-record#Encapsulate Record":"Exemple :\nAvant :\norganization = { name: \"Acme Gooseberries\", country: \"GB\" };\n\nAprès :\nclass Organization {\nconstructor(data) {\nthis._name = data.name;\nthis._country = data.country;\n}\nget name() {\nreturn this._name;\n}\nset name(arg) {\nthis._name = arg;\n}\nget country() {\nreturn this._country;\n}\nset country(arg) {\nthis._country = arg;\n}\n}\n\n\n\nÉtapes :\n\nOn utilise Encapsulate Variable sur la variable qui a la valeur du record.\n\n\n\nOn wrap la variable par une classe qui permet d’y accéder avec un accesseur, et on fait en sorte que les fonctions d’accès/modification qu’on vient de créer à l’étape 1 utilisent cet accesseur.\n\n\n\nOn teste.\n\n\n\nOn crée des fonctions d’accès/modification qui renvoie l’objet wrappant le record.\n\n\n\nPour chaque utilisation extérieure du record, on va utiliser les nouvelles fonctions qui renvoient l’objet wrapper.\n\n\nSi le record est complexe, on va s’intéresser d’abord aux occurrences qui le modifient.\nOn peut envisager de retourner une copie ou une version read-only pour les occurrences qui ne font que de la lecture.\n\n\n\nOn supprime les fonctions qui permettent d’accéder au record non wrappé sur l’objet, et en dehors.\n\n\n\nOn teste.\n\n\n\nSi le record a des champs qui sont eux-mêmes des records, on peut envisager d’utiliser Encapsulate Record et Encapsulate Collection de manière récursive sur ces champs aussi.\n\n\n\n\nThéorie :\nLes records sont des structures permettant de regrouper des données, souvent sous forme de HashMaps, en javascript des objets {}.\nQuand ces structures sont utilisées dans de nombreux endroits, on peut avoir du mal à trouver comment les utiliser. C’est pour cela qu'il est pratique d’y ajouter de la logique en les transformant en classe.\nIl peut y avoir un trade off entre encapsuler tous les champs (en particulier pour les structures imbriquées) dans la classe qui les englobe, et retourner la structure pour que le client puisse l’explorer en mode read-only ou en donnant une copie.\nWrapper tous les champs permet d’avoir plus de flexibilité mais demande plus de travail et n’en vaut pas toujours la peine.\nComme on ne veut pas casser l’encapsulation dans le cas des modifications, on peut soit créer une copie (mais ça peut poser des problèmes de performance), soit renvoyer un objet read-only (mais c’est difficile à faire en JavaScript.\nNDLR : Immer permet par exemple de retourner l’équivalent d’une valeur read-only. Typescript permet aussi de le faire à la compilation.\n\n\n\n\n\n\nExemple détaillé :\nOn a un record :\nconst organization = { name: \"Acme Gooseberries\", country: \"GB\" };\n\nOn va encapsuler la variable en la déplaçant dans un module, et en ajoutant un accesseur avec un nom moche qui sera bientôt supprimé.\nfunction getRawDataOfOrganization() {\nreturn organization;\n}\n\nOn encapsulate la variable dans une classe pour pouvoir ensuite la protéger plus finement, et on ajoute un accesseur pour l’instance de cette classe.\nclass Organization {\nconstructor(data) {\nthis._data = data;\n}\n}\n\nconst organization = new Organization({\nname: \"Acme Gooseberries\",\ncountry: \"GB\",\n});\n\nfunction getRawDataOfOrganization() {\nreturn organization._data;\n}\n\nfunction getOrganization() {\nreturn organization;\n}\n\nOn va maintenant chercher tous les endroits où on lit ou écrit dans notre record, et ajouter des getters ou setters dans notre classe.\nclass Organization {\n// ...\nset name(aString) {\nthis._data.name = aString;\n}\nget name() {\nreturn this._data.name;\n}\n}\n\n// là où on met à jour le record\ngetOrganization().name = newName;\n// là où on lit le record\nresult += `<h1>${getOrganization().name}</h1>`;\n\nOn peut maintenant supprimer l’accesseur du record brut qui avait un nom bâclé (getRawDataOfOrganization()).\nEt enfin on peut déplacer les champs du record dans la classe, et créer des getters/setters pour ces champs.\nclass Organization {\nconstructor(data) {\nthis._name = data.name;\nthis._country = data.country;\n}\nget name() {\nreturn this._name;\n}\nset name(aString) {\nthis._name = aString;\n}\nget country() {\nreturn this._country;\n}\nset country(aCountryCode) {\nthis._country = aCountryCode;\n}\n}","encapsulate-collection#Encapsulate Collection":"Exemple :\nAvant :\nclass Person {\n// ...\nget courses() {\nreturn this._courses;\n}\nset courses(aList) {\nthis._courses = aList;\n}\n}\n\nAprès :\nclass Person {\n// ...\nget courses() {\nreturn this._courses.slice();\n}\naddCourse(aCourse) {\n// ...\n}\nremoveCourse(aCourse) {\n// ...\n}\n}\n\n\n\nÉtapes :\n\nOn applique Encapsulate Variable sur la collection si elle n’est pas déjà encapsulée avec la valeur originale privée, et l’accès par getter et setter.\n\n\n\nOn ajoute des méthodes pour ajouter et supprimer un objet de la collection.\n\n\n\nSi il existe un setter pour la collection, on le supprime avec Remove Setting Method.\n\n\n\nOn lance la vérification statique du code.\n\n\n\nOn remplace toutes les modifications de la collection par des appels aux nouvelles méthodes d’ajout et de suppression. On teste à chaque fois.\n\n\n\nOn modifie le getter pour renvoyer une copie ou un objet en lecture seule.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nL’encapsulation est utile pour les données mutables, pour pouvoir contrôler leur modification, mais si on donne une référence à une collection, on ne contrôle plus la manière dont elle sera modifiée.\nL’auteur trouve dommage cependant d’empêcher entièrement l’accès à la collection, étant donné que le langage offre de nombreuses options pour la parcourir et la manipuler, options qu’on aurait du mal à recoder dans notre classe.\nCe qu’on peut faire c’est en fournir une copie, et utiliser les méthodes add et remove de la classe quand on veut vraiment la modifier.\nOn peut aussi, à la place de la copie, fournir la collection en lecture seule. Mais il faut choisir l’une des deux comme convention pour la codebase et s’y tenir.","replace-primitive-with-object#Replace Primitive with Object":"Exemple :\nAvant :\norders.filter((o) => \"high\" === o.priority || \"rush\" === o.priority);\n\nAprès :\norders.filter((o) => o.priority.higherThan(new Priority(\"normal\")));\n\n\n\nÉtapes :\n\nOn applique Encapsulate Variable si la valeur n’est pas déjà encapsulée, en créant un getter et un setter.\n\n\n\nOn crée une classe qui prend la valeur dans son constructeur, et fournit un getter pour cette donnée qu’elle contient.\n\n\n\nOn lance la vérification statique du code.\n\n\n\nOn modifie le setter de la variable encapsulée pour qu’il crée une instance de la classe qu’on a créée et qu’il la stocke.\n\n\n\nOn modifie le getter de la variable encapsulée pour qu’il renvoie le résultat du getter de l’instance de la classe qu’on a créée.\n\n\n\nOn teste.\n\n\n\nOn utilise éventuellement Rename Function sur le getter et le setter pour leur donner un meilleur nom (étant donné qu’on manipule maintenant une classe qui contient la valeur et non pas la valeur elle-même).\n\n\n\nOn peut clarifier le rôle de notre nouvel objet en tant que Value Object ou Reference Object, en appliquant Change Reference to Value ou Change Value to Reference.\n\n\n\n\nThéorie :\nDe nombreux programmeurs expérimentés considèrent que c’est un des refactorings les plus précieux.\nA chaque fois qu’une valeur sert à autre chose qu’à un simple affichage, l’auteur crée une classe pour contenir la valeur et de la logique liée à la valeur.\n\n\nExemple détaillé :\nOn a une classe Order avec un champ priority.\nclass Order {\nconstructor(public priority) {}\n}\n\nconst highPriorityCount = orders.filter(\no => \"high\" === o.priority || \"rush\" === o.priority\n).length;\n\nOn encapsule la variable avec un getter et un setter.\nclass Order {\nconstructor(private _priority) {}\nget priority() {\nreturn this._priority;\n}\nset priority(aString) {\nthis._priority = aString;\n}\n}\n\nOn crée une classe pour encapsuler la valeur.\nclass Priority {\nconstructor(value) {\nthis._value = value;\n}\ntoString() {\nreturn this._value;\n}\n}\n\nOn modifie les getter/setter d’Order pour manipuler une instance de la nouvelle classe Priority.\nclass Order {\nconstructor(private _priority) {}\nget priority() {\nreturn this._priority.toString();\n}\nset priority(aString) {\nthis._priority = new Priority(aString);\n}\n}\n\nOn renomme le getter en priorityString pour mieux refléter ce qu’il fait.\nOn décide que Priority va devenir un Value Object, pour qu’il puisse être utile aussi en dehors d’Order.\nclass Priority {\nconstructor(value) {\nif (value instanceof Priority) return value;\nif (Priority.legalValues().includes(value)) this._value = value;\nelse throw new Error(`<${value}> is invalid for Priority`);\n}\ntoString() {\nreturn this._value;\n}\nget _index() {\nreturn Priority.legalValues().findIndex((s) => s === this._value);\n}\nstatic legalValues() {\nreturn [\"low\", \"normal\", \"high\", \"rush\"];\n}\nhigherThan(other) {\nreturn this._index > other._index;\n}\n}","replace-temp-with-query#Replace Temp with Query":"Exemple :\nAvant :\nconst basePrice = this._quantity * this._itemPrice;\nif (basePrice > 1000) return basePrice * 0.95;\nelse return basePrice * 0.98;\n\nAprès :\nget basePrice() {\nthis._quantity * this._itemPrice;\n}\n\n//...\nif (this.basePrice > 1000)\nreturn this.basePrice * 0.95;\nelse\nreturn this.basePrice * 0.98;\n\n\n\nÉtapes :\n\nOn s’assure que la valeur de la variable est calculée avant son utilisation, et que le code qui la calcule ne renvoie pas des valeurs différentes à chaque appel.\n\n\n\nOn transforme si possible la variable en lecture seule (const).\n\n\n\nOn teste.\n\n\n\nOn extrait le calcul de la variable dans une fonction.\n\n\nSi la variable et la fonction doivent avoir le même nom, on choisit un nom temporaire pour la fonction.\nSi la fonction extraite a des effets secondaires, on va les séparer en utilisant Separate Query from Modifier.\n\n\n\nOn teste.\n\n\n\nOn utilise Inline Variable pour enlever la variable temporaire initiale.\n\n\n\n\nThéorie :\nUtiliser des appels de fonction au lieu de variables temporaires permet de :\nfaciliter le refactoring parce qu’on a moins de paramètres à passer aux fonctions qu’on extrait.\néviter la duplication de logique en appelant la même fonction à chaque fois qu’on croise cette logique-là.\n\n\nLes classes sont particulièrement propices à l’utilisation de ce genre de fonctions.\nCe refactoring ne marche qu’avec les variables qui sont calculées une fois puis lues mais pas assignées à nouveau. Il faut aussi que leur calcul soit déterministe.","extract-class#Extract Class":"Exemple :\nAvant :\nclass Person {\nget officeAreaCode() {\nreturn this._officeAreaCode;\n}\nget officeNumber() {\nreturn this._officeNumber;\n}\n}\n\nAprès :\nclass Person {\nget officeAreaCode() {\nreturn this._telephoneNumber.areaCode;\n}\nget officeNumber() {\nreturn this._telephoneNumber.number;\n}\n}\n\nclass TelephoneNumber {\nget areaCode() {\nreturn this._areaCode;\n}\nget number() {\nreturn this._number;\n}\n}\n\n\n\nÉtapes :\n\nOn réfléchit d’abord aux responsabilités à séparer dans une nouvelle classe.\n\n\n\nOn crée une nouvelle classe pour accueillir ces responsabilités.\n\n\nAu besoin, on renomme la classe initiale pour mieux refléter ce qu’elle fera après le refactoring.\n\n\n\nOn crée une instance de la classe enfant dans le constructeur de la classe parente.\n\n\n\nOn utilise Move Field sur chaque champ à déplacer, en testant à chaque fois.\n\n\n\nOn utilise Move Function sur chaque méthode à déplacer, en testant à chaque fois.\n\n\n\nOn revoit l’interface de chaque classe, en changeant le nom des méthodes si besoin.\n\n\n\nOn décide si on veut exposer l’instance de la nouvelle classe ou pas.\n\n\nSi oui, on peut envisager d’utiliser Change Reference to Value sur la nouvelle classe pour la transformer en value object.\n\n\n\n\nThéorie :\nLes classes ont tendance à grossir au cours du temps. Cette technique permet d’extraire une partie de la logique dans une classe distincte.\nOn se rend compte qu’il y a une classe à extraire quand on a des méthodes et des champs qui sont souvent modifiés et utilisés ensemble.","inline-class#Inline Class":"Exemple :\nAvant :\nclass Person {\nget officeAreaCode() {\nreturn this._telephoneNumber.areaCode;\n}\nget officeNumber() {\nreturn this._telephoneNumber.number;\n}\n}\n\nclass TelephoneNumber {\nget areaCode() {\nreturn this._areaCode;\n}\nget number() {\nreturn this._number;\n}\n}\n\nAprès :\nclass Person {\nget officeAreaCode() {\nreturn this._officeAreaCode;\n}\nget officeNumber() {\nreturn this._officeNumber;\n}\n}\n\n\n\nÉtapes :\n\nOn crée les fonctions de la classe à faire disparaître dans la classe qui accueille. Ces fonctions doivent déléguer à celles de la classe qui va disparaître.\n\n\n\nOn change toutes les occurrences qui utilisent la classe qui va disparaître pour qu’elles utilisent la classe qui accueille, avec les fonctions qui délèguent.\n\n\nOn teste après chaque changement.\n\n\n\nOn déplace un par un le contenu des fonctions de la classe qui va disparaître vers les fonctions qui délèguent.\n\n\nOn teste à chaque fois.\n\n\n\nOn supprime la classe.\n\n\n\n\nThéorie :\nOn utilise en général cette technique pour réorganiser une classe : on la vide de ses responsabilités en extrayant des classes, puis on réintègre intelligemment dans de nouvelles classes.\nUne autre approche peut être de fusionner une classe dans une autre, et réextraire mieux ensuite.","hide-delegate#Hide Delegate":"Exemple :\nAvant :\nmanager = aPerson.department.manager;\n\nclass Person {\n// …\nget department() {\nreturn this._department;\n}\n}\n\nAprès :\nmanager = aPerson.manager;\n\nclass Person {\n// …\nget manager() {\nreturn this._department.manager;\n}\n}\n\n\n\nÉtapes :\n\nPour chaque méthode à laquelle on accède sur le délégué, on crée une méthode de délégation sur notre classe initiale.\n\n\n\nOn ajuste les occurrences de code qui utilisaient le délégué à partir de notre classe pour qu’elles appellent les nouvelles méthodes de délégation.\n\n\nOn teste à chaque fois.\n\n\n\nSi possible, on supprime le getter qui renvoyait le délégué directement.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nL’idée de cette technique est d’encapsuler les objets (les délégués) qui sont accessibles à partir des champs d’une classe, comme ça quand leur interface change, seule la classe qui les expose sera impactée et pas les appelants.","remove-middle-man#Remove Middle Man":"Exemple :\nAvant :\nmanager = aPerson.manager;\n\nclass Person {\n// …\nget manager() {\nreturn this._department.manager;\n}\n}\n\nAprès :\nmanager = aPerson.department.manager;\n\nclass Person {\n// …\nget department() {\nreturn this._department;\n}\n}\n\n\n\nÉtapes :\n\nOn crée un getter pour renvoyer le délégué.\n\n\n\nOn ajuste les occurrences de code qui utilisaient les méthodes de délégation pour qu’elles utilisent le délégué par chaînage.\n\n\nOn teste à chaque fois.\nOn supprime les méthodes de délégation une par une quand elles ne sont plus utilisées.\n\n\n\n\nThéorie :\nL’ajout de délégation va dans le sens de la Loi de Demeter, mais Fowler aimerait qu’on l’appelle plutôt la Suggestion Utile de Demeter, dans la mesure où elle n’est pas à suivre à la lettre, et les délégations peuvent parfois devenir trop coûteuses.","substitute-algorithm#Substitute Algorithm":"Exemple :\nAvant :\nfunction foundPerson(people) {\nfor (let i = 0; i < people.length; i++) {\nif (people[i] === \"Don\") {\nreturn \"Don\";\n}\nif (people[i] === \"John\") {\nreturn \"John\";\n}\nif (people[i] === \"Kent\") {\nreturn \"Kent\";\n}\n}\nreturn \"\";\n}\n\nAprès :\nfunction foundPerson(people) {\nconst candidates = [\"Don\", \"John\", \"Kent\"];\nreturn people.find((p) => candidates.includes(p)) || \"\";\n}\n\n\n\nÉtapes :\n\nOn organise le code pour avoir l’algorithme dans une même fonction.\n\n\n\nOn crée des tests pour cette fonction, pour vérifier qu’elle ne change pas de comportement.\n\n\n\nOn code notre algorithme alternatif.\n\n\n\nOn joue nos vérifications statiques et on teste.\n\n\nSi les testent ne passent pas, on débogue.\n\n\n\n\nThéorie :\nParfois il est plus simple de réécrire complètement un algorithme plutôt que de le faire par petites étapes. Cette technique dit comment le faire.","8---déplacement-des-fonctionnalités#8 - Déplacement des fonctionnalités":"","move-function#Move Function":"Exemple :\nAvant :\nclass Account {\n// ...\nget overdraftCharge() {...}\n}\n\nclass AccountType {\n// ...\n}\n\nAprès :\nclass Account {\n// ...\n}\n\nclass AccountType {\n// ...\nget overdraftCharge() {...}\n}\n\n\n\nÉtapes :\n\nOn examine le contexte de notre fonction : est-ce qu’on veut aussi déplacer d’autres fonctions que notre fonction appelle ?\n\n\nSi oui, l’auteur conseille de déplacer ces fonctions en premier parce qu’elles ont moins de dépendances.\nSi la sous-fonction est appelée par une seule fonction, on peut l’imbriquer, les bouger ensemble, puis la ressortir.\n\n\n\nOn vérifie si la fonction est une méthode polymorphe, auquel cas il faudra aussi déplacer les fonctions des parents.\n\n\n\nOn copie la fonction dans son nouveau contexte, en ajustant son nom si besoin.\n\n\n\nOn lance l’analyse statique.\n\n\n\nOn change la fonction initiale en une fonction de délégation, pour qu’elle ne fasse qu’appeler la nouvelle fonction.\n\n\n\nOn teste.\n\n\n\nOn envisage de supprimer l’ancienne fonction devenue fonction de délégation, en utilisant Inline Function.\n\n\n\n\nThéorie :\nLes fonctions sont en général dans un contexte d’encapsulation (module, classe etc.). A mesure qu’on gagne en connaissance, on réorganise les fonctions.\nLes critères pour placer la fonction peuvent être par exemple de la mettre proche des fonctions qu’elle appelle, ou des fonctions qui l’appellent.\nDans le cadre d’un refactoring, on peut souvent travailler avec les fonctions dans un même contexte, et les regrouper dans d’autres contextes ensuite.\nL’auteur est plutôt méfiant à l’égard des fonctions imbriquées parce qu’elles gardent des relations de données cachées. Il préfère les modules ES6 pour encapsuler les fonctions (l’imbrication est souvent utilisée comme étape intermédiaire dans un refactoring).","move-field#Move Field":"Exemple :\nAvant :\nclass Customer {\nget plan() {\nreturn this._plan;\n}\nget discountRate() {\nreturn this._discountRate;\n}\n}\n\nAprès :\nclass Customer {\nget plan() {\nreturn this._plan;\n}\nget discountRate() {\nreturn this.plan.discountRate;\n}\n}\n\n\n\nÉtapes :\n\nOn s’assure que notre champ initial est encapsulé, sinon on fait.\n\n\nPar exemple, on auto-encapsule le champ dans sa propre classe en y accédant à travers un getter et setter privés.\n\n\n\nOn teste.\n\n\n\nOn crée le champ et des getter/setter dans la classe que le champ veut rejoindre.\n\n\n\nOn lance les vérifications statiques.\n\n\n\nOn s’assure qu’il y a une référence de la classe initiale à la classe qui accueille le nouveau champ, pour pouvoir le manipuler depuis l’ancienne classe.\n\n\nÇa peut être la nouvelle classe dont l’instance fait partie d’un champ de l’ancienne, ou une fonction dans l’ancienne permet d’y accéder. Au pire il faudra créer un champ dans l’ancienne classe, pour y mettre une instance de la nouvelle.\n\n\n\nOn ajuste les getter/setter de la classe initiale pour qu’ils lisent et écrivent dans le champ de la nouvelle classe.\n\n\nSi on est dans une situation complexe où le nouveau champ va être mis à jour par plusieurs classes, on peut choisir de mettre à jour l’ancien champ et le nouveau champ en vérifiant la cohérence des deux valeurs avec Introduce Assertion, puis en arrêtant d’écrire dans l’ancien champ quand on pense que c’est bon.\n\n\n\nOn teste.\n\n\n\nOn supprime le champ initial.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nLe design des structures de données est très important pour avoir un code clair.\nPlus notre connaissance du domaine est importante, mieux on va les concevoir. Donc on va être amenés à les modifier au fur et à mesure.\nDeux champs dans deux structures qui sont toujours lues ensemble, ou mises à jour ensemble, doivent rejoindre la même structure.\nCe refactoring est plus facile à faire si notre structure est en fait une classe, avec ses champs encapsulés, et qu’on peut donc ajouter de la logique aux getters/setters.\nSi la structure n’est pas une classe, l’auteur conseille de la transformer en une classe avec Encapsulate Record avant de faire le refactoring.","move-statement-into-function#Move Statement into Function":"Exemple :\nAvant :\nresult.push(`<p>title: ${person.photo.title}</p>`);\nresult.concat(photoData(person.photo));\n\nfunction photoData(aPhoto) {\nreturn [\n`<p>location: ${aPhoto.location}</p>`,\n`<p>date: ${aPhoto.date.toDateString()}</p>`,\n];\n}\n\nAprès :\nresult.concat(photoData(person.photo));\n\nfunction photoData(aPhoto) {\nreturn [\n`<p>title: ${aPhoto.title}</p>`,\n`<p>location: ${aPhoto.location}</p>`,\n`<p>date: ${aPhoto.date.toDateString()}</p>`,\n];\n}\n\n\n\nÉtapes :\n\nSi l’instruction exécutée en même temps que la fonction n’est pas à côté d’elle, on déplace l’instruction à côté avec Slide Statements.\n\n\n\nSi on n’avait en fait qu’une seule occurrence des instructions utilisées avec l’appel à la fonction, alors on peut simplement couper l’instruction et la coller dans la fonction, en ignorant le reste des étapes.\n\n\n\nSi on a plusieurs occurrences, on utilise Extract Function sur une des occurrences, pour extraire les instructions et l’appel à la fonction.\n\n\nOn nomme la nouvelle fonction avec un nom temporaire facile à rechercher.\n\n\n\nOn utilise la nouvelle fonction dans toutes les occurrences où on peut, en testant à chaque fois.\n\n\n\nOn utilise Inline Function pour mettre le contenu de la fonction initiale dans la nouvelle fonction.\n\n\n\nOn utilise Rename Function pour trouver un meilleur nom à la nouvelle fonction.\n\n\n\n\nThéorie :\nQuand on remarque que des instructions sont tout le temps exécutées en même temps que l’appel à une fonction, il faut se demander si elles poursuivent le même but que la fonction :\nSi oui il faut les déplacer dans la fonction avec cette technique.\nSi non il faut extraire les instructions et la fonction dans une nouvelle fonction qui fera quelque chose de spécifique et d’utile puisque souvent utilisée.\n\n\nVu les étapes, on peut parfois extraire les instructions et la fonction dans une nouvelle fonction, puis se dire qu’on veut aller plus loin et intégrer les instructions dans la fonction.","move-statement-to-callers#Move Statement to Callers":"Exemple :\nAvant :\nemitPhotoData(outStream, person.photo);\n\nfunction emitPhotoData(outStream, photo) {\noutStream.write(`<p>title: ${photo.title}</p>\\n`);\noutStream.write(`<p>location: ${photo.location}</p>\\n`);\n}\n\nAprès :\nemitPhotoData(outStream, person.photo);\noutStream.write(`<p>location: ${person.photo.location}</p>\\n`);\n\nfunction emitPhotoData(outStream, photo) {\noutStream.write(`<p>title: ${photo.title}</p>\\n`);\n}\n\n\n\nÉtapes :\n\nSi on est dans un cas simple avec un ou deux endroits où notre fonction est appelée, on peut simplement couper le code à sortir de la fonction, et le coller là où on l’appelle.\n\n\n\nSi on est dans un cas plus complexe, on va d’abord utiliser Extract Function pour toutes les instructions qu’on ne va pas déplacer de notre fonction, pour les mettre dans une fonction temporaire.\n\n\n\nOn applique ensuite Inline Function sur la fonction initiale pour la faire disparaître.\n\n\n\nOn utilise enfin Change Function Declaration pour renommer la fonction qu’on avait extraite avec le nom de la fonction qui vient de disparaître (ou un meilleur nom).\n\n\n\n\nThéorie :\nOn peut utiliser ce refactoring quand une fonction fait trop de choses, notamment quand on commence à vouloir la personnaliser pour répondre à des besoins particuliers où il faut éviter d’exécuter une partie de la fonction.\nCe refactoring marche bien si on a peu de changements à déplacer, sinon il faut d’abord tout remettre à plat avec Inline Function, et extraire de meilleures fonctions à partir de là.","replace-inline-code-with-function-call#Replace Inline Code with Function Call":"Exemple :\nAvant :\nlet appliesToMass = false;\nfor (const s of states) {\nif (s === \"MA\") appliesToMass = true;\n}\n\nAprès :\nappliesToMass = states.includes(\"MA\");\n\n\n\nÉtapes :\n\nOn remplace du code existant par un appel à une fonction qui fait déjà la même chose.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nCe refactoring consiste tout simplement à voir si on n’a pas déjà une fonction (maison ou dans la bibliothèque) qui fait déjà ce que fait un bout de code qu’on a, et si oui à l’appeler.\nDans le cas où le code est similaire au code d’une fonction, mais que cette similitude est une coïncidence, il ne faut pas faire le remplacement puisque ces deux codes ne doivent alors pas évoluer ensemble.\nLe nom de la fonction peut nous permettre de comprendre ce qu’elle est censée faire, pour savoir si c’est bien la même chose qu’on veut faire avec notre code.","slide-statements#Slide Statements":"Exemple :\nAvant :\nconst pricingPlan = retrievePricingPlan();\nconst order = retreiveOrder();\nlet charge;\nconst chargePerUnit = pricingPlan.unit;\n\nAprès :\nconst pricingPlan = retrievePricingPlan();\nconst chargePerUnit = pricingPlan.unit;\nconst order = retreiveOrder();\nlet charge;\n\n\n\nÉtapes :\n\nOn examine le code pour voir si les instructions qu’on veut déplacer vont créer des interférences avec du code avant ou après. S’il y a des interférences on abandonne le refactoring.\n\n\nOn ne peut pas déplacer une variable avant un élément qu’elle référence.\nOn ne peut pas déplacer une variable après un élément qui la référence.\nUne variable ne peut pas aller après une instruction qui modifie un élément qu’elle référence.\nUne variable qui modifie un élément ne peut pas aller après une instruction qui référence l'élément modifié.\n\n\n\nOn coupe les instructions et on les colle là où on veut qu’elles soient.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nOn va souvent vouloir regrouper les instructions qui agissent sur la même structure.\nL’auteur déclare en général les variables juste au-dessus de leur première utilisation (et non pas en haut de la fonction).\n\n\nEn général on regroupe les instructions ensemble pour ensuite faire un autre refactoring, par exemple Extract Function.\nSi après le refactoring nos tests ne passent plus, on peut recommencer avec moins d’instructions déplacées. On peut aussi laisser de côté pour le moment pour faire d’abord d’autres refactorings.\nLe fait qu’une valeur soit modifiée par l’instruction qu’on déplace et lue par l’instruction par dessus laquelle on déplace (ou l'inverse) n’est en fait pas forcément éliminatoire.\nIl peut y avoir des cas où les modifications sont interchangeables, mais il faut être très prudent. Appliquer Split Variable peut parfois clarifier la situation.\n\n\nL’auteur adhère au principe de séparation de commandes et queries, et donc ses fonctions vont soit retourner une valeur, soit avoir un side effect, mais pas les deux.","split-loop#Split Loop":"Exemple :\nAvant :\nlet averageAge = 0;\nlet totalSalary = 0;\nfor (const p of people) {\naverageAge += p.age;\ntotalSalary += p.salary;\n}\naverageAge = averageAge / people.length;\n\nAprès :\nlet totalSalary = 0;\nfor (const p of people) {\ntotalSalary += p.salary;\n}\nlet averageAge = 0;\nfor (const p of people) {\naverageAge += p.age;\n}\naverageAge = averageAge / people.length;\n\n\n\nÉtapes :\n\nOn copie la boucle en dessous de l’autre.\n\n\n\nOn supprime ce qu’il faut dans chacune deux boucles, et notamment les side-effects dupliqués auxquels il faut faire attention.\n\n\n\nOn teste.\n\n\n\nOn envisage Extract Function pour mettre la nouvelle boucle dans une fonction.\n\n\n\n\nThéorie :\nOn utilise souvent une seule boucle pour faire plusieurs choses parce qu’on a peur des performances.\nPourtant séparer la boucle en plusieurs boucles permet d’avoir du code plus compréhensible, et c’est avec du code compréhensible qu’on pourra au mieux optimiser si vraiment on a besoin.\n\n\nEn ayant des boucles qui font une seule chose, on arrive aussi à obtenir des fonctions qui exécutent la boucle et renvoient une seule valeur.\nEn général l’auteur fractionne une boucle avant d’extraire la fraction en boucle.","replace-loop-with-pipeline#Replace Loop with Pipeline":"Exemple :\nAvant :\nconst names = [];\nfor (const i of input) {\nif (i.job === \"programmer\") names.push(i.name);\n}\n\nAprès :\nconst names = input\n.filter((i) => i.job === \"programmer\")\n.map((i) => i.name);\n\n\n\nÉtapes :\n\nOn crée une nouvelle variable à laquelle on assigne la collection sur laquelle on boucle.\n\n\n\nOn prend chaque groupe d’instructions cohérentes de la boucle en commençant par le début, et on en crée une opération de pipeline.\n\n\nOn teste à chaque fois.\n\n\n\nUne fois que toutes les instructions de la boucle sont supprimées, on supprime la boucle.\n\n\n\n\nThéorie :\nLes pipelines permettent de suivre les opérations de manière linéaire, avec les entrées et sorties claires. L’auteur les trouve plus claires que les boucles.\nParmi les opérations de pipeline on a par exemple les classiques map, filter, reduce.\nPour plus d’exemples, Fowler propose son essai Refactoring with Loops and Collection Pipelines.","remove-dead-code#Remove Dead Code":"Exemple :\nAvant :\nif (false) {\ndoSomethingThatUsedToMatter();\n}\n\nAprès :\n\n\n\n\nÉtapes :\n\nSi c’est du code appelable, on recherche les références pour bien s’assurer que le code est mort.\n\n\n\nOn supprime le code.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nLe code mort alourdit le code pour rien. Il faut le supprimer pour des raisons de maintenabilité.\nLe gestionnaire de version retient le code de toute façon. Et au pire, si on veut vraiment garder une trace visible, on peut mettre une trace sous forme de commentaire disant dans quel commit se trouve le bout de code qu’on a supprimé.","9---organisation-des-données#9 - Organisation des données":"","split-variable#Split Variable":"Exemple :\nAvant :\nlet temp = 2 * (height + width);\nconsole.log(temp);\ntemp = height * width;\nconsole.log(temp);\n\nAprès :\nconst perimeter = 2 * (height + width);\nconsole.log(perimeter);\nconst area = height * width;\nconsole.log(area);\n\n\n\nÉtapes :\n\nOn renomme la variable par ce qui exprime le mieux sa première affectation.\n\n\nSi les affectations suivantes sont sous la forme variable += variable + valeur, on est sans doute face à une variable de collecte, il faut donc abandonner le refactoring.\n\n\n\nSi possible on déclare la variable comme immutable (const).\n\n\n\nOn change toutes les références à la variable jusqu'à sa prochaine affectation.\n\n\n\nOn teste.\n\n\n\nOn répète l’ensemble des étapes à chaque affectation, jusqu’à la dernière, en testant à chaque fois.\n\n\nA la fin on a bien des variables différentes pour chaque affectation.\n\n\n\n\nThéorie :\nCe refactoring permet d’éliminer les variables qui sont assignées plusieurs fois et qui ont plusieurs responsabilités, pour en faire une variable par responsabilité.\nLes variables qui comptant les tours de boucle, font des sommes ou des concaténations de chaîne sont réassignées de nombreuses fois, mais elles ont bien une seule responsabilité. Ce sont des variables de collecte.","rename-field#Rename Field":"Exemple :\nAvant :\nclass Organization {\nget name() {...}\n}\n\nAprès :\nclass Organization {\nget title() {...}\n}\n\n\n\nÉtapes :\n\nDans le cas où la structure a une petite portée, il suffit de renommer les références au champ qu’on renomme directement. Pas besoin d’aller plus loin dans les étapes.\n\n\n\nDans le cas contraire, si on est face à une structure non encapsulée dans une classe, on appliqué Encapsulate Record pour l’encapsuler.\n\n\n\nOn renomme le champ privé dans la classe, et on met à jour le getter et le setter pour que ça fonctionne.\n\n\n\nOn teste.\n\n\n\nOn applique Change Function Declaration pour modifier le nom du getter et du setter.\n\n\n\n\nThéorie :\nL’intérêt de ce refactoring est de faire en sorte que les structures de données restent en cohérence avec la connaissance nouvelle qu’on gagne à mesure qu’on travaille sur le projet.\nOn parle ici des structures de données simples mais aussi des classes.\nQuand on encapsule une structure de données dans une classe d’abord, on doit ensuite renommer le getter, le setter, le constructeur et la variable privée. Mais on se facilite en fait la vie parce qu’on traite chaque aspect indépendamment au lieu de tout faire d’un coup.\n\n\nExemple détaillé :\nOn a initialement une structure. On veut renommer name en title.\nconst organization = {\nname: \"Acme Gooseberries\",\n};\n\nOn va l’encapsuler dans une classe.\nclass Organization {\nconstructor(data) {\nthis._title = data.name;\n}\nget name() {\nreturn this._title;\n}\nset name(aString) {\nthis._title = aString;\n}\n}\n\nOn permet ensuite au constructeur d’accepter le nouveau nom du paramètre, pour pouvoir changer les références une par une en testant.\nclass Organization {\nconstructor(data) {\nthis._title = data.title !== undefined ? data.title : data.name;\n}\n// ...\n}\n\nUne fois qu’on a changé toutes les références, on peut définitivement renommer name en title.\nclass Organization {\nconstructor(data) {\nthis._title = data.title;\n}\nget title() {\nreturn this._title;\n}\nset title(aString) {\nthis._title = aString;\n}\n}","replace-derived-variable-with-query#Replace Derived Variable with Query":"Exemple :\nAvant :\nget discountedTotal() {\nreturn this._discountedTotal;\n}\nset discount(aNumber) {\nconst old = this._discount;\nthis._discount = aNumber;\nthis._discountedTotal += old - aNumber;\n}\n\nAprès :\nget discountedTotal() {\nreturn this._baseTotal - this._discount;\n}\nset discount(aNumber) {\nthis._discount = aNumber;\n}\n\n\n\nÉtapes :\n\nOn identifie la variable qu’on veut remplacer par un calcul, et on liste les endroits où elle est mise à jour.\n\n\nSI besoin on peut utiliser Split Variable pour la séparer en plusieurs variables avec une responsabilité chacune.\n\n\n\nOn crée une fonction qui calcule la valeur de la variable.\n\n\n\nOn utilise Introduce Assertion pour vérifier que la variable et la fonction fournissent la même valeur.\n\n\nSi besoin, on peut utiliser Encapsulate Variable pour avoir un endroit où mettre l’assertion.\n\n\n\nOn teste.\n\n\n\nOn remplace les accès à la variable par un appel à la fonction.\n\n\n\nOn teste.\n\n\n\nOn utilise Remove Dead Code pour éliminer la variable et le code qui la met à jour.\n\n\n\n\nThéorie :\nLes variables mutables sont une source de problèmes. Ce refactoring permet de les limiter, en remplaçant certaines variables par des calculs qui permettent de les réobtenir.\nDans le cas où les données à partir desquels la variable est calculée sont immutables, c’est OK de la laisser et de la rendre immutable aussi.\nEt si la donnée dérivée est temporaire, ça peut être OK aussi de la laisser dans une variable.\n\n\n\n\nExemple détaillé :\nOn a une classe avec une duplication de structure de données : production est calculable depuis adjustment, mais on stocke les deux valeurs en tant que variable membre.\nclass ProductionPlan {\n// ...\nget production() {\nreturn this._production;\n}\napplyAdjustment(anAdjustment) {\nthis._adjustments.push(anAdjustment);\nthis._production += anAdjustment.amount;\n}\n}\n\nOn crée une fonction qui calcule la valeur de production, et on vérifie avec un assert qu’elle renvoie la même valeur que production.\nclass ProductionPlan {\n// ...\nget production() {\nassert(this._production === this.calculatedProduction);\nreturn this._production;\n}\nget calculatedProduction() {\nreturn this._adjustments.reduce((sum, a) => sum + a.amount, 0);\n}\n}\n\nSi ça marche bien, on peut enlever l’assert, puis placer le contenu de la fonction calculatedProduction dans le getter de production.\nclass ProductionPlan {\n// ...\nget production() {\nreturn this._adjustments.reduce((sum, a) => sum + a.amount, 0);\n}\n}\n\nEt enfin on peut éliminer le code mort représenté par la variable privée this._production.","change-reference-to-value#Change Reference to Value":"Exemple :\nAvant :\nclass Product {\napplyDiscount(arg) {\nthis._price.amount -= arg;\n}\n}\n\nAprès :\nclass Product {\napplyDiscount(arg) {\nthis._price = new Money(this._price.amount - arg, this._price.currency);\n}\n}\n\n\n\nÉtapes :\n\nOn vérifie que l’objet qu’on veut transformer en valeur est déjà, ou peut devenir immutable.\n\n\n\nOn va effectivement le rendre immutable : pour chaque setter qu’on appelle pour modifier l’objet, on applique Remove Setting Method. De cette manière l’objet ne pourra plus être changé autrement qu’à sa construction.\n\n\n\nOn lui ajoute une méthode de comparaison d’égalité, basée sur la valeur des propriétés de notre objet.\n\n\n\n\nThéorie :\nUne instance d'objet qui est une propriété d'un autre objet peut être traitée soit comme une référence, soit comme une valeur.\nSi elle est une référence, on garde la même et on modifie des choses dessus si besoin.\nSi elle est une valeur, on la recrée avec les bonnes propriétés à chaque fois qu'elle est modifiée. Elle est alors un value object.\n\n\nL'avantage à manipuler des value objects est que ce sont des valeurs immutables, et donc on n'a pas à s'inquiéter qu'elles soient modifiées sans qu'on le sache (sans passer par notre setter).\nLes value objects sont particulièrement utiles pour les systèmes distribués et concurrents.\n\n\nLa plupart des langages permettent de surcharger l’opérateur ==, et en général on doit aussi surcharger l’opérateur de hachage pour que la valeur puisse servir de clé dans une hashmap.\nDans le cas où on ne peut pas surcharger l'opérateur d’égalité, on peut toujours créer une méthode equals().","change-value-to-reference#Change Value to Reference":"Exemple :\nAvant :\nlet customer = new Customer(customerData);\n\nAprès :\nlet customer = customerRepository.get(customerData.id);\n\n\n\nÉtapes :\n\nOn crée un repository pour les instances de l’objet concerné (si ça n’existe pas déjà).\n\n\n\nOn s’assure que le constructeur des objets qui instancient notre objet concerné a la possibilité de rechercher les bonnes instances de l’objet concerné.\n\n\n\nOn change le constructeur des objets qui instancient notre objet concerné, pour qu’ils utilisent le repository pour obtenir la bonne instance au lieu de construire l’objet directement.\n\n\n\n\nThéorie :\nQuand il faut mettre à jour des données partagées, si elles sont immutables, il faut toutes les trouver et les changer. Alors que quand on a une référence, on peut la changer une fois pour la voir changée partout.\n\n\nExemple détaillé :\nOn a une classe Order qui a une instance d’un objet Customer qu’on veut transformer en référence pour que les customers qui ont le même ID soient partagés :\nclass Order {\nconstructor(data) {\nthis._number = data.number;\nthis._customer = new Customer(data.customer);\n}\n}\n\nclass Customer {\nconstructor(id) {\nthis._id = id;\n}\n}\n\nOn va créer un repository qui nous permet de créer un customer d’il n’existe pas déjà, ou d’obtenir le customer existant qui a le même ID.\nlet _repositoryData;\n\nexport function initialize() {\n_repositoryData = {};\n_repositoryData.customers = new Map();\n}\n\nexport function registerCustomer(id) {\nif (!_repositoryData.customers.has(id))\n_repositoryData.customers.set(id, new Customer(id));\nreturn findCustomer(id);\n}\n\nexport function findCustomer(id) {\nreturn _repositoryData.customers.get(id);\n}\n\nOn peut maintenant utiliser le repository dans Order pour obtenir la bonne instance de Customer.\nclass Order {\nconstructor(data) {\nthis._number = data.number;\nthis._customer = registerCustomer(data.customer);\n}\n}","10---simplification-de-la-logique-conditionnelle#10 - Simplification de la logique conditionnelle":"","decompose-conditional#Decompose Conditional":"Exemple :\nAvant :\nif (!aDate.isBefore(plan.summerStart) && !aDate.isAfter(plan.summerEnd))\ncharge = quantity * plan.summerRate;\nelse charge = quantity * plan.regularRate + plan.regularServiceCharge;\n\nAprès :\nif (summer()) charge = summerCharge();\nelse charge = regularCharge();\n\n\n\nÉtapes :\n\nOn applique Extract Function sur la condition, et on l’applique sur le contenu de chaque branche de manière à obtenir un code tout petit.\n\n\n\n\nThéorie :\nLe but est de rendre lisible les conditions.","consolidate-conditional-expression#Consolidate Conditional Expression":"Exemple :\nAvant :\nif (anEmployee.seniority < 2) return 0;\nif (anEmployee.monthsDisabled > 12) return 0;\nif (anEmployee.isPartTime) return 0;\n\nAprès :\nif (isNotEligableForDisability()) return 0;\n\nfunction isNotEligableForDisability() {\nreturn (\nanEmployee.seniority < 2 ||\nanEmployee.monthsDisabled > 12 ||\nanEmployee.isPartTime\n);\n}\n\n\n\nÉtapes :\n\nOn s’assure qu’aucune des conditions (contenu des parenthèses du if) n’a de side effects.\n\n\nSi il y en a, on les sépare avec Separate Query from Modifier.\n\n\n\nOn prend deux conditions et on les combine dans une nouvelle condition avec des opérateurs logiques.\n\n\nDes suites de conditions se combinent avec un OR, des conditions imbriquées se combinent avec un AND.\n\n\n\nOn répète jusqu’à ce qu’il ne reste qu’une condition, en testant à chaque fois.\n\n\n\nOn envisage Extract Function sur la condition résultante.\n\n\n\n\nThéorie :\nIl s’agit ici du cas où les conditions sont différentes mais le contenu des branches est le même.\nRegrouper les conditions permet d’indiquer l’intention : le fait que ces conditions permettent en fait de protéger la même branche de code.\nSi le contenu des branches est le même par hasard, alors on ne fait pas ce refactoring parce qu’on pourrait induire en erreur en indiquant une mauvaise intention.","replace-nested-conditional-with-guard-clauses#Replace Nested Conditional with Guard Clauses":"Exemple :\nAvant :\nfunction getPayAmount() {\nlet result;\nif (isDead) result = deadAmount();\nelse {\nif (isSeparated) result = separatedAmount();\nelse {\nif (isRetired) result = retiredAmount();\nelse result = normalPayAmount();\n}\n}\nreturn result;\n}\n\nAprès :\nfunction getPayAmount() {\nif (isDead) return deadAmount();\nif (isSeparated) return separatedAmount();\nif (isRetired) return retiredAmount();\nreturn normalPayAmount();\n}\n\n\n\nÉtapes :\n\nOn prend la branche la plus externe de la condition et on la transforme en clause de garde.\n\n\n\nOn teste.\n\n\n\nOn répète si nécessaire.\n\n\n\n\nThéorie :\nL’idée est de laisser le if/else dans le cas où les deux conditions font partie du flow normal de la fonction, et de mettre la branche if comme clause de garde si le else serait tout le reste de la fonction.\nLe but principal est de mettre en avant la nature de la condition.\n\n\nCe refactoring est souvent utilisé avec une inversion de la condition :\nfunction adjustedCapital(anInstrument) {\nlet result = 0;\nif (anInstrument.capital > 0) {\nif (anInstrument.interestRate > 0 && anInstrument.duration > 0) {\nresult =\n(anInstrument.income / anInstrument.duration) *\nanInstrument.adjustmentFactor;\n}\n}\nreturn result;\n}\n\nDonne :\nfunction adjustedCapital(anInstrument) {\nif (\nanInstrument.capital <= 0 ||\nanInstrument.interestRate <= 0 ||\nanInstrument.duration <= 0\n)\nreturn 0;\nreturn (\n(anInstrument.income / anInstrument.duration) *\nanInstrument.adjustmentFactor\n);\n}","replace-conditional-with-polymorphism#Replace Conditional with Polymorphism":"Exemple :\nAvant :\nswitch (bird.type) {\ncase 'EuropeanSwallow':\nreturn \"average\";\ncase 'AfricanSwallow':\nreturn (bird.numberOfCoconuts > 2) ? \"tired\" : \"average\";\ncase 'NorwegianBlueParrot':\nreturn (bird.voltage > 100) ? \"scorched\" : \"beautiful\";\ndefault:\nreturn \"unknown\";\n\nAprès :\nclass EuropeanSwallow {\nget plumage() {\nreturn \"average\";\n}\n}\n\nclass AfricanSwallow {\nget plumage() {\nreturn this.numberOfCoconuts > 2 ? \"tired\" : \"average\";\n}\n}\n\nclass NorwegianBlueParrot {\nget plumage() {\nreturn this.voltage > 100 ? \"scorched\" : \"beautiful\";\n}\n}\n\n\n\nÉtapes :\n\nOn crée des classes pour le comportement conditionnel, et on crée une fonction factory qui permet de renvoyer une instance de la bonne classe.\n\n\n\nSi la logique conditionnelle n’est pas déjà dans une fonction, on utilise Extract Function pour qu’elle le soit.\n\n\n\nOn déplace la fonction avec la logique conditionnelle dans la classe mère de la hiérarchie de classes qu’on a créée à l’étape 1.\n\n\nCa peut être plusieurs fonctions, si on a plusieurs logiques conditionnelles similaires (comme par exemple plusieurs switchs similaires).\n\n\n\nOn crée une méthode qui surcharge la méthode avec la logique conditionnelle dans une des classes filles, et on y déplace le corps de la bonne branche de l’instruction conditionnelle.\n\n\n\nOn répète pour chaque branche conditionnelle et classe fille.\n\n\n\nOn laisse un cas par défaut dans la classe mère, ou alors si elle est abstraite on rend la méthode abstraite (ou on fait en sorte qu’elle lève une erreur).\n\n\n\n\nThéorie :\nA partir du moment où on a plusieurs fois le même switch pour faire des choses, ça devient plus avantageux d’utiliser une hiérarchie de classes pour remplacer ces switch.\nUne situation aussi où il est pertinent d’utiliser le polymorphisme c’est quand on a un cas de base, et des variantes secondaires qui vont venir apporter des changements au cas de base.\nLa classe mère pourra avoir une grande partie de la logique non redéfinie dans les classes filles, et seules certaines méthodes surchargées vont venir apporter des changements secondaires dans les classes filles.","introduce-special-case#Introduce Special Case":"Exemple :\nAvant :\nif (aCustomer === \"unknown\") {\ncustomerName = \"occupant\";\n}\n\nAprès :\ncustomerName = aCustomer.name;\n\nclass UnknownCustomer {\nget name() {\nreturn \"occupant\";\n}\n}\n\n\n\nÉtapes :\nCette description est cryptique sans regarder l’exemple détaillé plus bas.\nOn démarre avec un conteneur (classe ou structure) qui a une propriété (qu’on va appeler sujet) que le code appelant compare avec une valeur, et dont on aimerait que la valeur soit dans une classe special case.\n\nOn ajoute une propriété vérifiant le fait d’être le spatial case ou pas (et retournant false) au sujet.\n\n\n\nOn crée la classe special case avec la même propriété vérifiant le fait d’être spatial case ou pas, mais celle-là retourne true.\n\n\n\nOn applique Extract Function pour extraire la condition (contenu des parenthèses du if) des occurrences de code appelant, dans une fonction qui vérifiera si notre objet est un special cas ou non.\n\n\n\nOn introduit les instances de special case depuis le conteneur initial, quand le sujet est un special case.\n\n\n\nOn met à jour la fonction qui vérifie si l’objet est special case ou pas, pour qu’elle prenne enfin en compte la classe special case qu’on a créée.\n\n\n\nOn teste.\n\n\n\nOn utilise Combine Function into Class ou Combien Function into Transform pour déplacer toutes les valeurs et comportements liés aux conditions de special case dans l’objet special case.\n\n\n\nOn utilise Inline Function sur la fonction qui vérifie si l’objet est special case, pour les occurrences de code appelant où on en a besoin parce qu’on ne peut pas simplement utiliser l’objet special case (parce qu’on est sur du cas particulier de cas particulier).\n\n\nS’il n’y a pas de tels endroits, on peut supprimer la fonction qui vérifie si l’objet est special case.\n\n\n\n\nThéorie :\nQuand on a des conditions dupliquées qui donnent lieu aux mêmes instructions, ça peut être utile de créer un objet special case pour les regrouper au même endroit.\nL’objet special case peut retourner des valeurs spécifiques, ou contenir des méthodes avec de la logique à mettre en commun entre les conditions où il est utilisé.\nnull est un exemple d’objet de special case.\nL’objet de special case est un value object.\n\n\nExemple détaillé :\nOn a un Site avec un une propriété Customer.\nclass Site {\nget customer() {\nreturn this._customer;\n}\n}\n\nclass Customer {\nget name() {\n// ...\n}\n}\n\nLa plupart du temps un Site a un Customer, mais parfois il n’en a pas, et dans ce cas la propriété customer vaut “unknown”.\n// exemple code appelant\nconst aCustomer = site.customer;\nlet customerName;\nif (aCustomer === \"unknown\") {\ncustomerName = \"occupant\";\n}\n\nVu qu’on a de nombreux cas de code appelant qui traite le cas particulier de customer non existant, et que la plupart du temps le nom du customer va être “occupant”, on va mettre ça dans un objet special case.\nOn commence par ajouter une méthode à Customer pour indiquer qu’il n’est pas unknown.\nclass Customer {\nget isUnknown() {\nreturn false;\n}\n}\n\nPuis on crée notre objet special case qui lui va indiquer avec la même méthode qu’il est unknown.\nclass UnknownCustomer {\nget isUnknown() {\nreturn true;\n}\n}\n\nVu qu’on a beaucoup d’occurrences de code appelant qui traite le cas unknown, on a envie de pouvoir les changer petit à petit en obtenant à chaque fois un code qui marche et fait passer les tests. Donc on ajoute une fonction isUnknown qui sera utilisée par le code appelant.\nfunction isUnknown(arg) {\nif (!(arg instanceof Customer || arg === \"unknown\")) {\nthrow new Error(`Investigate bad value &lt;${arg}>`);\n}\nreturn arg === \"unknown\";\n}\n\n// exemple code appelant\nconst aCustomer = site.customer;\nlet customerName;\nif (isUnknown(aCustomer)) {\ncustomerName = \"occupant\";\n}\n\nUne fois qu’on a utilisé isUnknown dans tout le code appelant, on peut modifier Site pour renvoyer l’objet special case au lieu de la chaîne unknown dans le getter de customer.\nclass Site {\nget customer() {\nreturn this._customer === \"unknown\"\n? new UnknownCustomer()\n: this._customer;\n}\n}\n\nOn change aussi isUnknown pour prendre ça en compte et on teste.\nfunction isUnknown(arg) {\nif (!(arg instanceof Customer || arg instanceof UnknownCustomer)) {\nthrow new Error(`Investigate bad value &lt;${arg}>`);\n}\nreturn arg === \"unknown\";\n}\n\nOn ajoute une méthode au special case pour pouvoir éliminer la condition du code appelant.\nclass UnknownCustomer {\nget name() {\nreturn \"occupant\";\n}\n}\n\n// exemple code appelant\nconst aCustomer = site.customer;\nconst customerName = aCustomer.name;\n\nFinalement, une fois qu’on a remplacé partout, on supprime isUnknown que plus personne n’utilise. Elle nous a servi seulement pour le refactoring.","introduce-assertion#Introduce Assertion":"Exemple :\nAvant :\nif (this.discountRate) base = base - this.discountRate * base;\n\nAprès :\nassert(this.discountRate >= 0);\nif (this.discountRate) base = base - this.discountRate * base;\n\n\n\nÉtapes :\n\nQuand on voit dans le code qu’une condition doit toujours être vraie, on ajoute une assertion pour l’indiquer.\n\n\nLes assertions ne doivent pas modifier le comportement du système, elles l'arrêtent juste dans des cas qui ne doivent de toute façon pas se produire.\n\n\n\n\nThéorie :\nQuand on a des conditions qui ne sont jamais censées se produire, on peut placer une assertion qui arrête le programme au cas où elle n’est pas vérifiée.\nC’est utile pour trouver des bugs, mais aussi pour communiquer l’information sur l’état que le code n’est pas censé avoir au lecteur du code.\nAttention à ne pas en abuser, il faut vérifier ce qui doit être vrai, pas tout ce qu’on pense être vrai.\nPar exemple, si on lit des données externes qu’on doit parser, il ne faut pas utiliser des assertions sur elles parce qu’il y a des chances pour qu’elles ne soient pas dans le bon format sans que ce soit une erreur dans notre programme.","11---refactoring-des-apis#11 - Refactoring des APIs":"","separate-query-from-modifier#Separate Query from Modifier":"Exemple :\nAvant :\nfunction getTotalOutstandingAndSendBill() {\nconst result = customer.invoices.reduce(\n(total, each) => each.amount + total,\n0\n);\nsendBill();\nreturn result;\n}\n\nAprès :\nfunction totalOutstanding() {\nreturn customer.invoices.reduce((total, each) => each.amount + total, 0);\n}\n\nfunction sendBill() {\nemailGateway.send(formatBill(customer));\n}\n\n\n\nÉtapes :\n\nOn copie la fonction et on la renomme en tant que query.\n\n\nPour le choix du nom, on peut voir où la fonction est utilisée, et ce qui est fait de sa valeur de retour (par exemple le nom de la variable dans laquelle on la met).\n\n\n\nOn supprime les side effects de la nouvelle fonction.\n\n\n\nOn lance la vérification statique du code.\n\n\n\nPour chaque appel à la fonction initiale, si l’appel utilise la valeur de retour, on le remplace par un appel à la nouvelle fonction.\n\n\nOn teste à chaque fois.\n\n\n\nOn supprime la valeur de retour de la fonction initiale.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nFowler essaye de séparer les fonctions qui ont des side effects mais ne renvoient pas de résultat (modifier) des fonctions qui n’en ont pas et qui renvoient un résultat (query).\nC’est pas une règle absolue, mais ça permet d’être plus serein sur le code des queries.\n\n\nS’il y a beaucoup de duplication entre la fonction query et la fonction modifier, on peut voir s’il y a moyen d’utiliser Substitute Algorithm pour utiliser la fonction query dans la fonction modifier, et la raccourcir.","parameterize-function#Parameterize Function":"Exemple :\nAvant :\nfunction tenPercentRaise(aPerson) {\naPerson.salary = aPerson.salary.multiply(1.1);\n}\n\nfunction fivePercentRaise(aPerson) {\naPerson.salary = aPerson.salary.multiply(1.05);\n}\n\nAprès :\nfunction raise(aPerson, factor) {\naPerson.salary = aPerson.salary.multiply(1 + factor);\n}\n\n\n\nÉtapes :\n\nOn choisit une des fonctions qui doit être paramétrisée.\n\n\n\nOn utilise Change Function Declaration pour ajouter des paramètres pour les valeurs en dur à paramétriser.\n\n\n\nOn ajoute les nouveaux paramètres pour chaque code qui appelle la fonction qu’on vient de modifier.\n\n\n\nOn teste.\n\n\n\nOn change le corps de la fonction pour utiliser les nouveaux paramètres.\n\n\nOn teste à chaque changement dans la fonction.\n\n\n\nOn remplace les fonctions similaires avec la nouvelle fonction, en ajustant si besoin.\n\n\nOn teste à chaque fois.\n\n\n\n\nThéorie :\nIl s’agit de regrouper plusieurs fonctions qui font presque la même chose avec une différence de valeur en dur, dans une fonction similaire avec la valeur donnée en paramètre.","remove-flag-argument#Remove Flag Argument":"Exemple :\nAvant :\nfunction setDimension(name, value) {\nif (name === \"height\") {\nthis._height = value;\nreturn;\n}\nif (name === \"width\") {\nthis._width = value;\nreturn;\n}\n}\n\nAprès :\nfunction setHeight(value) {\nthis._height = value;\n}\n\nfunction setWidth(value) {\nthis._width = value;\n}\n\n\n\nÉtapes :\n\nOn crée une fonction explicite pour chaque valeur possible du flag argument.\n\n\nOn peut utiliser Decompose Conditional pour faciliter la séparation de la logique conditionnelle.\n\n\n\nOn remplace chaque fonction qui utilisait l’ancienne fonction avec un flag argument par un appel à une des nouvelles fonctions.\n\n\n\n\nThéorie :\nOn a un flag argument dans une fonction quand un des paramètres permet de choisir la logique qu’on déclenche dans la fonction, par exemple avec un booléen, ou avec une enum.\nL’auteur n’aime pas les flag arguments (et encore plus les booléens) parce qu’ils rendent moins clair ce que font les fonctions.\nLes flag arguments peuvent être acceptables s’il y en a plusieurs dans la fonction, et que créer des fonctions avec chaque combinaison en ferait trop.\nMais dans ce cas, il faut essayer d’appliquer un autre refactoring parce que notre fonction est probablement trop complexe.","preserve-whole-object#Preserve Whole Object":"Exemple :\nAvant :\nconst low = aRoom.daysTempRange.low;\nconst high = aRoom.daysTempRange.high;\nif (aPlan.withinRange(low, high))\n\nAprès :\nif (aPlan.withinRange(aRoom.daysTempRange))\n\n\n\nÉtapes :\n\nOn crée une fonction avec un nom bidon, et qui prend les paramètres tels qu’on les veut, avec l’objet au lieu de ses valeurs.\n\n\n\nOn remplit le corps de la nouvelle fonction avec un appel à l’ancienne, en faisant un mapping des paramètres.\n\n\n\nOn lance les vérifications statiques.\n\n\n\nOn change le code appelant un par un pour qu’il utilise la nouvelle fonction au lieu de l’ancienne.\n\n\nOn peut au passage utiliser Remove Dead Code pour supprimer le code en trop qui devrait apparaître pendant le remplacement.\nOn teste à chaque fois.\n\n\n\nQuand tous les remplacements sont faits, on utilise Inline Function sur la fonction initiale pour que son contenu se retrouve dans la nouvelle et qu’elle disparaisse.\n\n\n\nOn change le nom de la nouvelle fonction.\n\n\n\n\nThéorie :\nQuand on a plusieurs valeurs issues d’un même objet ou structure, qui sont données en argument à une fonction, il faut penser à donner plutôt l’objet entier.\nSi on doit extraire des valeurs d’un objet pour en faire quelque chose, on peut aussi être face à un cas de feature envy où il vaut mieux utiliser Extract Class pour extraire les valeurs qui veulent sortir de la classe avec la logique associée utilisée à chaque fois dans la fonction qui prend ces valeurs.\nSi une classe passe plusieurs de ses variables membre à une autre classe ou fonction, on peut passer this à la place.\nQuand la fonction et l’objet se trouvent dans deux modules différents par contre, et qu’on veut garder de l’encapsulation entre ceux-ci, on peut ne pas vouloir appliquer ce refactoring.","replace-parameter-with-query#Replace Parameter with Query":"Exemple :\nAvant :\navailableVacation(anEmployee, anEmployee.grade);\n\nfunction availableVacation(anEmployee, grade) {\n// ...\n}\n\nAprès :\navailableVacation(anEmployee);\n\nfunction availableVacation(anEmployee) {\nconst grade = anEmployee.grade;\n// ...\n}\n\n\n\nÉtapes :\n\nSi besoin, on utilise Extract Function pour extraire la logique qui permet de calculer la valeur passée en paramètre qu’on veut enlever.\n\n\n\nOn remplace l’utilisation de chaque paramètre à enlever de la fonction par un appel qui permet d’obtenir la même valeur depuis le corps de la fonction.\n\n\nOn teste à chaque fois.\n\n\n\nOn utilise Change Function Declaration pour supprimer le paramètre qui n’est plus utilisé.\n\n\n\n\nThéorie :\nLa liste des paramètres d’une fonction devrait montrer les diverses manières dont on peut la faire varier.\nIl vaut mieux éviter les duplications dans les paramètres, et un paramètre que la fonction peut facilement obtenir dans son corps est une forme de duplication.\nDans le cas où elle ne peut pas facilement l’obtenir, il vaut peut être mieux ne pas faire le refactoring. Par exemple, s' il vaut mieux qu’elle ne sache pas la manière dont on l’obtient pour des raisons d'encapsulation.\n\n\nTypiquement, si on peut obtenir un paramètre à partir d’un autre, on est quasi sûr qu’il vaut mieux n’en passer qu’un des deux.","replace-query-with-parameter#Replace Query with Parameter":"Exemple :\nAvant :\ntargetTemperature(aPlan);\n\nfunction targetTemperature(aPlan) {\ncurrentTemperature = thermostat.currentTemperature;\n// ...\n}\n\nAprès :\ntargetTemperature(aPlan, thermostat.currentTemperature);\n\nfunction targetTemperature(aPlan, currentTemperature) {\n// ...\n}\n\n\n\nÉtapes :\n\nOn utilise Extract Variable sur le code qui fait l’appel à la référence extérieure, de manière à ce que cet appel soit isolé du reste du corps de la fonction.\n\n\n\nOn applique Extract Function avec une fonction ayant un nom temporaire, pour extraire la partie du corps de notre fonction qui ne fait pas l’appel à la référence extérieure.\n\n\n\nOn utilise Inline Variable pour se débarrasser de la variable qu’on avait créée à l’étape 1.\n\n\n\nOn utilise Inline Function pour fondre la fonction initiale dans la nouvelle fonction extraite à l’étape 2.\n\n\n\nOn change le nom de la fonction qu’on a créée à l’étape 2 pour lui donner le nom de la fonction initiale qui vient de disparaître à l’étape 4.\n\n\n\n\nThéorie :\nOn a en fait une opposition entre interface de fonction simple (avec peu de paramètres), et faible couplage entre le corps de la fonction et d’autres fonctions.\nLa décision d’avoir une query ou un parameter n’est parfois pas évidente, et il faut tester pour voir ce que ça donne.\n\n\nOn pourra appliquer ce refactoring par exemple pour éviter la dépendance à une variable globale, ou un élément qu’on veut déplacer.\nUne autre possibilité c’est dans le cas où la fonction n’a pas de referential transparency, c’est à dire qu’elle ne donne pas le même résultat à chaque appel : on pourra vouloir créer des fonctions pures d’un côté, et des fonctions avec side effect passant des paramètres de l’autre.","remove-setting-method#Remove Setting Method":"Exemple :\nAvant :\nclass Person {\nget name() {...}\nset name(aString) {...}\n}\n\nAprès :\nclass Person {\nget name() {...}\n}\n\n\n\nÉtapes :\n\nSi la valeur qu’on set n’est pas donnée au constructeur, on l’ajoute au constructeur avec Change Function Declaration, et on appelle le setter depuis le constructeur avec la valeur qu’on reçoit.\n\n\n\nOn remplace l’utilisation extérieure du setter par le passage de la valeur au constructeur un par un.\n\n\nOn teste à chaque fois.\n\n\n\nOn utilise Inline Function sur le setter pour le faire disparaître complètement.\n\n\nSi possible on rend la valeur membre immutable.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nOn veut supprimer un setter à chaque fois qu’on veut que le champ soit immutable de l’extérieur.","replace-constructor-with-factory-function#Replace Constructor with Factory Function":"Exemple :\nAvant :\nleadEngineer = new Employee(document.leadEngineer, \"E\");\n\nAprès :\nleadEngineer = createEngineer(document.leadEngineer);\n\n\n\nÉtapes :\n\nOn crée une fonction factory, avec un appel au constructeur dans son corps.\n\n\n\nOn remplace chaque appel au constructeur par un appel à la fonction factory.\n\n\nOn teste à chaque fois.\n\n\n\nSi possible, on limite la visibilité du constructeur.\n\n\n\n\nThéorie :\nLe but de ce refactoring est de répondre au cas où on n’a pas envie d’utiliser la fonction de constructeur directement :\nparce qu’il peut avoir des limitations comme renvoyer une instance spécifique.\nparce qu’on ne peut en général pas personnaliser son nom.\nparce qu’on ne peut pas le passer comme une simple fonction.","replace-function-with-command#Replace Function with Command":"Exemple :\nAvant :\nfunction score(candidate, medicalExam, scoringGuide) {\nlet result = 0;\nlet healthLevel = 0;\n// long body code\n}\n\nAprès :\nclass Scorer {\nconstructor(candidate, medicalExam, scoringGuide) {\nthis._candidate = candidate;\nthis._medicalExam = medicalExam;\nthis._scoringGuide = scoringGuide;\n}\nexecute() {\nthis._result = 0;\nthis._healthLevel = 0;\n// long body code\n}\n}\n\n\n\nÉtapes :\n\nOn crée une classe vide avec un nom basé sur celui de la fonction.\n\n\n\nOn utilise Move Function pour déplacer la fonction dans la classe.\n\n\nOn peut appeler la fonction execute ou call par exemple.\n\n\n\nOn envisage de créer un champ pour chaque argument de la fonction, en déplaçant ces arguments sur le constructeur.\n\n\n\n\nThéorie :\nIl s’agit d’encapsuler une fonction dans une classe, pour lui donner la possibilité d’avoir des méthodes associées, potentiellement de l’héritage etc.\n95 fois sur 100 l’auteur utilise une fonction normale non encapsulée.\nUne des raisons pour utiliser le refactoring est aussi de diviser une fonction complexe en plus petits morceaux : on peut notamment transformer ses variables locales en variables membres qui seraient utilisées dans toutes les fonctions de la classe.","replace-command-with-function#Replace Command with Function":"Exemple :\nAvant :\nclass ChargeCalculator {\nconstructor(customer, usage) {\nthis._customer = customer;\nthis._usage = usage;\n}\nexecute() {\nreturn this._customer.rate * this._usage;\n}\n}\n\nAprès :\nfunction charge(customer, usage) {\nreturn customer.rate * usage;\n}\n\n\n\nÉtapes :\n\nOn utilise Extract Function pour extraire la création de l’instance de commande et de l’appel à la fonction.\n\n\n\nPour chaque méthode appelée par la méthode principale, on utilise Inline Function pour l’éliminer.\n\n\n\nOn utilise Change Function Declaration pour ajouter les paramètres du constructeur à la méthode principale.\n\n\n\nPour chaque champ de l’objet, on remplace son utilisation par l’utilisation des paramètres dans la méthode.\n\n\nOn teste à chaque fois.\n\n\n\nOn inline la construction de l’objet et l’appel à la méthode dans le code appelant.\n\n\n\nOn teste.\n\n\n\nOn utilise Remove Dead Code pour éliminer la classe de commande.\n\n\n\n\nThéorie :\nL’objet de commande est puissant, mais il arrive aussi avec une certaine complexité : si la fonction est simple en général on n’en a pas besoin.","12---gestion-de-lhéritage#12 - Gestion de l’héritage":"","pull-up-method#Pull Up Method":"Exemple :\nAvant :\nclass Employee {...}\n\nclass Salesman extends Employee {\nget name() {...}\n}\n\nclass Engineer extends Employee {\nget name() {...}\n}\n\nAprès :\nclass Employee {\nget name() {...}\n}\n\nclass Salesman extends Employee {...}\n\nclass Engineer extends Employee {...}\n\n\n\nÉtapes :\n\nOn examine les méthodes des classes filles à remonter pour s’assurer qu’elles sont identiques.\n\n\nSi c’est pas le cas, on refactore jusqu’à obtenir une méthode identique à remonter dans chaque classe.\n\n\n\nOn s’assure que tous les appels de méthode ou de champ seront accessibles depuis la classe mère.\n\n\n\nSI la signature des méthodes est différente, on les change avec Change Function Declaration pour qu’elles soient similaires.\n\n\n\nOn crée une méthode dans la classe mère, et on copie le code d’une des méthodes à remonter dedans.\n\n\n\nOn lance les vérifications statiques.\n\n\n\nOn supprime une à une les méthodes des classes filles, en testant à chaque fois.\n\n\n\n\nThéorie :\nLe but de ce refactoring est d’éviter la duplication de code dans les classes filles, en le remontant dans la classe mère.\nOn l’utilise souvent après avoir généralisé une méthode avec Parameterize Function pour faire en sorte que les deux classes filles aient la même méthode, qu’on peut alors remonter.\nSi la méthode à remonter fait référence à des champs dans les classes filles, on peut utiliser Pull Up Field pour les remonter d’abord.","pull-up-field#Pull Up Field":"Exemple :\nAvant :\nclass Employee {...}\n\nclass Salesman extends Employee {\nprivate name: string;\n}\n\nclass Engineer extends Employee {\nprivate name: string;\n}```\n\nAprès :\nclass Employee {\nprotected name: string;\n}\n\nclass Salesman extends Employee {...}\n\nclass Engineer extends Employee {...}\n\n\n\nÉtapes :\n\nOn inspecte bien l’utilisation du champ pour vérifier qu’il s’agit vraiment de la même chose.\n\n\n\nSi les deux champs ont des noms différents, on utilise Rename Field pour leur redonner le même nom.\n\n\n\nOn crée un champ dans la classe mère, avec une protection suffisamment lâche pour que les classes filles y aient accès (protected).\n\n\n\nOn supprime les champs des classes filles.\n\n\nOn teste.\n\n\nThéorie :\nOn se retrouve parfois avec le même champ présent dans deux classes filles, pas forcément sous le même nom.\nPour savoir si c’est le même champ, il faut voir comment il est utilisé dans la classe.\n\n\nOn va souvent vouloir déplacer le champ et ensuite déplacer les méthodes qui manipulent ce champ.","pull-up-constructor-body#Pull Up Constructor Body":"Exemple :\nAvant :\nclass Party {...}\n\nclass Employee extends Party {\nconstructor(name, id, monthlyCost) {\nsuper();\nthis._id = id;\nthis._name = name;\nthis._monthlyCost = monthlyCost;\n}\n}\n\nAprès :\nclass Party {\nconstructor(name){\nthis._name = name;\n}\n}\n\nclass Employee extends Party {\nconstructor(name, id, monthlyCost) {\nsuper(name);\nthis._id = id;\nthis._monthlyCost = monthlyCost;\n}\n}```\n\n\n\nÉtapes :\n\nOn définit un constructeur dans la classe mère (s’il n'existe pas déjà), et on l’appelle dans les constructeurs des classes filles.\n\n\n\nOn utilise Slide Statements pour déplacer les instructions communes du constructeur juste après l’appel à super().\n\n\n\nOn déplace le code commun dans le constructeur parent, en donnant tous les paramètres nécessaires dans l’appel à super().\n\n\n\nOn teste.\n\n\n\nSi on ne peut pas déplacer une partie du code vers le haut du constructeur juste après super(), on peut utiliser Extract Function pour l’extraire, puis Pull Up Method pour le remonter dans le parent et l’utiliser dans le constructeur parent.\n\n\n\n\nThéorie :\nCe refactoring fait la même chose que Pull Up Method, mais en répondant aux règles spécifiques des constructeurs.\nSi on a du mal à appliquer ce refactoring, on peut utiliser Replace Constructor with Factory Function à la place.","push-down-method#Push Down Method":"Exemple :\nAvant :\nclass Employee {\nget quota {...}\n}\n\nclass Engineer extends Employee {...}\n\nclass Salesman extends Employee {...}\n\nAprès :\nclass Employee {...}\n\nclass Engineer extends Employee {...}\n\nclass Salesman extends Employee {\nget quota {...}\n}\n\n\n\nÉtapes :\n\nOn copie la méthode dans les classes filles qui en ont besoin.\n\n\n\nOn supprime la méthode depuis la classe mère.\n\n\n\nOn teste.\n\n\n\nOn supprime la méthode dans chaque classe fille qui n’en a pas besoin.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nSi une méthode n’est utilisée que par une classe fille (ou un petit nombre de classes filles par rapport au total) et qu’elle se trouve dans la mère, on peut la descendre pour rendre plus clair que les autres filles n’en ont pas l’usage.","push-down-field#Push Down Field":"Exemple :\nAvant :\nclass Employee {\nprivate quota: string;\n}\n\nclass Engineer extends Employee {...}\n\nclass Salesman extends Employee {...}\n\nAprès :\nclass Employee {...}\n\nclass Engineer extends Employee {...}\n\nclass Salesman extends Employee {\nprotected quota: string;\n}\n\n\n\nÉtapes :\n\nOn crée le champ dans les classes filles qui en ont besoin.\n\n\n\nOn supprime le champ de la classe mère.\n\n\n\nOn teste.\n\n\n\nOn supprime le champ des classes filles qui n’en ont pas besoin.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nSi un champ n’est utilisé que par une classe fille (ou un petit nombre de classes filles par rapport au total), on le descend dans les classes qui en ont l’usage.","replace-type-code-with-subclasses#Replace Type Code with Subclasses":"Exemple :\nAvant :\nfunction createEmployee(name, type) {\nreturn new Employee(name, type);\n}\n\nAprès :\nfunction createEmployee(name, type) {\nswitch (type) {\ncase \"engineer\":\nreturn new Engineer(name);\ncase \"salesman\":\nreturn new Salesman(name);\ncase \"manager\":\nreturn new Manager(name);\n}\n}\n\n\n\nÉtapes :\n\nOn va auto-encapsuler le champ qui contient la valeur de type (on crée des getter/setter et on les utilise en interne dans la classe, pour ne plus accéder au champ directement autrement que par ces getter/setter).\n\n\n\nOn choisit une valeur de type, et on crée une classe fille pour cette valeur. En fonction de la méthode directe ou indirecte, on aura la classe mère à créer ou alors ce sera notre classe initiale.\n\n\n\nOn surcharge le getter du champ de type de cette nouvelle classe pour qu’il renvoie la valeur de type littérale (celle qu’on avait initialement).\n\n\n\nOn ajoute de la logique pour utiliser le getter issu de la nouvelle classe.\n\n\nAvec la méthode directe, il faudra qu’on instancie la bonne classe dès le début, donc on peut utiliser Replace Constructor with Factory Function.\nAvec la méthode indirecte, on peut instancier la bonne classe de type dans le constructeur de la classe initiale.\n\n\n\nOn teste.\n\n\n\nOn répète la création de classe fille pour chaque valeur de type.\n\n\nOn teste à chaque fois.\n\n\n\nOn supprime le champ de type initial.\n\n\n\nOn teste.\n\n\n\nOn utilise Push Down Method et Replace Conditional with Polymorphism sur les méthodes qui utilisent les getter/setter créés à l’étape 1.\n\n\n\nOn peut supprimer les getter/setter pour la valeur de type (plus besoin d’auto-encapsulation du type).\n\n\n\n\nThéorie :\nQuand on veut faire des choses différentes en fonction du type de chose, par exemple des employés différenciés par leur fonction, on peut se contenter d’une variable qui nous indique cette information. Mais on peut parfois vouloir une hiérarchie de classes.\nLa hiérarchie de classe est utile quand :\nOn veut transformer des conditions similaires en polymorphisme comme avec Replace Conditional with Polymorphism.\nOn a des fonctionnalités qui ne concernent que certains types, qu’on peut du coup mettre dans une méthode de la classe fille concernée.\n\n\nIl y a deux manières de le faire :\n1- Directement : transformer la classe qui prend le type en classe mère, et créer des classes filles pour chaque type.\nIl faut bien réfléchir au critère sur lequel on crée notre hiérarchie : si on le fait pour ce type, on ne pourra pas en même temps le faire pour un autre critère.\nSi le type est mutable, il vaut mieux partir sur la 2ème méthode.\n\n\n2- Indirectement : créer une hiérarchie de classes juste pour le type, et y placer la logique liée au type. On utiliserait ici la composition en gardant une instance de ce type dans la classe qui prend le type.","remove-subclass#Remove Subclass":"Exemple :\nAvant :\nclass Person {\nget genderCode() {\nreturn \"X\";\n}\n}\n\nclass Male extends Person {\nget genderCode() {\nreturn \"M\";\n}\n}\n\nclass Female extends Person {\nget genderCode() {\nreturn \"F\";\n}\n}\n\nAprès :\nclass Person {\nget genderCode() {\nreturn this._genderCode;\n}\n}\n\n\n\nÉtapes :\n\nOn utilise Replace Constructor with Factory Function pour que la classe fille qu’on veut supprimer soit construite depuis une fonction.\n\n\n\nSi on a du code qui applique une condition sur le type de classe fille, on extrait ce code avec Extract Function, et on le remonte vers la classe mère avec Move Function.\n\n\n\nOn crée un champ pour représenter le type de sous-classes dans la classe mère.\n\n\n\nOn modifie les méthodes qui utilisent la classe fille pour qu’elles fassent référence au champ de type.\n\n\n\nOn supprime la classe fille.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nQuand on a une hiérarchie de classes, on peut en profiter pour mettre des comportements dans chaque classe fille et obtenir quelque chose de flexible.\nMais parfois ces comportements ne sont pas (ou plus) suffisants pour justifier la complexité induite par la hiérarchie. Dans ce cas, il faut supprimer les classes filles.","extract-superclass#Extract Superclass":"Exemple :\nAvant :\nclass Department {\nget totalAnnualCost() {...}\nget name() {...}\nget headCount() {...}\n}\n\nclass Employee {\nget annualCost() {...}\nget name() {...}\nget id() {...}\n}\n\nAprès :\nclass Party {\nget name() {...}\nget annualCost() {...}\n}\n\nclass Department extends Party {\nget annualCost() {...}\nget headCount() {...}\n}\n\nclass Employee extends Party {\nget annualCost() {...}\nget id() {...}\n}\n\n\n\nÉtapes :\n\nOn crée une classe vide, et on fait hériter nos classes avec du code dupliqué de cette classe.\n\n\n\nOn teste.\n\n\n\nPour chaque classe fille, on utilise Pull Up Constructor Body, Pull Up Field et Pull Up Method pour déplacer le code commun aux classes filles dans la nouvelle classe mère.\n\n\n\nOn refait une passe sur les classes filles, et si on constate encore du code commun mais pas dans des méthodes isolées, on utilise Extract Function pour l’isoler, puis Pull Up Method pour le remonter.\n\n\n\nOn vérifie le code appelant, pour voir s' il ne faudrait pas utiliser l’interface de la classe mère quelque part.\n\n\n\n\nThéorie :\nLe but de ce refactoring est de rassembler une logique dupliquée dans plusieurs classes vers une classe mère commune.\nL’auteur conseille par défaut d’utiliser ce refactoring à la place de Extract Class, quitte à le transformer en délégation ensuite avec Replace Superclass with Delegate.","collapse-hierarchy#Collapse Hierarchy":"Exemple :\nAvant :\nclass Employee {...}\n\nclass Salesman extends Employee {...}\n\nAprès :\nclass Employee {...}\n\n\n\nÉtapes :\n\nOn choisit la classe qu’on veut supprimer (mère ou fille).\n\n\n\nOn utilise Pull Up Field, Pull Up Method, Push Down Field et Push Down Method pour déplacer tous les éléments de la classe à supprimer vers l’autre qui reste.\n\n\n\nOn ajuste les références des méthodes déplacées dans la classe qui reste pour qu’elles s'intègrent avec le reste du code de la classe.\n\n\n\nOn supprime la classe vide.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nQuand le fait d’avoir une hiérarchie de classes n’apporte plus suffisamment de choses pour contrebalancer la complexité induite par la hiérarchie elle-même, on supprime un les classes d’un des niveaux pour éliminer la hiérarchie.","replace-subclass-with-delegate#Replace Subclass with Delegate":"Exemple :\nAvant :\nclass Order {\nget daysToShip() {\nreturn this._warehouse.daysToShip;\n}\n}\n\nclass PriorityOrder extends Order {\nget daysToShip() {\nreturn this._priorityPlan.daysToShip;\n}\n}\n\nAprès :\nclass Order {\nget daysToShip() {\nreturn this._priorityDelegate\n? this._priorityDelegate.daysToShip\n: this._warehouse.daysToShip;\n}\n}\n\nclass PriorityOrderDelegate {\nget daysToShip() {\nreturn this._priorityPlan.daysToShip;\n}\n}\n\n\n\nÉtapes :\n\nS’il y a de nombreux endroits où le constructeur de la classe fille qu’on va supprimer est utilisé, on va d’abord encapsuler la création des objets avec Replace Constructor with Factory Function.\n\n\n\nOn crée une classe vide pour le délégué. On lui fait prendre au constructeur tous les paramètres spécifiques à la classe fille à supprimer.\n\n\n\nOn ajoute un champ à la classe mère pour stocker l'instance du délégué.\n\n\n\nOn modifie le code de création de la classe fille à supprimer, pour qu’il crée l’instance du délégué et le place sur la classe mère : soit dans le constructeur de la classe fille, soit dans la factory function (si on l’a créée à l’étape 1).\n\n\n\nOn choisit une méthode de la classe fille à déplacer dans le délégué.\n\n\n\nOn utilise Move Function pour la déplacer dans le délégué.\n\n\nOn ne fait pas la dernière étape de supprimer la fonction de délégation restée dans la classe fille.\nSi la méthode a besoin d’autres champs ou méthodes pour fonctionner, on les déplace aussi.\nSi elle a besoin de méthodes ou de champs qui doivent rester sur la classe mère, on passe une référence vers la classe mère au délégué.\n\n\n\nSi la méthode dans la classe fille qu’on est en train de traiter est appelée depuis l’extérieur de sa hiérarchie de classe, on déplace la méthode (qui est maintenant une méthode de délégation vers l’objet délégué) vers la classe mère. On y fait une vérification de l'existence du délégué avant l’appel.\n\n\nSi il n’y avait pas d’appelants externes, on applique simplement Remove Dead Code sur la méthode dans la classe fille.\n\n\n\nOn teste.\n\n\n\nOn répète les étapes 7 et 8 jusqu’à ce que toutes les méthodes de la classe fille soient transférées vers le délégué.\n\n\n\nOn trouve toutes les occurrences de construction d’objet de la classe fille, et on le modifie pour construire la classe mère à la place.\n\n\n\nOn teste.\n\n\n\nOn applique Remove Dead Code sur la classe fille vide.\n\n\n\n\nThéorie :\nL’héritage permet de représenter naturellement la catégorisation des objets.\nElle a par contre deux problèmes :\nOn ne peut utiliser l’héritage pour classer que selon un seul axe de catégorie. Et donc la thématique qui n’est pas choisie pour l’axe utilisé pour l’héritage doit bien être traitée autrement.\nElle induit un grand couplage entre classe mère et fille, avec les changements dans la mère qui impactent toutes les filles.\n\n\nLa délégation (ou composition) règle ces deux problèmes.\nL’auteur a connaissance du principe populaire “Favor object composition over class inheritance”, mais l’aurait bien remplacé par “Favor a judicious mixture of composition and inheritance over either alone”.\nPour autant, il préfère utiliser par défaut l’héritage qui a ses propres avantages, quitte à utiliser ce refactoring pour passer sur de la délégation quand il sent qu’il y a des frictions dans la hiérarchie.\nCette question de choix entre héritage et délégation est aussi discutée dans le livre de GoF.\n\n\nUne des possibilités peut aussi être d’avoir une délégation qui elle-même a une hiérarchie pour laisser l’héritage de la classe principale à un autre axe, et profiter quand même de la puissance de l’héritage pour l’axe sur lequel on délègue.","replace-superclass-with-delegate#Replace Superclass with Delegate":"Exemple :\nAvant :\nclass List {...}\n\nclass Stack extends List {...}\n\nAprès :\nclass Stack {\nconstructor() {\nthis._storage = new List();\n}\n}\n\nclass List {...}\n\n\n\nÉtapes :\n\nOn crée un champ dans la classe fille avec le type de la classe mère, et on l’instancie dans le constructeur de la classe fille.\n\n\n\nPour chaque méthode de la classe mère, on crée une forwarding function dans la classe fille, qui appelle simplement la bonne méthode sur le champ créé à l’étape 1.\n\n\nOn teste à chaque fois.\nParfois il faut déplacer plusieurs méthodes interdépendantes, par exemple getter/setter.\n\n\n\nQuand on a créé des forwarding functions pour toutes les méthodes, on supprime le lien d’héritage.\n\n\n\nOn teste.\n\n\n\n\nThéorie :\nUn signe typique qui indique qu’il ne faut pas utiliser l'héritage, c’est quand on a des fonctions de la classe mère qui n’ont pas d’utilité dans les classes filles.\nUn exemple connu c’est la liste qu’on prend comme classe mère de la pile : la plupart des opérations de liste ne sont pas utiles pour la pile.\n\n\nUn autre problème aussi c’est que l’héritage induit d’un point de vue modélisation l’idée que les classes filles sont des instances de la mère, à tout point de vue y compris dans le monde physique, ce qui est assez restrictif pour bien des situations.\nLa délégation pose une vraie limite entre les deux classes.\n\n\nUn des inconvénients de la délégation c’est que pour les fonctionnalités similaires entre une classe et son délégué, il faut faire une forwarding function.\nL’auteur conseille quand même, comme dans la technique précédente, de partir sur de l’héritage par défaut, et de remplacer si besoin par la délégation avec ce refactoring."}},"/books/the-design-of-web-apis":{"title":"The Design of Web APIs","data":{"":"","1---what-is-api-design#1 - What is API design":"Ici nous parlons des API web, c'est-à-dire utilisant le protocole HTTP.\nUne API est dite publique quand elle est fournie comme service à une autre entreprise, et privée quand elle est fournie à des services internes.\nLe design d’une API web doit être fait pour rendre la vie simple aux développeurs qui vont la consommer.\nL’API doit cacher l’implémentation, et n’exposer que ce dont l’utilisateur a besoin.\nL’API doit être intuitive, ressembler à ce à quoi on s’attendrait.","2---designing-an-api-for-users#2 - Designing an API for users":"Il faut comprendre les besoins de l’utilisateur de l’API pour designer une API qui y réponde, et non pas utiliser le point de vue du backend.\nÉviter d’adopter le point de vue du créateur de l’API passe par :\nÉviter l’influence des structures de données côté backend. Si on expose exactement ce qui est en BDD c’est un signe qu’on n’adopte probablement pas le point de vue du consommateur.\nIl faut correctement nommer les choses, mais aussi introduire de l’abstraction pour masquer les détails qui n’intéressent pas le consommateur.\n\n\nÉviter l’influence du code business côté backend. Si le consommateur n’a pas besoin de savoir ce qui se passe finement, on peut lui exposer une API grossière et faire ce qu’il faut dans l’implémentation, ce sera plus simple et plus sûr.\nÉviter l’influence de l’architecture du backend. Par exemple, si le backend utilise une architecture séparée en services, le fait de faire apparaître cette même séparation interne sur l’API exposée n’est probablement pas une bonne idée. Il faut se concentrer sur le besoin du consommateur.\nÉviter l’influence de l’organisation humaine du provider de l’API. Le fait que le provider soit structuré avec tels ou tels départements par exemple ne doit pas influencer l’API.\n\n\n\n\nEn fait, il faut traiter les consommateurs de l’API vis-à-vis de celle-ci comme des utilisateurs finaux vis-à-vis du produit : on doit se demander ce que le consommateur veut, et de quelle manière il le veut. Ça revient à créer des User Stories du consommateur d’API.\nLes deux autres choses dont on a besoin sur les utilisateurs c’est les inputs et outputs : ce que l’utilisateur doit apporter, et ce dont il a besoin en retour lors de l’action.\nEnfin, pour trouver des éléments manquants et compléter l’API, on va analyser d’où vont venir les inputs requis, et ce qui sera fait des outputs.\nAutre question importante qui nous permettra d’être plus efficace : trouver qui sont nos utilisateurs.\nL’API goals canvas est donc un tableau avec les cases suivantes :\nWhos : les utilisateurs de l’API\nWhats : Ce que les utilisateurs font\nHows : les étapes de ce qu’ils font\nInputs : ce dont ils ont besoin à chaque étape, et d’où ils l’ont\nOutputs : ce qu’ils récupèrent à chaque étape, et ce qu’ils en feront\nGoals : Reformulation concise du besoin\n\n\nCe tableau ne peut pas être rempli à l’avance pour toute l'application. Il s’agit d’un processus itératif où on traite les fonctionnalités par petits bouts.","3---designing-a-programming-interface#3 - Designing a programming interface":"REST est une méthode de design d’API qui se calque sur HTTP. Il s’agit d’utiliser à chaque fois une action HTTP (par exemple GET), et un chemin identifiant une ressource (par exemple /product/123), et un contenu optionnel.\nLa réponse est un statut qui indique si la transaction s’est bien passée, et un contenu.\n\n\nPour faire une API REST, il va s’agir de trouver les ressources à partir des API goals qu’on a pu analyser par exemple avec le canvas, et ensuite de leur attribuer des actions.\nQuand on a plusieurs ressources citées dans un API goal, il faut identifier la ressource principale. C’est sur elle que porte l’action. L’autre n’étant en général qu’une information donnée dans le chemin ou le contenu additionnel de la requête.\nExemple : Rechercher des produits dans un catalogue avec une requête textuelle : c’est bien le catalogue qui est la ressource principale.\n\n\nLes chemins doivent refléter cette organisation, en ayant un nom compréhensible pour la ressource au début (/catalogue), suivi des paramètres.\nLa manière la plus standard de faire des chemins c’est d’avoir un nom pluriel pour les collections, suivi d’un identifiant d’une des ressources de cette collection :\n/ressources/{ressourceId}\n/ressources/{ressourceId}/sub-ressources/{subRessourceId}\n\n\n\n\nSi on a un paramètre à donner dans une requête qui doit simplement récupérer un résultat (par exemple faire une recherche), alors on utilise GET et on donne la chaîne de recherche dans un paramètre de l’URL.\nEx : GET /products?free-query=blabla\n\n\nDe même que le path et l’action de l’API doit être conçue selon les besoins du client et non pas selon la structure du backend, le contenu des données doit aussi être structuré dans ce but :\nSi certaines propriétés ne sont pas nécessaires au client pour son usage, on les supprime.\nSi certaines propriétés seraient plus claires dans ce contexte sous un autre nom : on les renomme.\n\n\nQuand on ne sait pas quelle méthode REST choisir pour une action, par défaut on choisit POST.\nIl est facile de faire correspondre une action métier avec une API REST, tant qu’on est dans du CRUD d’une ressource. Mais quand on veut faire des actions plus complexes, alors il faut faire des compromis entre la user-friendlyness, et la conformité avec le standard REST.\nEx : Valider son panier peut être fait avec :\nPOST /cart/check-out, ce qui est très user friendly, mais pas très conforme à la norme de manipulation de ressources de REST.\nPOST /orders, ce qui est plus conforme à REST mais on perd la notion métier de panier dans l’histoire.\nEn réalité il n’y a pas de “bonne solution”, il s’agit de faire un compromis. Il n’y a pas toujours de bonne API.\n\n\n\n\nL’architecture REST :\na été élaborée par un thésard en 2000, et a depuis envahi l’univers des API réseau.\na été conçue spécifiquement pour les architectures distribuées, dont d’ailleurs l’architecture navigateur / serveur en est un rudiment. Il s’agit de favoriser l’efficacité, la stabilité, la scalabilité.\nPour être RESTful, il faut respecter ces 6 principes :\nSéparation Client / Serveur\nStatelessness : toute l’information d’une requête est contenue dans la requête. Le serveur ne conserve pas de session.\nCacheability : une réponse doit indiquer si elle peut être stockée par le client, et si oui pour combien de temps avant qu’il faille qu’il refasse la requête.\nLayered system : quand le client interagit avec le serveur, il n’en voit qu’une couche, le reste est caché derrière l’API.\nCode on demand (optionnel) : le serveur peut transférer du code exécutable au client, par exemple du JavaScript.\nUniform Interface :\nToutes les interactions se font au travers d’actions standardisées sur des ressources, et en fonction de l’état de ces ressources (qu’on obtient par GET, et qu’on modifie par PATCH etc.).\nLes interactions doivent aussi fournir les metadata suffisantes pour comprendre la structure de ces états et ce qu’il est possible de faire aux ressources.\n\n\n\n\n\n\nSi on se rend compte qu’un des paramètres d’une méthode de notre API ne peut pas être fournie par le client, alors c’est qu’on a sans doute oublié quelque chose… Il faut revoir l’API, ajouter une méthode etc.","4---describing-an-api-with-an-api-description-format#4 - Describing an API with an API description format":"Décrire une API selon un format standardisé permet de la partager avec des êtres humains qui vont savoir la lire, et avec des outils automatisés qui pourront l’exploiter pour générer une documentation HTML, ou dans n’importe quel autre but.\nL’OAS (Open API Specification) est un format ouvert, communautaire, et très utilisé pour REST.\nInitialement la spécification s’appelait Swagger, mais elle a été renommée OpenAPI en 2016.\nOn peut utiliser JSON ou YAML pour écrire le document, mais Arnaud recommande YAML pour la lisibilité accrue pour les humains.\nOn édite le fichier à la main et on le versionne.\nVSCode a une extension Swagger viewer qui aide à visualiser ce qu’on a écrit.\n\n\nTips YAML :\nPour indiquer qu’il n’y a rien, il faut mettre des {}, par exemple quand il n’y a pas encore de paths : paths: {}\nPour écrire une string multiligne, on met un | d’abord :\ndescription: |\nblabla\nblabla\n\nLes noms de propriété doivent être des strings en YAML, donc il faut entourer les chiffres par des guillemets quand ils sont en propriété.\n\n\nLe format OpenAPI :\nIl ne faut pas hésiter à écrire des descriptions.\nex\npaths:\n/objects:\ndescription: description du path\nget:\nsummary: résumé court de la méthode\ndescription: résumé long de la méthode\nresponses:\n\"200\":\ndescription: description de la réponse\ncontent:\napplication/json:\nschema:\n# et on décrit le schéma de la réponse\n\nDans le cas où une méthode GET prend un paramètre, ça se passe dans parameter :\nget:\nparameters:\n# il s'agit du nom qui va apparaître dans l'URL\n- name: nom-du-parametre\ndescription: description du paramètre\n# où se trouve le paramètre\nin: query\n# caractère obligatoire ou non\nrequired: false\nschema:\n# type de données du paramètre\ntype: string\n\nMême chose pour les path parameters quand un morceau du path est paramétrisé. Dans ce cas il faut l’indiquer entre accolades :\npaths:\n/objects/{objectId}\ndelete:\nparameters:\n- name: objectId\nin: path\n# obligatoire, sinon ça ne compilera pas\nrequired: true\nschema:\ntype: string\n\nPour décrire les formats JSON attendus en entrée, ou en réponse, OpenAPI utilise la spécification JSON-schema :\nExemple pour {prop1: \"\", prop2: { prop2-1: \"\"}}\ntype: object\ndescription: une description de l'objet JSON\nrequired:\n- prop1\n- prop2\nproperties:\nprop1:\ntype: string\ndescription: description de prop1\nexample: Exemple de valeur\nprop2:\ntype: object\nproperties:\nprop2-1\ntype: string\nexample: Exemple de valeur\n\nPour les méthodes comme POST qui ont besoin de données en entrée, ça se fait dans requestBody, à côté de responses.\n\n\nOn peut réutiliser des schémas JSON déjà définis pour éviter de les répéter dans notre fichier OpenAPI :\nIl faut renseigner le schéma à la racine du document, dans :\ncomponents:\nschemas:\nmon_objet:\ntype: object\nproperties:\n# [...]\n\nEt ensuite on peut le référencer avec :\nschema:\n$ref: #/components/schemas/mon_objet\n\n\n\nOn peut placer le mot-clé parameters non seulement au niveau des actions (get, post etc.), mais aussi un niveau au-dessus, au niveau des ressources (c’est-à-dire au niveau du path). Et alors à ce moment là ces paramètres s’appliquent à toutes les méthodes du path. Ça évite de les répéter, fût-ce avec un $ref.","5---designing-a-straightforward-api#5 - Designing a straightforward API":"Une des clés d’un design “straitforward” est de reproduire l’habitude des gens, et ça dans tous les domaines.\nIl faut des noms de propriété clairs et évocateurs.\nÉviter les abréviations (min ou max c’est OK parce que c’est entré dans la langue commune)\nÉviter les détails techniques qu’on peut retrouver autrement (ex. type de variable booléenne)\nÉviter ce qui n’intéresse pas le consommateur de manière générale.\nEssayer de ne pas dépasser 3 mots dans un nom quel qu’il soit.\n\n\nIl faut des formats de données bien choisis.\nPar exemple le format ISO 8601 pour une date.\nAdopter le point de vue du consommateur :\n\"type\": \"checking\" plutôt que \"type\": 1\n\n\nIl faut aussi que les données qu’on donne soient pertinentes pour le consommateur.\nNe pas hésiter à ajouter des données pour que le consommateur ait ce qu’il lui faut déjà préparé.\n\n\nA propos des erreurs :\nIl faut bien distinguer 3 types d’erreurs par leur type :\nerreur de format parmi les paramètres fournis\nerreur fonctionnelle\nerreur interne du serveur\n\n\nPour une erreur interne, en général la seule chose qu’a besoin de savoir l’utilisateur c’est qu’il y a eu une erreur et que ce n’est pas de sa faute.\nEn HTTP, les codes 4XX concernent les erreurs causés par l’utilisateur, alors que les codes 5XX concernent les erreurs sur lesquelles l’utilisateur n’a pas d’influence.\nLe code de statut ne suffit pas. Il faut aussi donner d’autres informations, à la fois du texte pour les êtres humains, et des informations pour les machines.\nPour les machines ça peut être un string représentant un type d’erreur précis, une valeur d’ID etc.\n\n\nDans le cas où il y a plusieurs erreurs, il vaut mieux les signaler toutes dans la même réponse.\nSi les erreurs sont de plusieurs types (par ex données mal formées et erreur fonctionnelle), on peut utiliser le code de retour 400 qui est générique.\n\n\n\n\nPour les messages de succès c’est comme pour les erreurs :\nBien renseigner le bon code 2XX\nDonner des informations supplémentaires, comme par exemple retourner l’objet qu’on vient de créer avec son ID générée.\n\n\nIl faut regarder le flow dans son ensemble, et repérer ce dont l’utilisateur d’API a besoin pour lui éviter des erreurs qui seraient de son fait.\nUne fois qu’on a repéré les choses dont il aurait besoin, on les lui fournit avec une API pour lui faciliter le travail.\nNe pas hésiter à agréger des données ensemble dans une API, si ça peut aider le consommateur à consulter une autre API en évitant des erreurs\nPar exemple lui fournir les éléments qui ont une particularité plutôt que bruts, pour qu’il puisse les utiliser dans une autre API qui a besoin en paramètre d’un élément qui a la particularité.","6---designing-a-predictable-api#6 - Designing a predictable API":"Il faut rester consistant au sein de l’API :\nDans le nommage et le format des différents paramètres dans notre API.\nEx : si on a choisi accountNumber quelque part, on ne le change pas en juste number dans un des autres endpoints.\nEt si c’était un nombre, ça reste un nombre.\n\n\nEntre les différents paths de nos API endpoints.\nDans la structure des données prises en paramètre ou renvoyées.\nDans les flows permis par les différents API endpoints.\n\n\nEn plus de la consistance au sein de l’API, il faut respecter la consistance au sein de l’organisation, au sein du domaine métier, et enfin avec ce qui est partagé par le reste du monde.\nNe pas hésiter à utiliser des standards ISO et autres.\nTIP : chercher “<some data> standard” ou “<some data> format” sur google.\n\n\nNe pas hésiter à copier ce que fait une autre API connue, les utilisateurs se sentiront chez eux et tout le monde sera gagnant.\n\n\nIl faut formaliser les règles choisies pour l’API dans un document (cf. chapitre 13), au risque de les oublier et de faire perdre à l’API peu à peu sa consistance.\nIl peut être intéressant de rendre l’API adaptable aux besoins de l’utilisateur :\nUn même endpoint peut proposer d’envoyer les données sous plusieurs formats, par ex CSV et JSON.\nOn peut utiliser un paramètre dans l’URL : /ressource?format=csv\nEt on peut aussi utiliser un header HTTP fait pour ça : Accept: text/csv\nSi le serveur ne peut pas répondre dans ce format, il peut renvoyer le code 406 Not Acceptable.\n\n\nA contrario, quand c’est le client qui envoie par exemple du contenu XML, il peut le spécifier avec le header HTTP Content-type: application/xml\nSi le serveur ne comprend pas ce format, il peut répondre 415 Unsupported Media.\n\n\n\n\nDe la même manière que pour les formats, un endpoint peut supporter plusieurs langues (traduction des phrases, système de mesure etc.).\nOn peut là aussi tirer avantage du protocole HTTP en utilisant ses headers :\nAccept-Language: en-US pour dire qu’on accepte l’anglais US.\nContent-Language: en-US pour dire que le body sera en anglais US.\n\n\n\n\nEnfin on peut aussi permettre à l’utilisateur des fonctionnalités de pagination, de filtrage ou d’ordre des éléments.\nLe plus courant est d’utiliser des paramètres dans l’URL :\n/ressources?page=3&sort=-amount&category=car\n\n\nPour la pagination on pourrait penser au header HTTP Range: items=10-19 qui dit qu’on veut les items du 10ème au 19ème.\n\n\n\n\nIl peut être très pratique le notre API soit discoverable.\nOn peut faire ça à l’aide de metadata qu’on place dans le body. Par exemple, si on renvoie du contenu paginé, avoir un attribut “pagination” qui va donner la page actuelle et le nombre total de pages, pour aider l’utilisateur.\nOn peut aussi ajouter des URLs pour aider l’utilisateur à faire ses prochaines requêtes d’API : ça fait des hypermedia API. Par exemple pour la pagination, ajouter en plus un attribut next et un last avec en valeur les URLs permettant d’obtenir ces pages.\nIl s’agit d’une certaine manière de faire la même chose qu’avec les pages web et leurs liens, mais avec les APIs.\nUsuellement, les URLs vers le reste de l’API sont mis dans des attributs nommés href, links, ou _links.\nPlusieurs spécifications ont été créées sur la manière d’organiser ces URLs d’API, parmi eux HAL, Collection+JSON, JSON API, JSON-LD, Hydra et Siren.\n\n\nUn des moyens d’obtenir des informations sur un endpoint est d’utiliser la méthode HTTP OPTIONS, qui devrait renvoyer les autres méthodes disponibles sur cet endpoint, mais aussi des informations sur le format, ou encore des URLs vers d’autres endpoints utiles liés.\nAttention cependant à bien documenter ce qu’on implémente dans notre API, les utilisateurs sont rarement des experts du protocole HTTP…","7---designing-a-concise-and-well-organized-api#7 - Designing a concise and well-organized API":"Il faut bien organiser son API :\nOrganiser les données :\nPar exemple en groupant les propriétés liées entre elles ensemble, côte à côte ou même au sein d’un objet.\nDans le cas où une propriété est optionnelle, et qu’une autre propriété n’a de sens que si la première est utilisée, les grouper dans un objet serait une bonne idée.\n\n\nEn classant les propriétés des plus importantes aux moins importantes à mesure qu’on descend\n\n\nOrganiser le feedback :\nQuand des erreurs se produisent, on peut les catégoriser avec un type précis avec un système d’enums (en plus du statut HTTP).\nEt on peut les classer de la plus importante à la moins importante.\n\n\nOrganiser les API endpoints :\nOn peut les grouper par catégorie fonctionnelle dans notre document OpenAPI, pour plus de clarté.\nEt dans chaque catégorie, classer du plus important au moins important.\n\n\n\n\n\n\nIl faut bien dimensionner ses endpoints :\nBien réfléchir au nombre de propriétés d’une structure de données, et à la profondeur d’imbrication en fonction de son cas d’usage.\nPour la profondeur, il est recommandé de ne pas dépasser 3 niveaux.\n\n\nBien réfléchir aux endpoints eux-mêmes et à la complexité qu’ils peuvent embarquer pour l’utilisateur :\nPar exemple, un endpoint qui permet d’avoir des infos sur un compte en banque, et qui donne en même temps la liste de toutes les transactions du compte sera probablement surchargé inutilement si l’utilisateur veut juste les infos sur le compte.\nIl vaut mieux s’assurer que notre endpoint ne fait pas 2 choses très différentes (ou plus) en même temps.\n\n\nLe dimensionnement dépend en fait du contexte et doit être optimisé pour l’expérience utilisateur des consommateurs. En général, les petites unités fonctionnent mieux qu’un énorme couteau suisse.","8---designing-a-secure-api#8 - Designing a secure API":"Une des techniques utilisées couramment pour sécuriser des API web est le framework OAuth 2.0.\n\nil faut enregistrer des consumers.\n\n\nCa peut se faire par exemple sur un portail de développement où on va se connecter, payer ce qu’il faut etc.\nQuand on enregistre le consumer, on va lui attribuer des scopes d’autorisation, qui vont correspondre à la possibilité d’accéder à un certain nombre d’API endpoints et de méthodes.\n\n\n\nle client consumer va pouvoir demander un token au serveur d’authentification.\n\n\nL’utilisateur final propriétaire entre ses identifiants sur la page du serveur d’authentification, et après vérification le serveur d’authentification envoie le token au client.\nIl existe d’autres flows possibles dans OAuth.\n\n\n\nLe client va pouvoir consommer l’API en fournissant à chaque fois le token.\n\n\nA chaque requête, le serveur qui a les ressources va valider auprès du serveur d'authentification que le token est valide et possède les bons scopes d’autorisation.\nLe serveur de ressources a connaissance du user ID et du scope d'autorisation attaché au token, et donc va pouvoir éventuellement filtrer le contenu avant de le renvoyer ce que spécifie l’API.\n\n\n\n\nL’ID de l’utilisateur et les scopes attachés à son compte ne sont en général pas dans le body de la requête ou les paramètres d’URL, mais sont pourtant là de par l’authentification initiale. Il est donc intéressant de remarquer qu’un même API endpoint peut renvoyer différentes données en fonction de l’utilisateur qui y fait appel.\nSelon l’organisation internationale OWASP, il faut réduire la surface d’attaque pour réduire le risque. Et donc il faut éviter de donner la possibilité de faire certaines actions à des utilisateurs qui n’en ont pas besoin.\nComment bien définir les scopes d’autorisation ?\nIl faut choisir la bonne granularité : un scope par endpoint et méthode c’est très flexible mais très complexe à configurer pour les utilisateurs, et si on a trop peu de scopes on risque d’être obligé de donner trop de pouvoir à certains utilisateurs.\nOn peut par exemple partir de l’API goals canvas qui nous a permis de construire notre API en fonction des besoins de l’utilisateur qu’on a définis, et regrouper sous un même scope les actions qui concernent un même flow.\nOn peut aussi avoir des scopes arbitraires, comme un scope “admin” qui a accès à tout.\nIl existe plusieurs manières de définir des scopes et pas de solution magique, et la bonne manière dépend en fait du contexte.\nOn peut aussi définir plusieurs granularités en même temps : des scopes grossiers mais faciles à configurer, et des scopes plus fins. L’utilisateur pourra alors utiliser un mélange et configurer plus finement les parties qu’il veut.\n\n\nOpenAPI permet de spécifier des scopes pour chaque endpoint/méthode défini :\nCa se fait dans components -> securitySchemes\ncomponents:\nsecuritySchemes:\nBankingAPIScopes:\ntype: oauth2\nflows:\nimplicit:\nauthorizationURL: \"https://...\"\nscopes:\n\"beneficiary:read\": List beneficiaries\n\"beneficiary:manage\": Create, list beneficiaries\n\nEt ensuite on les indique dans nos paths existants :\npaths:\n/beneficiaries:\nget:\ndescription: Get beneficiaries list\nsecurity:\n- BankingAPIScopes:\n- \"beneficiary:read\"\n- \"beneficiary:manage\"\nresponses:\n…\n\n\n\nToujours dans l’optique de réduire la surface d’attaque, il faut identifier les données sensibles et ne pas les fournir quand ce n’est pas nécessaire.\nPar exemple, quand on obtient les informations d’un compte, on peut donner par défaut les autres infos mais pas le numéro du compte, ou seulement les 4 derniers chiffres.\nQuand il faut les fournir quand même, ou quand il faut fournir une action avec des conséquences jugées sensibles, alors on peut y faire attention et utiliser des techniques pour s’assurer qu’elles sont protégées :\nIsoler l’action sensible dans un endpoint dédié qu’on va pouvoir mieux protéger, dans le cas où il y a une vraie différence de nature entre la version sensible et la version non sensible d’un point de vue utilisateur.\nUtiliser un scope dédié, pour que par défaut la donnée sensible ne soit pas retournée ou actionnée, et le soit si le scope d’autorisation est activé.\nSe baser sur les permissions de l’utilisateur qui fait l’appel pour ne dévoiler la donnée sensible qu’à certaines personnes.\nEt enfin on peut combiner l’access control de l’utilisateur et du consommateur (scope) pour être sûr qu’il s’agit de la bonne personne (et non pas d’une personne qui a une délégation par exemple) et qu’elle le fait dans un cadre où elle a la permission de le faire.\n\n\n\n\nConcernant le feedback lié à la sécurité, quand on demande quelque chose auquel on n’a pas accès, on peut recevoir une réponse 401 Unauthorized si on n’est pas correctement authentifié, ou 403 Forbidden si les scopes qui nous sont associés ne sont pas suffisants pour qu’on fasse l’action.\nAttention à ne pas dévoiler inutilement des informations : il vaut parfois mieux renvoyer systématiquement 404 Not Found plutôt qu’un 403 même si la donnée existe, pour éviter qu’un attaquant essaye de nombreuses données et puisse savoir lesquelles existent même s’il n’a pas les autorisations d’y accéder.","9---evolving-an-api-design#9 - Evolving an API design":"Quand on change notre API, il faut surtout éviter les breaking changes, c’est-à-dire ce qui obligerait les consommateurs à changer leur code pour ne pas que leur application casse.\nSur les données en sortie de l’API :\nIl ne faut pas :\nEnlever, modifier ou déplacer des propriétés.\nModifier le format / type d’une donnée.\nAugmenter la taille d’une chaîne qui était limitée.\nRendre optionnelle une propriété qui était obligatoire.\nAjouter des valeurs à un enum.\n\n\nEn revanche on peut :\nAjouter des propriétés, optionnelles ou obligatoires.\nRendre obligatoire une propriété qui ne l’était pas.\nDiminuer la taille maximale de chaînes limitées.\n\n\nUne des possibilités c’est d’ajouter des données déjà existantes dans de nouvelles propriétés, mais sous une autre forme. Ça peut par contre avoir des limites dans la mesure où ça complique aussi l’API.\n\n\nSur les données en entrée et les paramètres :\nC’est pareil que pour les données en entrée, sauf :\nOn peut rendre optionnelle une propriété qui était obligatoire, mais pas l’inverse.\nOn peut augmenter la taille maximale d’une chaîne et pas la diminuer.\nOn peut ajouter des valeurs à un enum.\nSi on ajoute une propriété, il faut qu’elle soit optionnelle.\n\n\n\n\nSur le feedback :\nLe feedback peut être traité comme des données de sortie.\nOn ne peut donc pas ajouter un élément à un enum d’erreurs existant.\n\n\nPour les codes de statut HTTP, la spécification dit que les clients sont censés au moins comprendre les classes d’erreur (2XX, 4XX, 5XX etc.), et donc que s’ils rencontrent une erreur qu’ils ne savent pas gérer (par ex 429), ils doivent se rabattre sur l’erreur par défaut de la classe (par ex 400).\nDans la réalité, les clients ne vont pas forcément suivre la spec, et n’importe quel changement de feedback pourrait les impacter (sauf peut être d’enlever un code d’erreur qui ne peut plus arriver). Donc si on peut éviter d’introduire de nouvelles erreurs avec des clients qu’on connait pas, c’est mieux.\n\n\n\n\nSur les endpoints et les flows d’appels eux-mêmes :\nRenommer le path d’un endpoint est en théorie possible en renvoyant 301 Moved Permanently sur l’ancien path, mais dans les faits les clients ne vont pas forcément le prendre en compte et se retrouver en état d’erreur.\nOn ne peut pas non plus enlever des endpoints sans casser les clients.\nOn peut en ajouter, mais à condition que ça ne change pas les flows existants.\nPar exemple, si on ajoute un endpoint de validation qui devra être appelé juste après un ancien endpoint pour améliorer la sécurité, les clients non mis à jour ne l’appelleront pas, et auront des échecs \"silencieux\", parce que le flow aura changé.\n\n\n\n\n\n\nMême sans changer le contrat d’API, on peut quand même casser les clients par des changements d’apparence anodins du contrat invisible.\nLa loi de Hyrum dit : “With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody”.\n\n\nLes breaking changes sont bien plus graves sur les API publiques. Sur les API privées on peut en général se débrouiller pour mettre à jour les clients. Mais c’est quand même un poids non négligeable, qui doit nous amener à nous poser la question pour chaque breaking change, de savoir si on le veut vraiment.\nQuand les breaking changes deviennent inévitables, on peut versionner son API.\nLe semantic versioning classique des implémentations (Major.Minor.patch) pourrait se transcrire dans l’univers des API par une version à 2 chiffres : Breaking.Non-breaking.\nPour créer les nouvelles versions, on peut :\nUtiliser l’URL (blabla.com/apiv2/) ou les sous-domaines (v2.blabla.com/api/). C’est ce qui est le plus courant.\nAjouter des paramètres aux requêtes, que ce soit dans l’URL, dans le header HTTP ou autre. Mais globalement cette option-là est moins claire, et moins appréciée des consommateurs.\nUtiliser le fait que l’utilisateur de l’API est identifié, et donc stocker côté provider une association UserID / Version. C’est plus simple pour le consommateur qui n’a rien à faire de son côté à part demander à passer à la V2, mais moins flexible aussi.\n\n\nIl est possible de versionner avec une plus grande granularité que l’API entière, par exemple ajouter une V2 seulement pour certains endpoints, seulement certaines méthodes, ou encore donner la possibilité de visionner le contenu du body avec des headers HTTP.\nMais c’est peu connu des consommateurs, surtout dans le monde de REST, et ça peut porter à confusion. Donc bien réfléchir avant d’aller par là.\n\n\n\n\nPour éviter d’avoir à faire des breaking changes, on peut essayer dès le début d’avoir des structures extensibles.\nPar exemple, prendre l’habitude d’encapsuler dans un objet toute propriété qui nous paraît importante va nous permettre d’y ajouter des propriétés par la suite.\nSi on a des propriétés similaires, il peut être judicieux de les mettre dans une liste ou un objet, pour pouvoir facilement les étendre plus tard.\nPar ex si on a une date de début, de fin, on peut les mettre dans une liste d’événements, pour peut être plus tard ajouter une date d’exécution comme élément de liste supplémentaire.\n\n\nUtiliser des formats de données standard permet aussi de baisser le risque de vouloir les changer plus tard.\nConcernant les erreurs, les mettre dans une liste, avec chacune dans son objet permet de les rendre extensibles, et de pouvoir en ajouter aussi.\nLa propriété type correspondant à un enum indiquant le type d’erreur doit être le plus générique possible parce qu’on ne peut pas changer les enums sans breaking change.\nEx : MISSING_MANDATORY_ATTRIBUTE, plutôt que MISSING_AMOUNT si la propriété manquante était un amount.\n\n\nAutre exemple : l’auteur conseille de ne pas retourner d’erreur si l’utilisateur demande 150 résultats par page et que le maximum est de 100, mais d’en retourner simplement 100. Et plus tard, si on veut abaisser ce maximum à 50, on ne provoquera pas non plus d’erreurs chez les clients.\nBien sûr, si l’utilisateur veut transférer 1500€ et qu’il a 1000€, il ne faut pas faire le transfert de 1000€ silencieusement, quand ça concerne le domaine métier ce genre de choix doit être réfléchi avec soin :)\n\n\n\n\nPlus l’API va grossir, et plus l’étendre va devenir difficile sans casser soit les données, soit les flows. Une bonne pratique est de garder les API petites, et d’en faire plusieurs autonomes.","10---designing-a-network-efficient-api#10 - Designing a network-efficient API":"Le “design idéal” d’API est contrebalancé par des facteurs supplémentaires à prendre en compte. L’un d’entre eux est la performance réseau. Il faut trouver un compromis entre les deux.\nLes problèmes de performance réseau dépendent :\nde la vitesse du réseau\ndu volume de données échangée\ndu nombre d’appels API\n\n\nIl y a d’abord des optimisations au niveau du protocole :\nLa compression et les connexions persistantes sont disponibles par défaut dans HTTP, et peuvent être activées dès le début.\nLa compression permet de réduire les données échangées\nLes connexions persistantes permettent de réutiliser les mêmes connexions pour plusieurs requêtes, pour gagner de la latence.\n\n\nChaque endpoint/méthode peut renvoyer des métadonnées indiquant combien de temps la réponse doit être mise en cache (donc conservée sans refaire d’appel API) par le client.\nPour le protocole HTTP ça se fait avec le header Cache-Control.\nPour choisir la valeur on se base sur le contexte au cas par cas : est-ce qu’il est probable statistiquement que telle donnée soit changée dans l’heure ? Dans les 10 mn ? Est-ce que c’est important d’avoir des données très à jour sur telle ou telle donnée ?\n\n\nEn plus du cache on peut aussi utiliser les requêtes conditionnelles : cette fois il s’agira pour le backend de soit retourner la donnée si elle a été modifiée depuis la dernière fois, soit une simple réponse 304 Not Modified sans rien dans le body.\nOn gagne en bande passante.\nLe mécanisme consiste à ce que le backend envoie un Etag dans le header HTTP la première fois, et que ce tag soit renvoyé par le client pour les prochaines fois. Le backend peut alors savoir si depuis cet Etag la donnée a changé ou non. Si elle a changé, il renverra un nouvel Etag pour la prochaine fois.\n\n\n\n\nMais on peut aussi optimiser l'utilisation réseau grâce à des techniques de design d’API :\nPermettre la pagination et le filtrage est une des panières d’envoyer moins de données à la fois.\nDans le cas d’une pagination, si des éléments sont ajoutés au fur et à mesure et qu’on veut garder une fiabilité et une exhaustivité de ce qui est affiché, on peut demander les X prochains éléments à partir de l’élément qui a tel ID, plutôt que juste la page 2.\n\n\nBien choisir les propriétés pour les éléments d’une liste.\nIl faut trouver une balance entre trop de propriétés (voir toutes) quand on fait GET sur une ressource sans préciser laquelle, et pas assez de propriétés. Dans un cas on a beaucoup de données quelle que soit l’utilisation, dans l’autre on peut se retrouver à devoir refaire un GET mais cette fois avec l’ID de chaque objet, pour obtenir le détail avec les propriétés qui nous intéressent.\nLa solution est de trouver les propriétés qui sont souvent utiles quand on traite une liste d’objets, et les ajouter à la réponse de l’API de liste.\nUne autre solution serait d’accepter un paramètre ou un header HTTP (par ex Accept:application/notre.content.type.custom+json), et de renvoyer la version courte, complète, ou même enrichie en propriétés en fonction de ce qui est demandé par le client.\n\n\nAgréger des données venant de ressources différentes dans une même réponse d’API. Il s’agit de dénormalisation de données dans l’API.\nÇa peut permettre d’économiser des appels d’API.\nPar exemple en obtenant dans un même appel le profil d’une personne, et la liste de ses adresses qui sont d'habitude dans une ressource séparée.\n\n\nMais c’est à manipuler avec précaution :\nD’abord c’est pas sûr que ça nous fasse économiser du temps dans tous les cas : parfois une énorme requête peut prendre plus de temps que plusieurs en parallèle, ou même être plus fragile sur des réseaux lents (type 3G), et donc plus souvent sujette à être retentée de zéro.\nEnsuite ça peut être plus difficile à mettre en cache, sachant que le temps de cache sera alors celui de la ressource contenue dans l’agrégation qui a le temps de vie le plus court.\nDonc mauvaise idée de grouper une ressource qui change peu souvent et une ressource qui change très souvent dans un même endpoint.\n\n\nOn peut aussi le faire de l’expansion à la demande avec un paramètre : dénormaliser les données des propriétés indiquées par le client, si c’est possible.\nMême exemple que le profil avec ses adresses, sauf que le client ajoute un paramètre /profile?expand=addresses\n\n\n\n\nPermettre le querying côté client avec des API du type GraphQL. Les clients indiquent alors chaque propriété qu’ils veulent parmi celles disponibles pour une même ressource.\nC’est utile quand le réseau est vraiment important pour nous, et que chaque milliseconde compte.\nPar contre GraphQL utilise uniquement la méthode POST sur un unique endpoint HTTP, donc les mécanismes de cache usuels qu’on peut mettre en place avec les API REST et les headers HTTP (Cache-Control etc.) ne marchent pas.\nL’éventuelle mise en cache doit être gérée par le client plutôt qu’un niveau du protocole, mais c’est de toute façon assez compliqué étant donné qu’on fait en fait de l’agrégation dans tous les sens.\n\n\n\n\nDe manière générale, revenir aux bases et envoyer aux utilisateurs ce dont ils ont besoin peut probablement permettre de réduire les appels réseau. Que ce soit avec de l’agrégation, l’ajout d’une propriété ici ou là, l’ajout de nouveaux endpoints spécifiques pour pour des besoins précis etc.\nMais comme toujours il faut trouver le bon compromis. En créant trop d’endpoints personnalisés on risque aussi d’obtenir une API compliquée et pas très réutilisable. Une solution peut être alors de créer des couches d’API intermédiaires consommant l’API initiale, et fournissant des endpoints spécifiques à un contexte donné.\nPar exemple, les BFF (Backend For Frontend) sont des API qui vont consommer des API de données plus génériques, et fournir des endpoints personnalisés pour répondre exactement aux besoins du frontend qu’ils ont en charge.","11---designing-an-api-in-context#11 - Designing an API in context":"La nature des informations et de la relation provider / consumer peut lourdement influencer le design d’API. On peut avoir parfois besoin d’une communication initiée par le provider.\nExemple : une transaction initiée par le client est acceptée tout de suite (donc réponse 202 Accepted), mais doit ensuite être validée puis exécutée, et ça peut prendre des minutes, heures voire jours.\nLe client peut faire des appels réguliers pour voir où ça en est, et le header Cache-Control peut donner au client une idée de la fréquence à laquelle faire ces appels.\nPour autant, il est hors de question que le client patiente pendant des heures ou des jours en faisant des appels réguliers, c’est trop inefficace.\n\n\nLa solution peut être la mise en place d’un webhook côté client.\nIl s’agit d’une API HTTP, standardisée par le provider (pour que ce ne soit pas l’anarchie) et mise en place par le client, et dont le client a donné l’URL au provider au moment de souscrire pour avoir un token et pouvoir consommer l’API du provider.\nUne fois que la transaction a été exécutée, le provider va alors faire un appel POST sur ce webhook, en donnant les infos de l’événement qui s’est produit. Ce backend maintenu par le client peut alors utiliser les moyens nécessaires pour envoyer l’info au téléphone, ou au navigateur (par des mécanismes push, des websockets, du simple polling HTTP de la part du navigateur etc.).\nAttention à penser à la sécurité du webhook. Une des possibilités c’est d’en faire un event assez générique qui va pousser le client à faire une autre requête pour obtenir les infos détaillées de l’event.\nCe système de webhooks est standardisé par le W3C sous le nom WebSub.\n\n\nDans le cas où c’est le client qui veut obtenir des updates très fréquents pendant un certain temps, et pour éviter qu’il fasse des calls permanents, il peut faire un appel qui initie une communication SSE (Server-Sent Events).\nLes SSE se basent sur HTTP, et ont été standardisés par le W3C pour HTML5, donc fonctionnent très bien avec les navigateurs.\nLa connexion HTTP reste ouverte, et le serveur peut simplement envoyer des données dès qu’il les a, jusqu’à ce que l’un des deux demande à fermer la connexion.\nPar contre c’est unidirectionnel : c’est le serveur qui envoie les données.\n\n\nSi on veut une connexion bidirectionnelle (par exemple pour les chats), on peut recourir aux WebSockets.\nLes WebSockets ne se basent pas sur HTTP mais directement sur TCP, donc la mise en place peut être plus compliquée vis-à-vis des proxies et autres.\n\n\n\n\nParfois, pour économiser des requêtes on peut vouloir faire des PATCH, PUT, POST ou DELETE sur une liste d’éléments.\nIl suffit d’envoyer en body une liste d’objets avec la même information qu’on aurait donné pour faire l’opération sur un seul élément.\nCôté réponse, dans un cas comme ça on peut valider les éléments qui réussissent même si certains ont échoué.\nOn peut retourner une liste avec là aussi la réponse qu’on aurait renvoyé pour chaque élément seul : le statut, la ou les erreurs éventuelles ou le contenu pour chaque élément.\nLe statut de la réponse HTTP peut être alors 207 Multi-Status.\nAttention à bien vérifier que dans notre cas valider certains éléments et avoir des éléments sur d’autres ne crée pas d’incohérence.\n\n\n\n\nLe type d’API qu’on utilise doit être choisi en fonction du contexte, à la fois celui du consumer parce que l’API est pour lui, et celui du provider parce qu’il a aussi des contraintes. Il faut éviter de céder au biais de choisir l’outil ou le format qu’on connaît le mieux à chaque fois.\nActuellement il y a 3 types d’API connues : REST, GraphQL et gRPC.\nREST étant de loin le plus connu, et répondant à la plupart des besoins, il est considéré comme celui à prendre par défaut si on n’a pas de contraintes particulières. Les autres peuvent aussi être très bien en fonction du contexte.\nREST est basé sur les ressources et suit le protocole HTTP de très près. Il est le plus standardisé, et permet de profiter nativement des fonctionnalités de HTTP (content-negotiation, caching, requêtes conditionnelles etc.).\nLa gestion d’erreur est standardisée : 4XX, 5XX.\n\n\ngRPC (g pour Google, et RPC pour Remote Procedure Call) consiste à appeler des fonctions dans son code en leur donnant des paramètres. Ces fonctions sont complètement custom de la part du provider, et ne se basent pas sur un modèle comme les ressources pour REST.\nIl est en général utilisé avec des données formatées en protobuf (à la place de JSON), qui est un format binaire et ne répétant pas le nom de chaque propriété contrairement à JSON (donc peut diviser par presque 2 la taille des données transférées). Par contre c'est moins connu que JSON, et moins facile à afficher / débugger parce que binaire.\nIl est construit par dessus HTTP mais n’utilise pas la plupart de ses fonctionnalités comme REST. Par contre, il permet les communications bidirectionnelles grâce à HTTP2.\nIl a un format standard d’erreurs inspiré de REST, et pas de mécanisme standard de caching.\nIl faut l’utiliser plutôt que REST quand on a une API privée vers client privé (communication entre services d’un même backend), quand les millisecondes gagnées importent vraiment.\n\n\nGraphQL adopte le même modèle basé sur les fonctions que gRPC pour la création/modification, par contre pour le querying il a un modèle basé sur les données : c’est l’utilisateur qui spécifie non seulement les propriétés qu’il veut, mais aussi avec quel autre donnée il veut faire un lien etc.\nPour le query l’utilisateur a une grande flexibilité, mais il est aussi en capacité de demander des requêtes extrêmement complexes et coûteuses. Le rate-limiting peut ne pas suffire pour se protéger parce qu’on ne peut pas prévoir ce que le client va faire.\nIl est agnostique niveau protocole, et est utilisé la plupart du temps par dessus HTTP avec des POST sur un endpoint unique.\nIl a un format standard d’erreurs qui consiste à donner un texte d’erreur, et la ligne qui a causé l’erreur dans la requête GraphQL et la propriété problématique dans la donnée en réponse (si une réponse partielle a pu être faite).\nIl n’a pas de mécanisme standard pour le caching, et celui-ci est difficile étant donné la flexibilité des données.\nIl faut aller vers GraphQL plutôt que REST quand on est dans un contexte d’API privée et alimentant des périphériques mobiles qui ont besoin d’économiser la quantité de données échangée.\n\n\nA noter qu’on peut tout à fait avoir par ex une API BFF spécialisée qui expose du GraphQL pour les mobiles, et qui consomme une API REST ou gRPC plus générique dans notre backend.","12---documenting-an-api#12 - Documenting an API":"Créer une documentation est une manière de tester le design : si on est incapable de l’expliquer c’est qu’il y a peut être des incohérences.\nDans la documentation d’une API, il faut bien évidemment une référence des endpoints possibles. Typiquement le genre de documentation qu’on peut générer à partir d’une spécification OpenAPI.\nPour fournir plusieurs exemples dans OpenAPI :\nrequestBody:\ncontent:\n\"application/json\":\n#...\nexamples:\npremierExemple:\n# ...\ndeuxiemeExemple:\nsummary: Résumé du 2ème exemple\ndescription: Description du 2ème exemple\nvalue:\nprop1: \"blablabla\"\nprop2: 3\nprop3: \"bliblibli\"\n\nLes outils comme ReDoc sont même capables de générer des exemples de données JSON complets à partir des types et formats déclarés dans la spécification OpenAPI.\nIl faut décrire ce que fait chaque endpoint, les données qu’il prend et renvoie, mais aussi chaque cas possible de réponse de réussite ou d’erreur. Et ne pas oublier la sécurité (scopes, autorisations). Tout ça se fait dans OpenAPI.\nA propos de l’idée de générer la documentation à partir du code (avec éventuellement un format OpenAPI automatique intermédiaire) versus maintenir le fichier OpenAPI à la main :\nL’avantage c’est la synchronisation plus facile entre le code et la doc.\nLes inconvénients sont surtout le manque de flexibilité : la documentation finale contiendra moins de choses si tout vient du code. Et le risque que l’API ait plus tendance à exposer le point de vue du provider, ce qu’on souhaite éviter.\n\n\n\n\nIl faut aussi un guide utilisateur qui va expliquer l’API d’un point de vue global, les principes généraux, comment éventuellement s’inscrire pour avoir un token etc.\nPour le coup cette partie doit être écrite à la main, par exemple en markdown.\nOn y ajoute des cas d’usage détaillés, qui seront le reflet de l’API goal canvas qu’on a utilisé pour designer notre API : telle personne a besoin de telle et telle info pour pour faire telle chose + le détail des infos qu’elle donne, les API calls, les réponses qu’elle reçoit.\nOn peut aussi y ajouter les choses communes à l’API comme le type de pagination utilisée, les formats de données et plus généralement les headers HTTP supportés etc.\n\n\nIl peut être utile d’avoir une documentation spécifique pour les développeurs de l’implémentation.\nElle peut contenir des choses que les consommateurs n’ont pas besoin de savoir, et qui sont liées à la manière dont l’API s’intègre avec l’implémentation. Par exemple des infos sur d’où vient telle ou telle propriété dans le système, des liens vers des formats pour aider à implémenter etc.\n\n\nEt enfin il faut un changelog listant pour chaque version ce qui est ajouté/modifié, et surtout les breaking changes.\nOpenAPI ne permet pas de faire de changelog, mais il permet d’ajouter une propriété deprecated: true sur les paramètres, les endpoints, les propriétés.","13---growing-apis#13 - Growing APIs":"A partir du moment où plusieurs personnes travaillent sur l’API, ou qu’il y a plusieurs APIs dans l’organisation, il y a de fortes chances pour que de l’incohérence s’installe entre APIs et au sein d’une API. Il faut pour ça rédiger des API guidelines à destination des designers d’API.\nIl y a 3 parties :\nIl y a d’abord les reference guidelines qui sont le minimum : le document décrit quelles méthodes, codes de statut, headers sont utilisés couramment dans l’API, le format des ressources (la structure de leur path), la structure habituelle des données, la manière dont la pagination est gérée etc.\nOn y définit aussi les termes utilisés dans l’API (ressource, version etc.) à l’image du langage ubiquitaire du DDD.\n\n\nEnsuite il faut les use case guidelines qui sont une version plus digeste et prête à l’emploi pour étendre l’API. Par exemple comment créer un nouvel élément dans l’API, et on montre pas à pas, comme un tuto.\nOn fait bien attention à utiliser le même vocabulaire partagé que celui défini dans les reference guidelines.\n\n\nEnfin, on peut ajouter des design process guidelines où on va donner des référence vers des ressources utiles pour creuser tel ou tel aspect utile pour le design de l’API. Par exemple des liens vers des spécifications, tutoriels, livres etc.\n\n\nLe site webconcepts.info rassemble des informations fiables sur HTTP, OAuth et d’autres concepts liés avec des liens vers les spécifications, intéressant quand on cherche la bonne spécification à suivre.\nLe site apistylebook.com rassemble des API guidelines célèbres dont on peut s’inspirer.\nIl faut écrire les guidelines petit à petit de manière itérative, en n’y mettant que ce dont on est sûr à chaque fois, et en étant prêt à revoir le contenu.\nIl faut aussi que ce soit un travail collectif et permanent de tous ceux qui sont en charge du design de l’API, sinon les guidelines ne seront pas respectées.\n\n\n\n\nUne API étant potentiellement difficile à changer, et pouvant poser des problèmes importants de sécurité, il est important qu’il y ait une procédure de review sérieuse à chaque changement.\nIl faut avant tout identifier les besoins derrière un changement d’API.\nLa méthode des 5 “Pourquoi ?” (Pourquoi vouloir faire ceci ? Parce que cela. Et pourquoi cela ? etc.) permet en général de remonter au besoin racine.\nIl faut aussi bien comprendre le contexte des utilisateurs, du provider de l’API, les aspects sécurité concernés (données sensibles ?) etc.\n\n\nEnsuite il faut linter l’API :\nExaminer si le design proposé suit bien les guidelines.\nS’il est cohérent avec le reste de l’API (même si c’est pas encore écrit dans les guidelines).\nSi des erreurs de forme ne sont pas glissées dedans. Par exemple des propriétés abrégées ou manifestement pas user friendly.\nUne partie peut être faite avec des outils automatiques pour gagner du temps.\n\n\nIl faut reviewer le design du point de vue du provider.\nIl faut vérifier que l’API est extensible, sécurisée et ne devrait à priori pas poser de problèmes de performance ou d’implémentation.\n\n\nLe reviewer du point de vue du consommateur.\nSe demander si on n’expose pas la structure interne du provider au lieu de répondre au besoin de l’utilisateur. Vérifier que l’API est facile à utiliser, que le flow est suffisamment simple etc.\nObtenir du feedback des clients peut aussi avoir de la valeur pour obtenir une bonne API.\n\n\nEt enfin une fois que l’implémentation est faite (ou en cours), vérifier qu’elle correspond bien avec la spécification de l’API.\nÇa se fait avec des tests unitaires, et des tests au niveau de l’API.\nIl faut toujours réaliser les tests de sécurité (vérifier le contrôle d’accès, vérifier que les données sensibles ne fuitent pas).\nIl faut tester que l’API fonctionne comme prévu au runtime, pas seulement avec des techniques statiques sans faire tourner l’API.\nIl ne faut pas oublier de tester le caractère obligatoire des propriétés ou encore le fait qu’une valeur doive être entre un min et un max.\nIl faut tester l’API aussi dans toute la chaîne réseau pour vérifier que des proxis, pare-feux, routeurs ne nous causent pas de problèmes."}}}